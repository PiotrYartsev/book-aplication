{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common-words.txt') as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "of\n",
      "and\n",
      "to\n",
      "a\n",
      "in\n",
      "for\n",
      "is\n",
      "on\n",
      "that\n",
      "by\n",
      "this\n",
      "with\n",
      "i\n",
      "you\n",
      "it\n",
      "not\n",
      "or\n",
      "be\n",
      "are\n",
      "from\n",
      "at\n",
      "as\n",
      "your\n",
      "all\n",
      "have\n",
      "new\n",
      "more\n",
      "an\n",
      "was\n",
      "we\n",
      "will\n",
      "home\n",
      "can\n",
      "us\n",
      "about\n",
      "if\n",
      "page\n",
      "my\n",
      "has\n",
      "search\n",
      "free\n",
      "but\n",
      "our\n",
      "one\n",
      "other\n",
      "do\n",
      "no\n",
      "information\n",
      "time\n",
      "they\n",
      "site\n",
      "he\n",
      "up\n",
      "may\n",
      "what\n",
      "which\n",
      "their\n",
      "news\n",
      "out\n",
      "use\n",
      "any\n",
      "there\n",
      "see\n",
      "only\n",
      "so\n",
      "his\n",
      "when\n",
      "contact\n",
      "here\n",
      "business\n",
      "who\n",
      "web\n",
      "also\n",
      "now\n",
      "help\n",
      "get\n",
      "pm\n",
      "view\n",
      "online\n",
      "c\n",
      "e\n",
      "first\n",
      "am\n",
      "been\n",
      "would\n",
      "how\n",
      "were\n",
      "me\n",
      "s\n",
      "services\n",
      "some\n",
      "these\n",
      "click\n",
      "its\n",
      "like\n",
      "service\n",
      "x\n",
      "than\n",
      "find\n"
     ]
    }
   ],
   "source": [
    "common_words=[]\n",
    "\n",
    "\n",
    "for n in lines[:100]:\n",
    "    print(n.replace(\"\\n\",\"\"))\n",
    "    common_words.append(n.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import re\n",
    "text = textract.process('/home/piotr/Desktop/Books/Programming/Python 2019 Pack/Clean Code in Python/PDF/PDF.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x0cClean Code in Python\\n\\nRefactor your legacy code base\\n\\nMariano Anaya\\n\\nBIRMINGHAM - MUMBAI\\n\\n\\x0cClean Code in Python\\nCopyright \\xc2\\xa9 2018 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form\\nor by any means, without the prior written permission of the publisher, except in the case of brief quotations\\nembedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented.\\nHowever, the information contained in this book is sold without warranty, either express or implied. Neither the\\nauthor, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to\\nhave been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products\\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy\\nof this information.\\nCommissioning Editor: Merint Mathew\\nAcquisition Editor: Denim Pinto\\nContent Development Editor: Priyanka Sawant\\nTechnical Editor: Gaurav Gala\\nCopy Editor: Safis Editing\\nProject Coordinator: Vaidehi Sawant\\nProofreader: Safis Editing\\nIndexer: Rekha Nair\\nGraphics: Jason Monteiro\\nProduction Coordinator: Shantanu Zagade\\nFirst published: August 2018\\nProduction reference: 1270818\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-78883-583-1\\n\\nwww.packtpub.com\\n\\n\\x0c\\x0cTo my family and friends, for their unconditional love and support.\\n\\xe2\\x80\\x93 Mariano Anaya\\n\\n\\x0cmapt.io\\n\\nMapt is an online digital library that gives you full access to over 5,000 books and videos, as\\nwell as industry leading tools to help you plan your personal development and advance\\nyour career. For more information, please visit our website.\\n\\nWhy subscribe?\\nSpend less time learning and more time coding with practical eBooks and Videos\\nfrom over 4,000 industry professionals\\nImprove your learning with Skill Plans built especially for you\\nGet a free eBook or video every month\\nMapt is fully searchable\\nCopy and paste, print, and bookmark content\\n\\nPacktPub.com\\nDid you know that Packt offers eBook versions of every book published, with PDF and\\nePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a\\nprint book customer, you are entitled to a discount on the eBook copy. Get in touch with us\\nat service@packtpub.com for more details.\\nAt www.PacktPub.com, you can also read a collection of free technical articles, sign up for a\\nrange of free newsletters, and receive exclusive discounts and offers on Packt books and\\neBooks.\\n\\n\\x0cContributors\\nAbout the author\\nMariano Anaya is a software engineer who spends most of his time creating software with\\nPython and mentoring fellow programmers. Mariano\\'s main areas of interests besides\\nPython are software architecture, functional programming, distributed systems, and\\nspeaking at conferences.\\nHe was a speaker at Euro Python 2016 and 2017. To know more about him, you can refer to\\nhis GitHub account with the username rmariano.\\nHis speakerdeck username is rmariano.\\n\\n\\x0cAbout the reviewer\\nNimesh Kiran Verma has a dual degree in Maths and Computing from IIT Delhi and has\\nworked with companies such as LinkedIn, Paytm and ICICI for about 5 years in software\\ndevelopment and data science.\\nHe co-founded a micro-lending company, Upwards Fintech and presently serves as its\\nCTO. He loves coding and has mastery in Python and its popular frameworks, Django and\\nFlask.\\nHe extensively leverages Amazon Web Services, design patterns, SQL and NoSQL\\ndatabases, to build reliable, scalable and low latency architectures.\\nTo my mom, Nutan Kiran Verma, who made me what I am today and gave the confidence\\nto pursue all my dreams. Thanks Papa, Naveen, and Prabhat, who motivated me to steal\\ntime for this book when in fact I was supposed to be spending it with them. Ulhas and the\\nentire Packt team\\'s support was tremendous. Thanks Varsha Shetty for introducing me to\\nPackt.\\n\\nPackt is searching for authors like you\\nIf you\\'re interested in becoming an author for Packt, please visit authors.packtpub.com\\nand apply today. We have worked with thousands of developers and tech professionals,\\njust like you, to help them share their insight with the global tech community. You can\\nmake a general application, apply for a specific hot topic that we are recruiting an author\\nfor, or submit your own idea.\\n\\n\\x0cTable of Contents\\nPreface\\n\\n1\\n\\nChapter 1: Introduction, Code Formatting, and Tools\\nThe meaning of clean code\\nThe importance of having clean code\\nThe role of code formatting in clean code\\nAdhering to a coding style guide on your project\\n\\nDocstrings and annotations\\n\\nDocstrings\\nAnnotations\\nDo annotations replace docstrings?\\nConfiguring the tools for enforcing basic quality gates\\nType hinting with Mypy\\nChecking the code with Pylint\\nSetup for automatic checks\\n\\nSummary\\nChapter 2: Pythonic Code\\nIndexes and slices\\n\\nCreating your own sequences\\n\\nContext managers\\n\\nImplementing context managers\\n\\nProperties, attributes, and different types of methods for objects\\nUnderscores in Python\\nProperties\\n\\nIterable objects\\n\\nCreating iterable objects\\nCreating sequences\\n\\nContainer objects\\nDynamic attributes for objects\\nCallable objects\\nSummary of magic methods\\nCaveats in Python\\nMutable default arguments\\nExtending built-in types\\n\\nSummary\\nReferences\\nChapter 3: General Traits of Good Code\\nDesign by contract\\n\\n7\\n8\\n8\\n9\\n10\\n12\\n13\\n16\\n18\\n20\\n20\\n21\\n21\\n24\\n25\\n26\\n28\\n29\\n32\\n35\\n35\\n38\\n40\\n40\\n43\\n45\\n46\\n48\\n49\\n49\\n50\\n51\\n53\\n53\\n55\\n56\\n\\n\\x0cTable of Contents\\n\\nPreconditions\\nPostconditions\\nPythonic contracts\\nDesign by contract \\xe2\\x80\\x93 conclusions\\n\\nDefensive programming\\nError handling\\n\\nValue substitution\\nException handling\\n\\nHandle exceptions at the right level of abstraction\\nDo not expose tracebacks\\nAvoid empty except blocks\\nInclude the original exception\\n\\nUsing assertions in Python\\n\\nSeparation of concerns\\nCohesion and coupling\\n\\nAcronyms to live by\\nDRY/OAOO\\nYAGNI\\nKIS\\nEAFP/LBYL\\n\\nComposition and inheritance\\n\\nWhen inheritance is a good decision\\nAnti-patterns for inheritance\\nMultiple inheritance in Python\\nMethod Resolution Order (MRO)\\nMixins\\n\\nArguments in functions and methods\\nHow function arguments work in Python\\nHow arguments are copied to functions\\nVariable number of arguments\\n\\nThe number of arguments in functions\\n\\nFunction arguments and coupling\\nCompact function signatures that take too many arguments\\n\\nFinal remarks on good practices for software design\\nOrthogonality in software\\nStructuring the code\\n\\nSummary\\nReferences\\nChapter 4: The SOLID Principles\\nSingle responsibility principle\\n\\nA class with too many responsibilities\\nDistributing responsibilities\\n\\nThe open/closed principle\\n\\nExample of maintainability perils for not following the open/closed principle\\nRefactoring the events system for extensibility\\nExtending the events system\\n\\n[ ii ]\\n\\n58\\n59\\n59\\n59\\n60\\n61\\n61\\n62\\n64\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n72\\n74\\n74\\n76\\n77\\n78\\n79\\n82\\n82\\n84\\n85\\n85\\n86\\n87\\n91\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n99\\n99\\n100\\n102\\n103\\n103\\n105\\n107\\n\\n\\x0cTable of Contents\\n\\nFinal thoughts about the OCP\\n\\nLiskov\\'s substitution principle\\nDetecting LSP issues with tools\\n\\nDetecting incorrect datatypes in method signatures with Mypy\\nDetecting incompatible signatures with Pylint\\n\\nMore subtle cases of LSP violations\\nRemarks on the LSP\\n\\nInterface segregation\\n\\nAn interface that provides too much\\nThe smaller the interface, the better\\nHow small should an interface be?\\n\\nDependency inversion\\n\\nA case of rigid dependencies\\nInverting the dependencies\\n\\nSummary\\nReferences\\nChapter 5: Using Decorators to Improve Our Code\\nWhat are decorators in Python?\\nDecorate functions\\nDecorate classes\\nOther types of decorator\\nPassing arguments to decorators\\n\\nDecorators with nested functions\\nDecorator objects\\n\\nGood uses for decorators\\n\\nTransforming parameters\\nTracing code\\n\\nEffective decorators \\xe2\\x80\\x93 avoiding common mistakes\\nPreserving data about the original wrapped object\\nDealing with side-effects in decorators\\nIncorrect handling of side-effects in a decorator\\nRequiring decorators with side-effects\\n\\nCreating decorators that will always work\\n\\nThe DRY principle with decorators\\nDecorators and separation of concerns\\nAnalyzing good decorators\\nSummary\\nReferences\\nChapter 6: Getting More Out of Our Objects with Descriptors\\nA first look at descriptors\\nThe machinery behind descriptors\\nExploring each method of the descriptor protocol\\n__get__(self, instance, owner)\\n__set__(self, instance, value)\\n\\n[ iii ]\\n\\n109\\n110\\n111\\n111\\n113\\n113\\n116\\n117\\n118\\n118\\n119\\n119\\n120\\n121\\n122\\n123\\n124\\n124\\n126\\n127\\n131\\n131\\n132\\n134\\n135\\n135\\n136\\n136\\n136\\n139\\n139\\n141\\n143\\n146\\n147\\n149\\n150\\n151\\n152\\n152\\n153\\n156\\n156\\n157\\n\\n\\x0cTable of Contents\\n\\n__delete__(self, instance)\\n__set_name__(self, owner, name)\\n\\nTypes of descriptors\\n\\nNon-data descriptors\\nData descriptors\\n\\nDescriptors in action\\n\\nAn application of descriptors\\n\\nA first attempt without using descriptors\\nThe idiomatic implementation\\n\\nDifferent forms of implementing descriptors\\nThe issue of global shared state\\nAccessing the dictionary of the object\\nUsing weak references\\n\\nMore considerations about descriptors\\nReusing code\\nAvoiding class decorators\\n\\nAnalysis of descriptors\\n\\nHow Python uses descriptors internally\\nFunctions and methods\\nBuilt-in decorators for methods\\nSlots\\n\\nImplementing descriptors in decorators\\n\\nSummary\\nReferences\\nChapter 7: Using Generators\\nTechnical requirements\\nCreating generators\\nA first look at generators\\nGenerator expressions\\n\\nIterating idiomatically\\nIdioms for iteration\\n\\nThe next() function\\nUsing a generator\\nItertools\\nSimplifying code through iterators\\nRepeated iterations\\nNested loops\\n\\nThe iterator pattern in Python\\n\\nThe interface for iteration\\nSequence objects as iterables\\n\\nCoroutines\\n\\nThe methods of the generator interface\\n\\nclose()\\nthrow(ex_type[, ex_value[, ex_traceback]])\\nsend(value)\\n\\nMore advanced coroutines\\n\\nReturning values in coroutines\\n\\n[ iv ]\\n\\n159\\n161\\n163\\n163\\n165\\n168\\n168\\n168\\n169\\n172\\n172\\n173\\n174\\n175\\n175\\n176\\n180\\n180\\n180\\n183\\n185\\n185\\n186\\n187\\n\\n188\\n188\\n189\\n189\\n192\\n193\\n193\\n195\\n196\\n196\\n197\\n198\\n198\\n200\\n200\\n201\\n203\\n204\\n204\\n205\\n206\\n209\\n209\\n\\n\\x0cTable of Contents\\n\\nDelegating into smaller coroutines \\xe2\\x80\\x93 the yield from syntax\\nThe simplest use of yield from\\nCapturing the value returned by a sub-generator\\nSending and receiving data to and from a sub-generator\\n\\nAsynchronous programming\\nSummary\\nReferences\\nChapter 8: Unit Testing and Refactoring\\nDesign principles and unit testing\\n\\nA note about other forms of automated testing\\nUnit testing and agile software development\\nUnit testing and software design\\nDefining the boundaries of what to test\\n\\nFrameworks and tools for testing\\n\\nFrameworks and libraries for unit testing\\nunittest\\n\\nParametrized tests\\n\\npytest\\n\\nBasic test cases with pytest\\nParametrized tests\\nFixtures\\n\\nCode coverage\\n\\nSetting up rest coverage\\nCaveats of test coverage\\n\\nMock objects\\n\\nA fair warning about patching and mocks\\nUsing mock objects\\nTypes of mocks\\nA use case for test doubles\\n\\nRefactoring\\n\\nEvolving our code\\nProduction code isn\\'t the only thing that evolves\\n\\nMore about unit testing\\nProperty-based testing\\nMutation testing\\n\\nA brief introduction to test-driven development\\nSummary\\nReferences\\nChapter 9: Common Design Patterns\\nConsiderations for design patterns in Python\\nDesign patterns in action\\nCreational patterns\\n\\nFactories\\nSingleton and shared state (monostate)\\nShared state\\nThe borg pattern\\n\\n[v]\\n\\n210\\n211\\n212\\n213\\n215\\n217\\n218\\n\\n219\\n219\\n221\\n222\\n222\\n225\\n226\\n226\\n228\\n230\\n232\\n233\\n234\\n235\\n236\\n236\\n237\\n238\\n239\\n239\\n240\\n241\\n243\\n244\\n246\\n247\\n248\\n248\\n251\\n252\\n252\\n253\\n254\\n255\\n256\\n256\\n257\\n257\\n260\\n\\n\\x0cTable of Contents\\n\\nBuilder\\n\\nStructural patterns\\nAdapter\\nComposite\\nDecorator\\nFacade\\n\\nBehavioral patterns\\n\\nChain of responsibility\\nThe template method\\nCommand\\nState\\n\\nThe null object pattern\\nFinal thoughts about design patterns\\n\\nThe influence of patterns over the design\\nNames in our models\\n\\nSummary\\nReferences\\nChapter 10: Clean Architecture\\nFrom clean code to clean architecture\\nSeparation of concerns\\nAbstractions\\n\\nSoftware components\\nPackages\\nContainers\\n\\nUse case\\n\\nThe code\\n\\nDomain models\\nCalling from the application\\nAdapters\\n\\nThe services\\nAnalysis\\n\\nThe dependency flow\\nLimitations\\nTestability\\nIntention revealing\\n\\nSummary\\nReferences\\nSumming it all up\\nOther Books You May Enjoy\\n\\n262\\n262\\n263\\n264\\n266\\n268\\n269\\n270\\n272\\n273\\n274\\n280\\n282\\n282\\n283\\n284\\n285\\n\\n286\\n286\\n287\\n288\\n290\\n290\\n293\\n295\\n296\\n296\\n298\\n300\\n300\\n304\\n304\\n305\\n305\\n306\\n306\\n307\\n307\\n308\\n\\nIndex\\n\\n311\\n\\n[ vi ]\\n\\n\\x0cPreface\\nThis is a book about software engineering principles applied to Python.\\nThere are many books about software engineering, and many resources available with\\ninformation about Python. The intersection of those two sets, though, is something that\\nrequires action, and that\\'s the gap this book tries to bridge.\\nIt would not be realistic to cover all possible topics about software engineering in a single\\nbook because the field is so wide that there are entire books dedicated to certain topics. This\\nbook focuses on the main practices or principles of software engineering that will help us\\nwrite more maintainable code, and how to write it by taking advantage of the features of\\nPython at the same time.\\nA word to the wise: there is no single solution to a software problem. It\\'s usually about\\ntrade-offs. Each solution will have upsides and downsides, and some criteria must be\\nfollowed to choose between them, accepting the costs and getting the benefits. There is\\nusually no single best solution, but there are principles to be followed, and as long as we\\nfollow them we will be walking a much safer path. And that is what this book is about:\\ninspiring the readers to follow principles and make the best choices, because even when\\nfacing difficulties, we will be much better off if we have followed good practices.\\nAnd, speaking of good practices, while some of the explanations follow established and\\nproven principles, other parts are opinionated. But that doesn\\'t mean it has to be done in\\nthat particular way only. The author does not claim to be any sort of authority on the\\nmatter of clean code, because such a title cannot possible exist. The reader is encouraged to\\nengage in critical thinking: take what works the best for your project, and feel free to\\ndisagree. Differences of opinions are encouraged as long as they yield an enlightening\\ndebate.\\nMy intention behind this book is to share the joys of Python, and idioms I have learned\\nfrom experience, in the hope that readers will find them useful to elevate their expertise\\nwith the language.\\nThe book explains the topics through code examples. These examples assume the latest\\nversion of Python at the time of this writing is used, namely Python 3.7, although future\\nversions should be compatible as well. There are no peculiarities in the code that bind it to\\nany particular platform, therefore with a Python interpreter, the code examples can be\\ntested on any operating system.\\n\\n\\x0cPreface\\n\\nIn most of the examples, with the goal of keeping the code as simple as possible, the\\nimplementations and their tests are written in plain Python using just the standard\\nlibraries. In some chapters, extra libraries were needed, and in order to run the examples of\\nthose cases, instructions have been provided along with the respective requirements.txt\\nfile.\\nThroughout this book we will discover all the features Python has to offer to make our code\\nbetter, more readable, and easier to maintain. We do so not only by exploring the features\\nof the language, but also by analyzing how software engineering practices can be applied in\\nPython. The reader will notice that some of the reference implementations differ in Python,\\nother principles or patterns change slightly, and others might not be even applicable all\\nalong. Understanding each case represents an opportunity to understand Python more\\ndeeply.\\n\\nWho this book is for\\nThis book is suitable for all software engineering practitioners who are interested in\\nsoftware design or learning more about Python. It is assumed that the reader is already\\nfamiliar with the principles of object-oriented software design and has some experience\\nwriting code.\\nIn terms of Python, the book is suitable for all levels. It\\'s good for learning Python because\\nit is organized in such a way that the content is in increasing order of complexity. The first\\nchapters will cover the basics of Python, which is a good way to learn the main idioms,\\nfunctions, and utilities available in the language. The idea is not just to solve some\\nproblems with Python, but to do so in an idiomatic way.\\nExperienced programmers will also benefit from the topics in this book, as some sections\\ncover advanced topics in Python, such as decorators, descriptors, and an introduction to\\nasynchronous programming. It will help the reader discover more about Python because\\nsome of the cases are analyzed from the internals of the language itself.\\nIt is worth emphasizing the word practitioners in the first sentence of this section. This is a\\npragmatic book. Examples are limited to what the case of study requires, but are also\\nintended to resemble the context of a real software project. It is not an academic book, and\\nas such the definitions made, the remarks made, and the recommendations given are to be\\ntaken with caution. The reader is expected to examine these recommendations critically and\\npragmatically rather than dogmatically. After all, practicality beats purity.\\n\\n[2]\\n\\n\\x0cPreface\\n\\nWhat this book covers\\nChapter 1, Introduction, Code Formatting, and Tools, is an introduction to the main tools you\\n\\nneed to set up a development environment in Python. We cover the basics a Python\\ndeveloper is recommended to know to start working with the language, as well as some\\nguidelines for maintaining readable code in the project, such as tools for static analysis,\\ndocumentation, type checking, and code formatting.\\n\\nChapter 2, Pythonic Code, looks at the first idioms in Python, which we will continue to use\\n\\nin the following chapters. We cover the particular features of Python, how they should be\\nused, and we start building knowledge around the idea that Pythonic code is in general\\nmuch better quality code.\\nChapter 3, General Traits of Good Code, reviews general principles of software engineering\\n\\nthat focus on writing maintainable code. We explore the idea and apply the concepts with\\nthe tools in the language.\\nChapter 4, The SOLID Principles, covers a set of design principles for object-oriented\\n\\nsoftware design. This acronym is part of the language or jargon of software engineering,\\nand we see how each one of them can be applied to Python. Arguably not all of them are\\nentirely applicable due to the nature of the language.\\nChapter 5, Using Decorators to Improve Our Code, looks at one of the greatest features of\\n\\nPython. After understanding how to create decorators (for functions and classes), we put\\nthem in action for reusing code, separating responsibilities, and creating more granular\\nfunctions.\\nChapter 6, Getting More Out of Our Objects with Descriptors, explores descriptors in Python,\\n\\nwhich take object-oriented design to a new level. While this is a feature more related to\\nframeworks and tools, we can see how to improve the readability of our code with\\ndescriptors, and also reuse code.\\n\\nChapter 7, Using Generators, shows that generators are probably the best feature of Python.\\n\\nThe fact that iteration is a core component of Python could make us think that it leads to a\\nnew programming paradigm. By using generators and iterators in general, we can think\\nabout the way we write our programs. With the lessons learned from generators, we go\\nfurther and learn about coroutines in Python, and the basics of asynchronous\\nprogramming.\\n\\n[3]\\n\\n\\x0cPreface\\nChapter 8, Unit Testing and Refactoring, discusses the importance of unit tests in any code\\n\\nbase that claims to be maintainable. The chapter reviews the importance of unit tests, and\\nwe explore the main frameworks for this (unittest and pytest).\\nChapter 9, Common Design Patterns, reviews how to implement the most common design\\n\\npatterns in Python, not from the point of view of solving a problem, but by examining how\\nthey solve problems by leveraging a better and more maintainable solution. The chapter\\nmentions the peculiarities of Python that have made some of the design patterns invisible\\nand takes a pragmatic approach to implement some of them.\\nChapter 10, Clean Architecture, focuses on the idea that clean code is the base of a good\\n\\narchitecture. All those details we mentioned in the first chapter, and everything else\\nrevisited along the way, will play a critical role in the entire design when the system is\\ndeployed.\\n\\nTo get the most out of this book\\nThe reader should be familiarized with Python\\'s syntax and have a valid Python interpreter\\ninstalled, which can be downloaded from https://www.python.org/downloads/\\nIt is recommended to follow the examples in the book and test the code locally. For this, it is\\nhighly recommended to create a virtual environment with Python 3.7 and run the code\\nwith this interpreter. Instructions to create a virtual environment can be found at https:/\\xe2\\x80\\x8b/\\ndocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8btutorial/\\xe2\\x80\\x8bvenv.\\xe2\\x80\\x8bhtml.\\n\\nDownload the example code files\\nYou can download the example code files for this book from your account at\\nwww.packtpub.com. If you purchased this book elsewhere, you can visit\\nwww.packtpub.com/support and register to have the files emailed directly to you.\\nYou can download the code files by following these steps:\\n1.\\n2.\\n3.\\n4.\\n\\nLog in or register at www.packtpub.com.\\nSelect the SUPPORT tab.\\nClick on Code Downloads & Errata.\\nEnter the name of the book in the Search box and follow the onscreen\\ninstructions.\\n\\n[4]\\n\\n\\x0cPreface\\n\\nOnce the file is downloaded, please make sure that you unzip or extract the folder using the\\nlatest version of:\\nWinRAR/7-Zip for Windows\\nZipeg/iZip/UnRarX for Mac\\n7-Zip/PeaZip for Linux\\nThe code bundle for the book is also hosted on GitHub at\\n\\nhttps://github.com/PacktPublishing/Clean-Code-in-Python. In case there\\'s an update\\n\\nto the code, it will be updated on the existing GitHub repository.\\n\\nWe also have other code bundles from our rich catalog of books and videos available\\nat https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bgithub.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bPacktPublishing/\\xe2\\x80\\x8b. Check them out!\\n\\nConventions used\\nThere are a number of text conventions used throughout this book.\\nCodeInText: Indicates code words in text, database table names, folder names, filenames,\\n\\nfile extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an\\nexample: \"Then, just running the pylint command is enough to check it in the code.\"\\nA block of code is set as follows:\\nclass Point:\\ndef __init__(self, lat, long):\\nself.lat = lat\\nself.long = long\\n\\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines\\nor items are set in bold:\\nsetup(\\nname=\"apptool\",\\ndescription=\"Description of the intention of the package\",\\nlong_description=long_description,\\n\\nAny command-line input or output is written as follows:\\n>>> locate.__annotations__\\n{\\'latitude\\': float, \\'longitue\\': float, \\'return\\': __main__.Point}\\n\\nBold: Indicates a new term, an important word, or words that you see onscreen. For\\nexample, words in menus or dialog boxes appear in the text like this. Here is an example:\\n\"Select System info from the Administration panel.\"\\n\\n[5]\\n\\n\\x0cPreface\\n\\nWarnings or important notes appear like this.\\n\\nTips and tricks appear like this.\\n\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: Email feedback@packtpub.com and mention the book title in the\\nsubject of your message. If you have questions about any aspect of this book, please email\\nus at questions@packtpub.com.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes\\ndo happen. If you have found a mistake in this book, we would be grateful if you would\\nreport this to us. Please visit www.packtpub.com/submit-errata, selecting your book,\\nclicking on the Errata Submission Form link, and entering the details.\\nPiracy: If you come across any illegal copies of our works in any form on the Internet, we\\nwould be grateful if you would provide us with the location address or website name.\\nPlease contact us at copyright@packtpub.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise in\\nand you are interested in either writing or contributing to a book, please visit\\nauthors.packtpub.com.\\n\\nReviews\\nPlease leave a review. Once you have read and used this book, why not leave a review on\\nthe site that you purchased it from? Potential readers can then see and use your unbiased\\nopinion to make purchase decisions, we at Packt can understand what you think about our\\nproducts, and our authors can see your feedback on their book. Thank you!\\nFor more information about Packt, please visit packtpub.com.\\n\\n[6]\\n\\n\\x0c1\\nIntroduction, Code Formatting,\\nand Tools\\nIn this chapter, we will explore the first concepts related to clean code, starting with what it\\nis and what it means. The main point of the chapter is to understand that clean code is not\\njust a nice thing to have or a luxury in software projects. It\\'s a necessity. Without quality\\ncode, the project will face the perils of failing due to an accumulated technical debt.\\nAlong the same lines, but going into a bit more detail, are the concepts of formatting and\\ndocumenting the code. This also might sound like a superfluous requirement or task, but\\nagain, we will discover that it plays a fundamental role in keeping the code base\\nmaintainable and workable.\\nWe will analyze the importance of adopting a good coding guideline for this project.\\nRealizing that maintaining the code align to the reference is a continuous task, and we will\\nsee how we can get help from automated tools that will ease our work. For this reason, we\\nquickly discuss how to configure the main tools so that they automatically run on the\\nproject as part of the build.\\nAfter reading this chapter, you will have an idea of what clean code is, why it is important,\\nwhy formatting and documenting the code are crucial tasks, and how to automate this\\nprocess. From this, you should acquire the mindset for quickly organizing the structure of a\\nnew project, aiming for good code quality.\\nAfter reading this chapter, you will have learned the following:\\nThat clean code really means something far more important than formatting in\\nsoftware construction\\nThat even so, having a standard formatting is a key component to have in a\\nsoftware project, for the sake of its maintainability\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nHow to make the code self-documenting by using the features that Python\\nprovides\\nHow to configure tools to help arrange the layout of the code in a consistent way\\nso that team members can focus on the essence of the problem\\n\\nThe meaning of clean code\\nThere is no sole or strict definition of clean code. Moreover, there is probably no way of\\nformally measuring clean code, so you cannot run a tool on a repository that could tell you\\nhow good, bad, or maintainable or not that code is. Sure, you can run tools such as\\ncheckers, linters, static analyzers, and so on. And those tools are of much help. They are\\nnecessary, but not sufficient. Clean code is not something a machine or script could tell (so\\nfar), but rather something that us, as professionals, can decide.\\nFor decades of using the terms programming languages, we thought that they were\\nlanguages to communicate our ideas to the machine, so it can run our programs. We were\\nwrong. That\\'s not the truth, but part of the truth. The real language behind programming\\nlanguages is to communicate our ideas to other developers.\\nHere is where the true nature of clean code lies. It depends on other engineers to be able to\\nread and maintain the code. Therefore, we, as professionals, are the only ones who can\\njudge this. Think about it; as developers, we spend much more time reading code than\\nactually writing it. Every time we want to make a change or add a new feature, we first\\nhave to read all the surroundings of the code we have to modify or extend. The language\\n(Python), is what we use to communicate among ourselves.\\nSo, instead of giving you a definition (or my definition) of clean code, I invite you to go\\nthrough the book, read all about idiomatic Python, see the difference between good and\\nbad code, identify traits of good code and good architecture, and then come up with your\\nown definition. After reading this book, you will be able to judge and analyze code for\\nyourself, and you will have a more clear understanding of clean code. You will know what\\nit is and what it means, regardless of any definition given to you.\\n\\nThe importance of having clean code\\nThere are a huge number of reasons why clean code is important. Most of them revolve\\naround the ideas of maintainability, reducing technical debt, working effectively with agile\\ndevelopment, and managing a successful project.\\n\\n[8]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nThe first idea I would like to explore is in regards to agile development and continuous\\ndelivery. If we want our project to be able to successfully deliver features constantly at a\\nsteady and predictable pace, then having a good and maintainable code base is a must.\\nImagine you are driving a car on a road toward a destination you want to reach at a certain\\npoint in time. You have to estimate your arrival time so that you can tell the person who is\\nwaiting for you. If the car works fine, and the road is flat and perfect, then I do not see why\\nyou would miss your estimation by a large margin. Now, if the road is broken and you\\nhave to step out to move rocks out of the way, or avoid cracks, stop to check the engine\\nevery few kilometers, and so on, then it is very unlikely that you will know for sure when\\nare you going to arrive (or if you are). I think the analogy is clear; the road is the code. If\\nyou want to move at a steady, constant, and predictable pace, the code needs to be\\nmaintainable and readable. If it is not, every time product management asks for a new\\nfeature, you will have to stop to refactor and fix the technical debt.\\nTechnical debt refers to the concept of problems in the software as a result of a compromise,\\nand a bad decision being made. In a way, it\\'s possible to think about technical debt in two\\nways. From the present to the past. What if the problems we are currently facing are the\\nresult of previously written bad code? From the present to the future\\xe2\\x80\\x94if we decide to take\\nthe shortcut now, instead of investing time in a proper solution, what problems are we\\ncreating for ourselves in the future?\\nThe word debt is a good choice. It\\'s a debt because the code will be harder to change in the\\nfuture than it would be to change it now. That incurred cost is the interests of the debt.\\nIncurring in technical debt means that tomorrow, the code will be harder and more\\nexpensive (it would be possible to even measure this) than today, and even more expensive\\nthe day after, and so on.\\nEvery time the team cannot deliver something on time and has to stop to fix and refactor\\nthe code is paying the price of technical debt.\\nThe worst thing about technical debt is that it represents a long-term and underlying\\nproblem. It is not something that raises a high alarm. Instead, it is a silent problem,\\nscattered across all parts of the project, that one day, at one particular time, will wake up\\nand become a show-stopper.\\n\\nThe role of code formatting in clean code\\nIs clean code about formatting and structuring the code, according to some standards (for\\nexample, PEP-8, or a custom standard defined by the project guidelines)? The short answer\\nis no.\\n\\n[9]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nClean code is something else that goes way beyond coding standards, formatting, linting\\ntools, and other checks regarding the layout of the code. Clean code is about achieving\\nquality software and building a system that is robust, maintainable, and avoiding technical\\ndebt. A piece of code or an entire software component could be 100% with PEP-8 (or any\\nother guideline), and still not satisfy these requirements.\\nHowever, not paying attention to the structure of the code has some perils. For this reason,\\nwe will first analyze the problems with a bad code structure, how to address them, and\\nthen we will see how to configure and use tools for Python projects in order to\\nautomatically check and correct problems.\\nTo sum this up, we can say that clean code has nothing to do with things like PEP-8 or\\ncoding styles. It goes way beyond that, and it means something more meaningful to the\\nmaintainability of the code and the quality of the software. However, as we will see,\\nformatting the code correctly is important in order to work efficiently.\\n\\nAdhering to a coding style guide on your project\\nA coding guideline is a bare minimum a project should have to be considered being\\ndeveloped under quality standards. In this section, we will explore the reasons behind this,\\nso in the following sections, we can start looking at ways to enforce this automatically by\\nthe means of tools.\\nThe first thing that comes to my mind when I try to find good traits in a code layout is\\nconsistency. I would expect the code to be consistently structured so that it is easier to read\\nand follow. If the code is not correct or consistently structured, and everyone on the team is\\ndoing things in their own way, then we will end up with code that will require extra effort\\nand concentration to be followed correctly. It will be error-prone, misleading, and bugs or\\nsubtleties might slip through easily.\\nWe want to avoid that. What we want is exactly the opposite of that\\xe2\\x80\\x94code that we can read\\nand understand as quickly as possible at a single glance.\\nIf all members of the development team agree on a standardized way of structuring the\\ncode, the resulting code would look much more familiar. As a result of that, you will\\nquickly identify patterns (more about this in a second), and with these patterns in mind, it\\nwill be much easier to understand things and detect errors. For example, when something\\nis amiss, you will notice that somehow, there is something odd in the patterns you are used\\nto seeing, which will catch your attention. You will take a closer look, and you will more\\nthan likely spot the mistake!\\n\\n[ 10 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nAs it was stated in the classical book, Code Complete, an interesting analysis of this was done\\non the paper titled Perceptions in Chess (1973), where an experiment was conducted in order\\nto identify how different people can understand or memorize different chess positions. The\\nexperiment was conducted on players of all levels (novices, intermediate, and chess\\nmasters), and with different chess positions on the board. They found out that when the\\nposition was random, the novices did as well as the chess masters; it was just a\\nmemorization exercise that anyone could do at reasonably the same level. When the\\npositions followed a logical sequence that might occur in a real game (again, consistency,\\nadhering to a pattern), then the chess masters performed exceedingly better than the rest.\\nNow imagine this same situation applied to software. We, as the software engineers experts\\nin Python, are like the chess masters in the previous example. When the code is structured\\nrandomly, without following any logic, or adhering to any standard, then it would be as\\ndifficult for us to spot mistakes as a novice developer. On the other hand, if we are used to\\nreading code in a structured fashion, and we have learned to quickly get the ideas from the\\ncode by following patterns, then we are at a considerable advantage.\\nIn particular, for Python, the sort of coding style you should follow is PEP-8. You can\\nextend it or adopt some of its parts to the particularities of the project you are working on\\n(for example, the length of the line, the notes about strings, and so on). However, I do\\nsuggest that regardless of whether you are using just plain PEP-8 or extending it, you\\nshould really stick to it instead of trying to come up with another different standard from\\nscratch.\\nThe reason for this is that this document already takes into consideration many of the\\nparticularities of the syntax of Python (that would not normally apply for other languages),\\nand it was created by core Python developers who actually contributed to the syntax of\\nPython. For this reason, it is hard to think that the accuracy of PEP-8 can be otherwise\\nmatched, not to mention, improved.\\nIn particular, PEP-8 has some characteristics that carry other nice improvements when\\ndealing with code, such as following:\\nGrepability: This is the ability to grep tokens inside the code; that is, to search in\\ncertain files (and in which part of those files) for the particular string we are\\nlooking for. One of the items introduced by this standard is something that\\ndifferentiates the way of writing the assignment of values to variables, from the\\nkeyword arguments being passed to functions.\\n\\n[ 11 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nTo see this better, let\\'s use an example. Let\\'s say we are debugging, and we need\\nto find where the value to a parameter named location is being passed. We can\\nrun the following grep command, and the result will tell us the file and the line\\nwe are looking for:\\n$ grep -nr \"location=\" .\\n./core.py:13: location=current_location,\\n\\nNow, we want to know where this variable is being assigned this value, and the\\nfollowing command will also give us the information we are looking for:\\n$ grep -nr \"location =\" .\\n./core.py:10: current_location = get_location()\\n\\nPEP-8 establishes the convention that, when passing arguments by keyword to a\\nfunction, we don\\'t use spaces, but we do when we assign variables. For that\\nreason, we can adapt our search criteria (no spaces around the = on the first\\nsearch, and one space on the second) and be more efficient in our search. That is\\none of the advantages of following a convention.\\nConsistency: If the code looks like a uniform format, the reading of it will be\\nmuch easier. This is particularly important for onboarding, if you want to\\nwelcome new developers to your project, or even hire new (and probably less\\nexperienced) programmers on your team, and they need to become familiar with\\nthe code (which might even consist of several repositories). It will make their\\nlives much easier if the code layout, documentation, naming convention, and\\nsuch is identical across all files they open, in all repositories.\\nCode quality: By looking at the code in a structured fashion, you will become\\nmore proficient at understanding it at a glance (again, like in Perception in Chess),\\nand you will spot bugs and mistakes more easily. In addition to that, tools that\\ncheck for the quality of the code will also hint at potential bugs. Static analysis of\\nthe code might help to reduce the ratio of bugs per line of code.\\n\\nDocstrings and annotations\\nThis section is about documenting the code in Python, from within the code. Good code is\\nself-explanatory but is also well-documented. It is a good idea to explain what it is\\nsupposed to do (not how).\\n\\n[ 12 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nOne important distinction; documenting the code is not the same as adding comments on it.\\nComments are bad, and they should be avoided. By documentation, we refer to the fact of\\nexplaining the data types, providing examples of them, and annotating the variables.\\nThis is relevant in Python, because being dynamically typed, it might be easy to get lost on\\nthe values of variables or objects across functions and methods. For this reason, stating this\\ninformation will make it easier for future readers of the code.\\nThere is another reason that specifically relates to annotations. They can also help in\\nrunning some automatic checks, such as type hinting, through tools such as Mypy. We will\\nfind that, in the end, adding annotations pays off.\\n\\nDocstrings\\nIn simple terms, we can say that docstrings are basically documentation embedded in the\\nsource code. A docstring is basically a literal string, placed somewhere in the code, with the\\nintention of documenting that part of the logic.\\nNotice the emphasis on the word documentation. This subtlety is important because it\\'s\\nmeant to represent explanation, not justification. Docstrings are not comments; they are\\ndocumentation.\\nHaving comments in the code is a bad practice for multiple reasons. First, comments\\nrepresent our failure to express our ideas in the code. If we actually have to explain why or\\nhow we are doing something, then that code is probably not good enough. For starters, it\\nfails to be self-explanatory. Second, it can be misleading. Worst than having to spend some\\ntime reading a complicated section is to read a comment on how it is supposed to work,\\nand figuring out that the code actually does something different. People tend to forget to\\nupdate comments when they change the code, so the comment next to the line that was just\\nchanged will be outdated, resulting in a dangerous misdirection.\\nSometimes, on rare occasions, we cannot avoid having comments. Maybe there is an error\\non a third-party library that we have to circumvent. In those cases, placing a small but\\ndescriptive comment might be acceptable.\\nWith docstrings, however, the story is different. Again, they do not represent comments,\\nbut the documentation of a particular component (a module, class, method, or function) in\\nthe code. Their use is not only accepted but also encouraged. It is a good practice to add\\ndocstrings whenever possible.\\n\\n[ 13 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nThe reason why they are a good thing to have in the code (or maybe even required,\\ndepending on the standards of your project) is that Python is dynamically typed. This\\nmeans that, for example, a function can take anything as the value for any of its parameters.\\nPython will not enforce, nor check, anything like this. So, imagine that you find a function\\nin the code that you know you will have to modify. You are even lucky enough that the\\nfunction has a descriptive name, and that its parameters do as well. It might still not be\\nquite clear what types you should pass to it. Even if this is the case, how are they expected\\nto be used?\\nHere is where a good docstring might be of help. Documenting the expected input and\\noutput of a function is a good practice that will help the readers of that function understand\\nhow it is supposed to work.\\nConsider this good example from the standard library:\\nIn [1]: dict.update??\\nDocstring:\\nD.update([E, ]**F) -> None. Update D from dict/iterable E and F.\\nIf E is present and has a .keys() method, then does: for k in E: D[k] =\\nE[k]\\nIf E is present and lacks a .keys() method, then does: for k, v in E: D[k]\\n= v\\nIn either case, this is followed by: for k in F: D[k] = F[k]\\nType: method_descriptor\\n\\nHere, the docstring for the update method on dictionaries gives us useful information, and\\nit is telling us that we can use it in different ways:\\n1. We can pass something with a .keys() method (for example, another\\ndictionary), and it will update the original dictionary with the keys from the\\nobject passed per parameter:\\n>>>\\n>>>\\n>>>\\n{1:\\n\\nd = {}\\nd.update({1: \"one\", 2: \"two\"})\\nd\\n\\'one\\', 2: \\'two\\'}\\n\\n2. We can pass an iterable of pairs of keys and values, and we will unpack them to\\nupdate:\\n>>> d.update([(3, \"three\"), (4, \"four\")])\\n>>> d\\n{1: \\'one\\', 2: \\'two\\', 3: \\'three\\', 4: \\'four\\'}\\n\\n[ 14 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nIn any case, the dictionary will be updated with the rest of the keyword arguments passed\\nto it.\\nThis information is crucial for someone that has to learn and understand how a new\\nfunction works, and how they can take advantage of it.\\nNotice that in the first example, we obtained the docstring of the function by using the\\ndouble question mark on it (dict.update??). This is a feature of the IPython interactive\\ninterpreter. When this is called, it will print the docstring of the object you are expecting.\\nNow, imagine that in the same way, we obtained help from this function of the standard\\nlibrary; how much easier could you make the lives of your readers (the users of your code),\\nif you place docstrings on the functions you write so that others can understand their\\nworkings in the same way?\\nThe docstring is not something separated or isolated from the code. It becomes part of the\\ncode, and you can access it. When an object has a docstring defined, this becomes part of it\\nvia its __doc__ attribute:\\n>>> def my_function():\\n... \"\"\"Run some computation\"\"\"\\n... return None\\n...\\n>>> my_function.__doc__\\n\\'Run some computation\\'\\n\\nThis means that it is even possible to access it at runtime and even generate or compile\\ndocumentation from the source code. In fact, there are tools for that. If you run Sphinx, it\\nwill create the basic scaffold for the documentation of your project. With the autodoc\\nextension (sphinx.ext.autodoc) in particular, the tool will take the docstrings from the\\ncode and place them in the pages that document the function.\\nOnce you have the tools in place to build the documentation, make it public so that it\\nbecomes part of the project itself. For open source projects, you can use read the docs,\\nwhich will generate the documentation automatically per branch or version (configurable).\\nFor companies or projects, you can have the same tools or configure these services onpremise, but regardless of this decision, the important part is that the documentation\\nshould be ready and available to all members of the team.\\nThere is, unfortunately, one downside to docstrings, and it is that, as it happens with all\\ndocumentation, it requires manual and constant maintenance. As the code changes, it will\\nhave to be updated. Another problem is that for docstrings to be really useful, they have to\\nbe detailed, which requires multiple lines.\\n\\n[ 15 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nMaintaining proper documentation is a software engineering challenge that we cannot\\nescape from. It also makes sense to be like this. If you think about it, the reason for\\ndocumentation to be manually written is because it is intended to be read by other humans.\\nIf it were automated, it would probably not be of much use. For the documentation to be of\\nany value, everyone on the team must agree that it is something that requires manual\\nintervention, hence the effort required. The key is to understand that software is not just\\nabout code. The documentation that comes with it is also part of the deliverable. Therefore,\\nwhen someone is making a change on a function, it is equally important to also update the\\ncorresponding part of the documentation to the code that was just changed, regardless of\\nwhether its a wiki, a user manual, a README file, or several docstrings.\\n\\nAnnotations\\nPEP-3107 introduced the concept of annotations. The basic idea of them is to hint to the\\nreaders of the code about what to expect as values of arguments in functions. The use of the\\nword hint is not casual; annotations enable type hinting, which we will discuss later on in\\nthis chapter, after the first introduction to annotations.\\nAnnotations let you specify the expected type of some variables that have been defined. It is\\nactually not only about the types, but any kind of metadata that can help you get a better\\nidea of what that variable actually represents.\\nConsider the following example:\\nclass Point:\\ndef __init__(self, lat, long):\\nself.lat = lat\\nself.long = long\\n\\ndef locate(latitude: float, longitude: float) -> Point:\\n\"\"\"Find an object in the map by its coordinates\"\"\"\\n\\nHere, we use float to indicate the expected types of latitude and longitude. This is\\nmerely informative for the reader of the function so that they can get an idea of these\\nexpected types. Python will not check these types nor enforce them.\\nWe can also specify the expected type of the returned value of the function. In this case,\\nPoint is a user-defined class, so it will mean that whatever is returned will be an instance\\nof Point.\\n\\n[ 16 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nHowever, types or built-ins are not the only kind of thing we can use as annotations.\\nBasically, everything that is valid in the scope of the current Python interpreter could be\\nplaced there. For example, a string explaining the intention of the variable, a callable to be\\nused as a callback or validation function, and so on.\\nWith the introduction of annotations, a new special attribute is also included, and it is\\n__annotations__. This will give us access to a dictionary that maps the name of the\\nannotations (as keys in the dictionary) with their corresponding values, which are those we\\nhave defined for them. In our example, this will look like the following:\\n>>> locate.__annotations__\\n{\\'latitude\\': float, \\'longitue\\': float, \\'return\\': __main__.Point}\\n\\nWe could use this to generate documentation, run validations, or enforce checks in our code\\nif we think we have to.\\nSpeaking of checking the code through annotations, this is when PEP-484 comes into play.\\nThis PEP specifies the basics of type hinting; the idea of checking the types of our functions\\nvia annotations. Just to be clear again, and quoting PEP-484 itself:\\n\"Python will remain a dynamically typed language, and the authors have no desire to ever\\nmake type hints mandatory, even by convention.\"\\nThe idea of type hinting is to have extra tools (independent from the interpreter) to check\\nand assess the correct use of types throughout the code and to hint to the user in case any\\nincompatibilities are detected. The tool that runs these checks, Mypy, is explained in more\\ndetail in a later section, where we will talk about using and configuring the tools for the\\nproject. For now, you can think of it as a sort of linter that will check the semantics of the\\ntypes used on the code. This sometimes helps in finding bugs early on, when the tests and\\nchecks are run. For this reason, it is a good idea to configure Mypy on the project and use it\\nat the same level as the rest of the tools for static analysis.\\nHowever, type hinting means more than just a tool for checking the types on the code.\\nStarting with Python 3.5, the new typing module was introduced, and this significantly\\nimproved how we define the types and the annotations in our Python code.\\nThe basic idea behind this is that now the semantics extend to more meaningful concepts,\\nmaking it even easier for us (humans) to understand what the code means, or what is\\nexpected at a given point. For example, you could have a function that worked with lists or\\ntuples in one of its parameters, and you would have put one of these two types as the\\nannotation, or even a string explaining it. But with this module, it is possible to tell Python\\nthat it expects an iterable or a sequence. You can even identify the type or the values on it;\\nfor example, that it takes a sequence of integers.\\n\\n[ 17 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nThere is one extra improvement made in regards to annotations at the time of writing this\\nbook, and that is that starting from Python 3.6, it is possible to annotate variables directly,\\nnot just function parameters and return types. This was introduced in PEP-526, and the\\nidea is that you can declare the types of some variables defined without necessarily\\nassigning a value to them, as shown in the following listing:\\nclass Point:\\nlat: float\\nlong: float\\n>>> Point.__annotations__\\n{\\'lat\\': <class \\'float\\'>, \\'long\\': <class \\'float\\'>}\\n\\nDo annotations replace docstrings?\\nThis is a valid question, since on older versions of Python, long before annotations were\\nintroduced, the way of documenting the types of the parameters of functions or attributes\\nwas done by putting docstrings on them. There are even some conventions on formats on\\nhow to structure docstrings to include the basic information for a function, including types\\nand meaning of each parameter, type, and meaning of the result, and possible exceptions\\nthat the function might raise.\\nMost of this has been addressed already in a more compact way by means of annotations,\\nso one might wonder if it is really worth having docstrings as well. The answer is yes, and\\nthis is because they complement each other.\\nIt is true that a part of the information previously contained on the docstring can now be\\nmoved to the annotations. But this should only leave more room for a better documentation\\non the docstring. In particular, for dynamic and nested data types, it is always a good idea\\nto provide examples of the expected data so that we can get a better idea of what we are\\ndealing with.\\nConsider the following example. Let\\'s say we have a function that expects a dictionary to\\nvalidate some data:\\ndef data_from_response(response: dict) -> dict:\\nif response[\"status\"] != 200:\\nraise ValueError\\nreturn {\"data\": response[\"payload\"]}\\n\\n[ 18 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nHere, we can see a function that takes a dictionary and returns another dictionary.\\nPotentially, it can raise an exception if the value under the key \"status\" is not the\\nexpected one. However, we do not have much more information about it. For example,\\nwhat does a correct instance of a response object look like? What would an instance\\nof result look like? To answer both of these questions, it would be a good idea to\\ndocument examples of the data that is expected to be passed in by a parameter and\\nreturned by this function.\\nLet\\'s see if we can explain this better with the help of a docstring:\\ndef data_from_response(response: dict) -> dict:\\n\"\"\"If the response is OK, return its payload.\\n- response: A dict like::\\n{\\n\"status\": 200, # <int>\\n\"timestamp\": \"....\", # ISO format string of the current\\ndate time\\n\"payload\": { ... } # dict with the returned data\\n}\\n- Returns a dictionary like::\\n{\"data\": { .. } }\\n- Raises:\\n- ValueError if the HTTP status is != 200\\n\"\"\"\\nif response[\"status\"] != 200:\\nraise ValueError\\nreturn {\"data\": response[\"payload\"]}\\n\\nNow, we have a better idea of what is expected to be received and returned by this\\nfunction. The documentation serves as valuable input, not only for understanding and\\ngetting an idea of what is being passed around, but also as a valuable source for unit tests.\\nWe can derive data like this to use as input, and we know what would be the correct and\\nincorrect values to use on the tests. Actually, the tests also work as actionable\\ndocumentation for our code, but this will be explained in more detail.\\nThe benefit is that now we know what the possible values of the keys are, as well as their\\ntypes, and we have a more concrete interpretation of what the data looks like. The cost is\\nthat, as we mentioned earlier, it takes up a lot of lines, and it needs to be verbose and\\ndetailed to be effective.\\n\\n[ 19 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nConfiguring the tools for enforcing basic quality\\ngates\\nIn this section, we will explore how to configure some basic tools and automatically run\\nchecks on the code, with the goal of leveraging part of the repetitive verification checks.\\nThis is an important point: remember that code is for us, people, to understand, so only we\\ncan determine what is good or bad code. We should invest time in code reviews, thinking\\nabout what good code is, and how readable and understandable it is. When looking at the\\ncode written by a peer, you should ask such questions:\\nIs this code easy to understand and follow for a fellow programmer?\\nDoes it speak in terms of the domain of the problem?\\nWould a new person joining the team be able to understand it and work with it\\neffectively?\\nAs we saw previously, code formatting, consistent layout, and proper indentation are\\nrequired but not sufficient traits to have in a code base. Moreover, this is something that\\nwe, as engineers with a high sense of quality, would take for granted, so we would read\\nand write code far beyond the basic concepts of its layout. Therefore, we are not willing to\\nwaste time reviewing these kinds of items, so we can invest our time more effectively by\\nlooking at actual patterns in the code in order to understand its true meaning and provide\\nvaluable results.\\nAll of these checks should be automated. They should be part of the tests or checklist, and\\nthis, in turn, should be part of the continuous integration build. If these checks do not pass,\\nmake the build fail. This is the only way to actually ensure the continuity of the structure of\\nthe code at all times. It also serves as an objective parameter for the team to have as a\\nreference. Instead of having some engineers or the leader of the team always having to tell\\nthe same comments about PEP-8 on code reviews, the build will automatically fail, making\\nit something objective.\\n\\nType hinting with Mypy\\nMypy (http:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bmypy-\\xe2\\x80\\x8blang.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b) is the main tool for optional static type checking in\\nPython. The idea is that, once you install it, it will analyze all of the files on your project,\\nchecking for inconsistencies on the use of the types. This is useful since, most of the time, it\\nwill detect actual bugs early, but sometimes it can give false positives.\\n\\n[ 20 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nYou can install it with pip, and it is recommended to include it as a dependency for the\\nproject on the setup file:\\n$ pip install mypy\\n\\nOnce it is installed in the virtual environment, you just have to run the preceding command\\nand it will report all of the findings on the type checks. Try to adhere to its report as much\\nas possible, because most of the time, the insights provided by it help to avoid errors that\\nmight otherwise slip into production. However, the tool is not perfect, so if you think it is\\nreporting a false positive, you can ignore that line with the following marker as a comment:\\ntype_to_ignore = \"something\" # type: ignore\\n\\nChecking the code with Pylint\\nThere are many tools for checking the structure of the code (basically, this is compliance\\nwith PEP-8) in Python, such as pycodestyle (formerly known as PEP-8), Flake8, and many\\nmore. They all are configurable and are as easy to use as running the command they\\nprovide. Among all of them, I have found Pylint to be the most complete (and strict). It is\\nalso configurable.\\nAgain, you just have to install it in the virtual environment with pip:\\n$ pip install pylint\\n\\nThen, just running the pylint command would be enough to check it in the code.\\nIt is possible to configure Pylint via a configuration file named pylintrc.\\nIn this file, you can decide the rules you would like to enable or disable, and parametrize\\nothers (for example, to change the maximum length of the column).\\n\\nSetup for automatic checks\\nOn Unix development environments, the most common way of working is through\\nmakefiles. Makefiles are powerful tools that let us configure commands to be run in the\\nproject, mostly for compiling, running, and so on. Besides this, we can use a makefile in the\\nroot of our project, with some commands configured to run checks of the formatting and\\nconventions on the code, automatically.\\n\\n[ 21 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nA good approach for this would be to have targets for the tests, and each particular test,\\nand then have another one that will run altogether. For example:\\ntypehint:\\nmypy src/ tests/\\ntest:\\npytest tests/\\nlint:\\npylint src/ tests/\\nchecklist: lint typehint test\\n.PHONY: typehint test lint checklist\\n\\nHere, the command we should run (both in our development machines and in the\\ncontinuous integration environment builds) is the following:\\nmake checklist\\n\\nThis will run everything in the following steps:\\n1. It will first check the compliance with the coding guideline (PEP-8, for instance)\\n2. Then it will check for the use of types on the code\\n3. Finally, it will run the tests\\nIf any of these steps fail, consider the entire process a failure.\\nBesides configuring these checks automatically in the build, it is also a good idea if the team\\nadopts a convention and an automatic approach for structuring the code. Tools such as\\nBlack (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bgithub.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bambv/\\xe2\\x80\\x8bblack) automatically format the code. There are many\\ntools that will edit the code automatically, but the interesting thing about Black is that it\\ndoes so in a unique form. It\\'s opinionated and deterministic, so the code will always end up\\narranged in the same way.\\nFor example, Black strings will always be double-quotes, and the order of the parameters\\nwill always follow the same structure. This might sound rigid, but it\\'s the only way to\\nensure the differences in the code are minimal. If the code always respects the same\\nstructure, changes in the code will only show up in pull requests with the actual changes\\nthat were made, and no extra cosmetic modifications. It\\'s more restrictive than PEP-8, but\\nit\\'s also convenient because, by formatting the code directly through a tool, we don\\'t have\\nto actually worry about that, and we can focus on the crux of the problem at hand.\\n\\n[ 22 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nAt the time of writing this book, the only thing that can be configured is the length of the\\nlines. Everything else is corrected by the criteria of the project.\\nThe following code is PEP-8 correct, but it doesn\\'t follow the conventions of black:\\ndef my_function(name):\\n\"\"\"\\n>>> my_function(\\'black\\')\\n\\'received Black\\'\\n\"\"\"\\nreturn \\'received {0}\\'.format(name.title())\\n\\nNow, we can run the following command to format the file:\\nblack -l 79 *.py\\n\\nNow, we can see what the tool has written:\\ndef my_function(name):\\n\"\"\"\\n>>> my_function(\\'black\\')\\n\\'received Black\\'\\n\"\"\"\\nreturn \"received {0}\".format(name.title())\\n\\nOn more complex code, a lot more would have changed (trailing commas, and more), but\\nthe idea can be seen clearly. Again, it\\'s opinionated, but it\\'s also a good idea to have a tool\\nthat takes care of details for us. It\\'s also something that the Golang community learned a\\nlong time ago, to the point that there is a standard tool library, got fmt, that automatically\\nformats the code according to the conventions of the language. It\\'s good that Python has\\nsomething like this now.\\nThese tools (Black, Pylint, Mypy, and many more) can be integrated with the editor or IDE\\nof your choice to make things even easier. It\\'s a good investment to configure your editor to\\nmake these kinds of modifications either when saving the file or through a shortcut.\\n\\n[ 23 ]\\n\\n\\x0cIntroduction, Code Formatting, and Tools\\n\\nChapter 1\\n\\nSummary\\nWe now have a first idea of what clean code is, and a workable interpretation of it, which\\nwill serve us as a reference point for the rest of this book.\\nMore importantly, we understood that clean code is something much more important than\\nthe structure and layout of the code. We have to focus on how the ideas are represented on\\nthe code to see if they are correct. Clean code is about readability, maintainability of the\\ncode, keeping technical debt to the minimum, and effectively communicating our ideas into\\nthe code so that others can understand the same thing we intended to write in the first\\nplace.\\nHowever, we discussed that the adherence to coding styles or guidelines is important for\\nmultiple reasons. We have agreed that this is a condition that is necessary, but not\\nsufficient, and since it is a minimal requirement every solid project should comply with, it\\nis clear that is something we better leave to the tools. Therefore, automating all of these\\nchecks becomes critical, and in this regard, we have to keep in mind how to configure tools\\nsuch as Mypy, Pylint, and more.\\nThe next chapter is going to be more focused on the Python-specific code, and how to\\nexpress our ideas in idiomatic Python. We will explore the idioms in Python that make for\\nmore compact and efficient code. In this analysis, we will see that, in general, Python has\\ndifferent ideas or different ways to accomplish things compared to other languages.\\n\\n[ 24 ]\\n\\n\\x0c2\\nPythonic Code\\nIn this chapter, we will explore the way ideas are expressed in Python, with its own\\nparticularities. If you are familiar with the standard ways of accomplishing some tasks in\\nprogramming (such as getting the last element of a list, iterating, searching, and so on), or if\\nyou come from more traditional programming languages (like C, C++, and Java), then you\\nwill find that, in general, Python provides its own mechanism for most common tasks.\\nIn programming, an idiom is a particular way of writing code in order to perform a specific\\ntask. It is something common that repeats and follows the same structure every time. Some\\ncould even argue and call them a pattern, but be careful because they are not designed\\npatterns (which we will explore later on). The main difference is that design patterns are\\nhigh-level ideas, independent from the language (sort of), but they do not translate into\\ncode immediately. On the other hand, idioms are actually coded. It is the way things should\\nbe written when we want to perform a particular task.\\nAs idioms are code, they are language dependent. Every language will have its own\\nidioms, which means the way things are done in that particular language (for example, how\\nyou would open and write a file in C, C++, and so on). When the code follows these idioms,\\nit is known as being idiomatic, which in Python is often referred to as Pythonic.\\nThere are multiple reasons to follow these recommendations and write Pythonic code first\\n(as we will see and analyze), writing code in an idiomatic way usually performs better. It is\\nalso more compact and easier to understand. These are traits that we always want in our\\ncode so that it works effectively. Secondly, as introduced in the previous chapter, it is\\nimportant that the entire development team can get used to the same patterns and structure\\nof the code because this will help them focus on the true essence of the problem, and will\\nhelp them avoid making mistakes.\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThe goals of this chapter are as follows:\\nTo understand indices and slices, and correctly implement objects that can be\\nindexed\\nTo implement sequences and other iterables\\nTo learn good use cases for context managers\\nTo implement more idiomatic code through magic methods\\nTo avoid common mistakes in Python that lead to undesired side-effects\\n\\nIndexes and slices\\nIn Python, as in other languages, some data structures or types support accessing its\\nelements by index. Another thing it has in common with most programming languages is\\nthat the first element is placed in the index number zero. However, unlike those languages,\\nwhen we want to access the elements in a different order than usual, Python provides extra\\nfeatures.\\nFor example, how would you access the last element of an array in C? This is something I\\ndid the first time I tried Python. Thinking the same way as in C, I would get the element in\\nthe position of the length of the array minus one. This could work, but we could also use a\\nnegative index number, which will start counting from the last, as shown in the following\\ncommands:\\n>>> my_numbers = (4, 5, 3, 9)\\n>>> my_numbers[-1]\\n9\\n>>> my_numbers[-3]\\n5\\n\\nIn addition to getting just one element, we can obtain many by using slice, as shown in\\nthe following commands:\\n>>> my_numbers = (1, 1, 2, 3, 5, 8, 13, 21)\\n>>> my_numbers[2:5]\\n(2, 3, 5)\\n\\nIn this case, the syntax on the square brackets means that we get all of the elements on the\\ntuple, starting from the index of the first number (inclusive), up to the index on the second\\none (not including it). Slices work this way in Python by excluding the end of the selected\\ninterval.\\n\\n[ 26 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nYou can exclude either one of the intervals, start or stop, and in that case, it will act from\\nthe beginning or end of the sequence, respectively, as shown in the following commands:\\n>>>\\n(1,\\n>>>\\n(3,\\n>>>\\n(1,\\n>>>\\n(1,\\n\\nmy_numbers[:3]\\n1, 2)\\nmy_numbers[3:]\\n5, 8, 13, 21)\\nmy_numbers[::]\\n1, 2, 3, 5, 8, 13, 21)\\nmy_numbers[1:7:2]\\n3, 8)\\n\\nIn the first example, it will get everything up to the index in the position number 3. In the\\nsecond example, it will get all the numbers from the position 3 (inclusive), up to the end. In\\nthe second to last example, where both ends are excluded, it is actually creating a copy of\\nthe original tuple.\\nThe last example includes a third parameter, which is the step. This indicates how many\\nelements to jump when iterating over the interval. In this case, it would mean to get the\\nelements between the positions one and seven, jumping by two.\\nIn all of these cases, when we pass intervals to a sequence, what is actually happening is\\nthat we are passing slice. Note that slice is a built-in object in Python that you can build\\nyourself and pass directly:\\n>>> interval = slice(1, 7, 2)\\n>>> my_numbers[interval]\\n(1, 3, 8)\\n>>> interval = slice(None, 3)\\n>>> my_numbers[interval] == my_numbers[:3]\\nTrue\\n\\nNotice that when one of the elements is missing (start, stop, or step), it is considered to be\\nnone.\\nYou should always prefer to use this built-in syntax for slices, as opposed\\nto manually trying to iterate the tuple, string, or list inside a for loop,\\nexcluding the elements by hand.\\n\\n[ 27 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nCreating your own sequences\\nThe functionality we just discussed works thanks to a magic method called __getitem__.\\nThis is the method that is called, when something like myobject[key] is called, passing\\nthe key (value inside the square brackets) as a parameter. A sequence, in particular, is an\\nobject that implements both __getitem__ and __len__, and for this reason, it can be\\niterated over. Lists, tuples, and strings are examples of sequence objects in the standard\\nlibrary.\\nIn this section, we care more about getting particular elements from an object by a key than\\nbuilding sequences or iterable objects, which is a topic explored in Chapter 7, Using\\nGenerators.\\nIf you are going to implement __getitem__ in a custom class in your domain, you will\\nhave to take into account some considerations in order to follow a Pythonic approach.\\nIn the case that your class is a wrapper around a standard library object, you might as well\\ndelegate the behavior as much as possible to the underlying object. This means that if your\\nclass is actually a wrapper on the list, call all of the same methods on that list to make sure\\nthat it remains compatible. In the following listing, we can see an example of how an object\\nwraps a list, and for the methods we are interested in, we just delegate to its corresponding\\nversion on the list object:\\nclass Items:\\ndef __init__(self, *values):\\nself._values = list(values)\\ndef __len__(self):\\nreturn len(self._values)\\ndef __getitem__(self, item):\\nreturn self._values.__getitem__(item)\\n\\nThis example uses encapsulation. Another way of doing it is through inheritance, in which\\ncase we will have to extend the collections.UserList base class, with the\\nconsiderations and caveats mentioned in the last part of this chapter.\\nIf, however, you are implementing your own sequence, that is not a wrapper or does not\\nrely on any built-in object underneath, then keep in mind the following points:\\nWhen indexing by a range, the result should be an instance of the same type of\\nthe class\\nIn the range provided by the slice, respect the semantics that Python uses,\\nexcluding the element at the end\\n\\n[ 28 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThe first point is a subtle error. Think about it\\xe2\\x80\\x94when you get slice of a list, the result is a\\nlist; when you ask for a range in a tuple, the result is a tuple; and when you ask for a\\nsubstring, the result is a string. It makes sense in each case that the result is of the same type\\nof the original object. If you are creating, let\\'s say, an object that represents an interval of\\ndates, and you ask for a range on that interval, it would be a mistake to return a list or\\ntuple, and many more. Instead, it should return a new instance of the same class with the\\nnew interval set. The best example of this is in the standard library, with the range function.\\nPreviously, in Python 2, the range function used to build a list. Now, if you call range\\nwith an interval, it will construct an iterable object that knows how to produce the values in\\nthe selected range. When you specify an interval for range, you get a new range (which\\nmakes sense), not a list:\\n>>> range(1, 100)[25:50]\\nrange(26, 51)\\n\\nThe second rule is also about consistency\\xe2\\x80\\x94users of your code will find it more familiar and\\neasier to use if it is consistent with Python itself. As Python developers, we are already used\\nto the idea of how the slices work, how the range function works, and so on. Making an\\nexception on a custom class will create confusion, which means that it will be harder to\\nremember and it might lead to bugs.\\n\\nContext managers\\nContext managers are a distinctively useful feature that Python provides. The reason why\\nthey are so useful is that they correctly respond to a pattern. The pattern is actually every\\nsituation where we want to run some code, and has preconditions and postconditions,\\nmeaning that we want to run things before and after a certain main action.\\nMost of the time, we see context managers around resource management. For example, on\\nsituations when we open files, we want to make sure that they are closed after processing\\n(so we do not leak file descriptors), or if we open a connection to a service (or even a\\nsocket), we also want to be sure to close it accordingly, or when removing temporary files,\\nand so on.\\n\\n[ 29 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nIn all of these cases, you would normally have to remember to free all of the resources that\\nwere allocated and that is just thinking about the best case\\xe2\\x80\\x94but what about exceptions and\\nerror handling? Given the fact that handling all possible combinations and execution paths\\nof our program makes it harder to debug, the most common way of addressing this issue is\\nto put the cleanup code on a finally block so that we are sure we do not miss it. For\\nexample, a very simple case would look like the following:\\nfd = open(filename)\\ntry:\\nprocess_file(fd)\\nfinally:\\nfd.close()\\n\\nNonetheless, there is a much elegant and Pythonic way of achieving the same thing:\\nwith open(filename) as fd:\\nprocess_file(fd)\\n\\nThe with statement (PEP-343) enters the context manager. In this case, the open function\\nimplements the context manager protocol, which means that the file will be automatically\\nclosed when the block is finished, even if an exception occurred.\\nContext managers consist of two magic methods: __enter__ and __exit__. On the first\\nline of the context manager, the with statement will call the first method, __enter__, and\\nwhatever this method returns will be assigned to the variable labeled after as. This is\\noptional\\xe2\\x80\\x94we don\\'t really need to return anything specific on the __enter__ method, and\\neven if we do, there is still no strict reason to assign it to a variable if it is not required.\\nAfter this line is executed, the code enters a new context, where any other Python code can\\nbe run. After the last statement on that block is finished, the context will be exited, meaning\\nthat Python will call the __exit__ method of the original context manager object we first\\ninvoked.\\nIf there is an exception or error inside the context manager block, the __exit__ method\\nwill still be called, which makes it convenient for safely managing cleaning up conditions.\\nIn fact, this method receives the exception that was triggered on the block in case we want\\nto handle it in a custom fashion.\\nDespite the fact that context managers are very often found when dealing with resources\\n(like the example we mentioned with files, connections, and so on), this is not the sole\\napplication they have. We can implement our own context managers in order to handle the\\nparticular logic we need.\\n\\n[ 30 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nContext managers are a good way of separating concerns and isolating parts of the code\\nthat should be kept independent, because if we mix them, then the logic will become harder\\nto maintain.\\nAs an example, consider a situation where we want to run a backup of our database with a\\nscript. The caveat is that the backup is offline, which means that we can only do it while the\\ndatabase is not running, and for this we have to stop it. After running the backup, we want\\nto make sure that we start the process again, regardless of how the process of the backup\\nitself went. Now, the first approach would be to create a huge monolithic function that tries\\nto do everything in the same place, stop the service, perform the backup task, handle\\nexceptions and all possible edge cases, and then try to restart the service again. You can\\nimagine such a function, and for that reason, I will spare you the details, and instead come\\nup directly with a possible way of tackling this issue with context managers:\\ndef stop_database():\\nrun(\"systemctl stop postgresql.service\")\\n\\ndef start_database():\\nrun(\"systemctl start postgresql.service\")\\n\\nclass DBHandler:\\ndef __enter__(self):\\nstop_database()\\nreturn self\\ndef __exit__(self, exc_type, ex_value, ex_traceback):\\nstart_database()\\n\\ndef db_backup():\\nrun(\"pg_dump database\")\\n\\ndef main():\\nwith DBHandler():\\ndb_backup()\\n\\nIn this example, we don\\'t need the result of the context manager inside the block, and that\\'s\\nwhy we can consider that, at least for this particular case, the return value of __enter__ is\\nirrelevant. This is something to take into consideration when designing context\\nmanagers\\xe2\\x80\\x94what do we need once the block is started? As a general rule, it should be good\\npractice (although not mandatory), to always return something on the __enter__.\\n\\n[ 31 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nIn this block, we only run the task for the backup, independently from the maintenance\\ntasks, as we saw previously. We also mentioned that even if the backup task has an error,\\nthe __exit__ will still be called.\\nNotice the signature of the __exit__ method. It receives the values for the exception that\\nwas raised on the block. If there was no exception on the block, they are all none.\\nThe return value of __exit__ is something to consider. Normally, we would want to leave\\nthe method as it is, without returning anything in particular. If this method returns True, it\\nmeans that the exception that was potentially raised; it will not propagate to the caller and\\nwill stop there. Sometimes, this is the desired effect, maybe even depending on the type of\\nexception that was raised, but in general it is not a good idea to swallow the exception.\\nRemember: errors should never pass silently.\\nKeep in mind not to accidentally return True on the __exit__. If you do,\\nmake sure that this is exactly what you want, and that there is a good\\nreason for it.\\n\\nImplementing context managers\\nIn general, we can implement context managers like the one in the previous example. All\\nwe need is just a class that implements the __enter__ and __exit__ magic methods, and\\nthen that object will be able to support the context manager protocol. While this is the most\\ncommon way for context managers to be implemented, it is not the only one.\\nIn this section, we will see not only different (sometimes more compact) ways of\\nimplementing context managers but also how to take full advantage of them by using the\\nstandard library, in particular with the contextlib module.\\nThe contextlib module contains a lot of helper functions and objects to either implement\\ncontext managers or use some already provided ones that can help us write more compact\\ncode.\\nLet\\'s start by looking at the contextmanager decorator.\\nWhen the contextlib.contextmanager decorator is applied to a function, it converts the\\ncode on that function into a context manager. The function in question has to be a particular\\nkind of function called a generator function, which will separate the statements into what is\\ngoing to be on the __enter__ and __exit__ magic methods, respectively.\\n\\n[ 32 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nIf at this point you are not familiar with decorators and generators, this is not a problem\\nbecause the examples we will be looking at will be self-contained, and the recipe or idiom\\ncan be applied and understood regardless. These topics are discussed in detail in Chapter\\n7, Using Generators.\\nThe equivalent code of the previous example can be rewritten with the contextmanager\\ndecorator like this:\\nimport contextlib\\n@contextlib.contextmanager\\ndef db_handler():\\nstop_database()\\nyield\\nstart_database()\\n\\nwith db_handler():\\ndb_backup()\\n\\nHere, we define the generator function and apply the @contextlib.contextmanager\\ndecorator to it. The function contains a yield statement, which makes it a generator\\nfunction. Again, details on generators are not relevant in this case. All we need to know is\\nthat when this decorator is applied, everything before the yield statement will be run as if\\nit were part of the __enter__ method. Then, the yielded value is going to be the result of\\nthe context manager evaluation (what __enter__ would return), and what would be\\nassigned to the variable if we chose to assign it like as x:\\xe2\\x80\\x94in this case, nothing is yielded\\n(which means the yielded value will be none, implicitly), but if we wanted to, we could\\nyield a statement which will become something we might want to use inside the context\\nmanager block.\\nAt that point, the generator function is suspended, and the context manager is entered,\\nwhere, again, we run the backup code for our database. After this completes, the execution\\nresumes, so we can consider that every line that comes after the yield statement will be\\npart of the __exit__ logic.\\nWriting context managers like this has the advantage that it is easier to refactor existing\\nfunctions, reuse code, and in general is a good idea when we need a context manager that\\ndoesn\\'t belong to any particular object. Adding the extra magic methods would make\\nanother object of our domain more coupled, with more responsibilities, and supporting\\nsomething that it probably shouldn\\'t. When we just need a context manager function,\\nwithout preserving many states, and completely isolated and independent from the rest of\\nour classes, this is probably a good way to go.\\n\\n[ 33 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThere are, however, more ways in which we can implement context manager, and once\\nagain, the answer is in the contextlib package from the standard library.\\nAnother helper we could use is contextlib.ContextDecorator. This is a mixin base\\nclass that provides the logic for applying a decorator to a function that will make it run\\ninside the context manager, while the logic for the context manager itself has to be provided\\nby implementing the aforementioned magic methods.\\nIn order to use it, we have to extend this class and implement the logic on the required\\nmethods:\\nclass dbhandler_decorator(contextlib.ContextDecorator):\\ndef __enter__(self):\\nstop_database()\\ndef __exit__(self, ext_type, ex_value, ex_traceback):\\nstart_database()\\n\\n@dbhandler_decorator()\\ndef offline_backup():\\nrun(\"pg_dump database\")\\n\\nDo you notice something different from the previous examples? There is no with\\nstatement. We just have to call the function, and offline_backup() will automatically\\nrun inside a context manager. This is the logic that the base class provides to use it as a\\ndecorator that wraps the original function so that it runs inside a context manager.\\nThe only downside of this approach is that by the way the objects work, they are\\ncompletely independent (which is a good trait)\\xe2\\x80\\x94the decorator doesn\\'t know anything\\nabout the function that is decorating, and vice versa. This, however good, means that you\\ncannot get an object that you would like to use inside the context manager (for example,\\nassigning with offline_backup() as bp:), so if you really need to use the object\\nreturned by the __exit__ method, one of the previous approaches will have to be the one\\nof choice.\\nBeing a decorator, this also poses the advantage that the logic is defined only once, and we\\ncan reuse it as many times as we want by simply applying the decorators to other functions\\nthat require the same invariant logic.\\nLet\\'s explore one last feature of contextlib, to see what we can expect from context\\nmanagers and get an idea of the sort of thing we could use them for.\\n\\n[ 34 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nNote that contextlib.suppress is a util package that enters a context manager, which,\\nif one of the provided exceptions is raised, doesn\\'t fail. It\\'s similar to running that same\\ncode on a try/except block and passing an exception or logging it, but the difference is\\nthat calling the suppress method makes it more explicit that those exceptions that are\\ncontrolled as part of our logic.\\nFor example, consider the following code:\\nimport contextlib\\nwith contextlib.suppress(DataConversionException):\\nparse_data(input_json_or_dict)\\n\\nHere, the presence of the exception means that the input data is already in the expected\\nformat, so there is no need for conversion, hence making it safe to ignore it.\\n\\nProperties, attributes, and different types of\\nmethods for objects\\nAll of the properties and functions of an object are public in Python, which is different from\\nother languages where properties can be public, private, or protected. That is, there is no\\npoint in preventing caller objects from invoking any attributes an object has. This is another\\ndifference with respect to other programming languages in which you can mark some\\nattributes as private or protected.\\nThere is no strict enforcement, but there are some conventions. An attribute that starts with\\nan underscore is meant to be private to that object, and we expect that no external agent\\ncalls it (but again, there is nothing preventing this).\\nBefore jumping into the details of properties, it\\'s worth mentioning some traits of\\nunderscores in Python, understanding the convention, and the scope of attributes.\\n\\nUnderscores in Python\\nThere are some conventions and implementation details that make use of underscores in\\nPython, which is an interesting topic that\\'s worthy of analysis.\\n\\n[ 35 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nLike we mentioned previously, by default all attributes of an object are public. Consider the\\nfollowing example to illustrate this:\\n>>> class Connector:\\n...\\ndef __init__(self, source):\\n...\\nself.source = source\\n...\\nself._timeout = 60\\n...\\n>>> conn = Connector(\"postgresql://localhost\")\\n>>> conn.source\\n\\'postgresql://localhost\\'\\n>>> conn._timeout\\n60\\n>>> conn.__dict__\\n{\\'source\\': \\'postgresql://localhost\\', \\'_timeout\\': 60}\\n\\nHere, a Connector object is created with source, and it starts with two attributes\\xe2\\x80\\x94the\\naforementioned source and timeout. The former is public, and the latter private.\\nHowever, as we can see from the following lines when we create an object like this, we can\\nactually access both of them.\\nThe interpretation of this code is that _timeout should be accessed only within connector\\nitself and never from a caller. This means that you should organize the code in a way so\\nthat you can safely refactor the timeout at all of the times it\\'s needed, relying on the fact that\\nit\\'s not being called from outside the object (only internally), hence preserving the same\\ninterface as before. Complying with these rules makes the code easier to maintain and more\\nrobust because we don\\'t have to worry about ripple effects when refactoring the code if we\\nmaintain the interface of the object. The same principle applies to methods as well.\\nObjects should only expose those attributes and methods that are relevant\\nto an external caller object, namely, entailing its interface. Everything that\\nis not strictly part of an object\\'s interface should be kept prefixed with a\\nsingle underscore.\\nThis is the Pythonic way of clearly delimiting the interface of an object. There is, however, a\\ncommon misconception that some attributes and methods can be actually made private.\\nThis is, again, a misconception. Let\\'s imagine that now the timeout attribute is defined\\nwith a double underscore instead:\\n>>> class Connector:\\n...\\ndef __init__(self, source):\\n...\\nself.source = source\\n...\\nself.__timeout = 60\\n...\\n...\\ndef connect(self):\\n\\n[ 36 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\n...\\nprint(\"connecting with {0}s\".format(self.__timeout))\\n...\\n# ...\\n...\\n>>> conn = Connector(\"postgresql://localhost\")\\n>>> conn.connect()\\nconnecting with 60s\\n>>> conn.__timeout\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nAttributeError: \\'Connector\\' object has no attribute \\'__timeout\\'\\n\\nSome developers use this method to hide some attributes, thinking, like in this example,\\nthat timeout is now private and that no other object can modify it. Now, take a look at\\nthe exception that is raised when trying to access __timeout. It\\'s AttributeError, saying\\nthat it doesn\\'t exist. It doesn\\'t say something like \"this is private\" or \"this can\\'t\\nbe accessed\" and so on. It says it does not exist. This should give us a clue that, in fact,\\nsomething different is happening and that this behavior is instead just a side effect, but not\\nthe real effect we want.\\nWhat\\'s actually happening is that with the double underscores, Python creates a different\\nname for the attribute (this is called name mangling). What it does is create the attribute\\nwith the following name instead: \"_<class-name>__<attribute-name>\". In this case,\\nan attribute named \\'_Connector__timeout\\', will be created, and such an attribute can be\\naccessed (and modified) as follows:\\n>>> vars(conn)\\n{\\'source\\': \\'postgresql://localhost\\', \\'_Connector__timeout\\': 60}\\n>>> conn._Connector__timeout\\n60\\n>>> conn._Connector__timeout = 30\\n>>> conn.connect()\\nconnecting with 30s\\n\\nNotice the side effect that we mentioned earlier\\xe2\\x80\\x94the attribute only exists with a different\\nname, and for that reason the AttributeError was raised on our first attempt to access it.\\nThe idea of the double underscore in Python is completely different. It was created as a\\nmeans to override different methods of a class that is going to be extended several times,\\nwithout the risk of having collisions with the method names. Even that is a too far-fetched\\nuse case as to justify the use of this mechanism.\\nDouble underscores are a non-Pythonic approach. If you need to define attributes as\\nprivate, use a single underscore, and respect the Pythonic convention that it is a private\\nattribute.\\n\\n[ 37 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nDo not use double underscores.\\n\\nProperties\\nWhen the object needs to just hold values, we can use regular attributes. Sometimes, we\\nmight want to do some computations based on the state of the object and the values of\\nother attributes. Most of the time, properties are a good choice for this.\\nProperties are to be used when we need to define access control to some attributes in an\\nobject, which is another point where Python has its own way of doing things. In other\\nprogramming languages (like Java), you would create access methods (getters and setters),\\nbut idiomatic Python would use properties instead.\\nImagine that we have an application where users can register and we want to protect\\ncertain information about the user from being incorrect, such as their email, as shown in the\\nfollowing code:\\nimport re\\nEMAIL_FORMAT = re.compile(r\"[^@]+@[^@]+\\\\.[^@]+\")\\n\\ndef is_valid_email(potentially_valid_email: str):\\nreturn re.match(EMAIL_FORMAT, potentially_valid_email) is not None\\n\\nclass User:\\ndef __init__(self, username):\\nself.username = username\\nself._email = None\\n@property\\ndef email(self):\\nreturn self._email\\n@email.setter\\ndef email(self, new_email):\\nif not is_valid_email(new_email):\\nraise ValueError(f\"Can\\'t set {new_email} as it\\'s not a\\nvalid email\")\\nself._email = new_email\\n\\n[ 38 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nBy putting email under a property, we obtain some advantages for free. In this example, the\\nfirst @property method will return the value held by the private attribute email. As\\nmentioned earlier, the leading underscore determines that this attribute is intended to be\\nused as private, and therefore should not be accessed from outside this class.\\nThen, the second method uses @email.setter, with the already defined property of the\\nprevious method. This is the one that is going to be called when <user>.email =\\n<new_email> runs from the caller code, and <new_email> will become the parameter of\\nthis method. Here, we explicitly defined a validation that will fail if the value that is trying\\nto be set is not an actual email address. If it is, it will then update the attribute with the new\\nvalue as follows:\\n>>> u1 = User(\"jsmith\")\\n>>> u1.email = \"jsmith@\"\\nTraceback (most recent call last):\\n...\\nValueError: Can\\'t set jsmith@ as it\\'s not a valid email\\n>>> u1.email = \"jsmith@g.co\"\\n>>> u1.email\\n\\'jsmith@g.co\\'\\n\\nThis approach is much more compact than having custom methods prefixed with get_ or\\nset_. It\\'s clear what is expected because it\\'s just email.\\nDon\\'t write custom get_* and set_* methods for all attributes on your\\nobjects. Most of the time, leaving them as regular attributes is just enough.\\nIf you need to modify the logic for when an attribute is retrieved or\\nmodified, then use properties.\\nYou might find that properties are a good way to achieve command and query separation\\n(CC08). Command and query separation state that a method of an object should either\\nanswer to something or do something, but not both. If a method of an object is doing\\nsomething and at the same time it returns a status answering a question of how that\\noperation went, then it\\'s doing more than one thing, clearly violating the principle that\\nfunctions should do one thing, and one thing only.\\nDepending on the name of the method, this can create even more confusion, making it\\nharder for readers to understand what the actual intention of the code is. For example, if a\\nmethod is called set_email, and we use it as if self.set_email(\"a@j.com\"): ...,\\nwhat is that code doing? Is it setting the email to a@j.com? Is it checking if the email is\\nalready set to that value? Both (setting and then checking if the status is correct)?\\n\\n[ 39 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nWith properties, we can avoid this kind of confusion. The @property decorator is the\\nquery that will answer to something, and the @<property_name>.setter is the command\\nthat will do something.\\nAnother piece of good advice derived from this example is as follows\\xe2\\x80\\x94don\\'t do more than\\none thing on a method. If you want to assign something and then check the value, break\\nthat down into two or more sentences.\\nMethods should do one thing only. If you have to run an action and then\\ncheck for the status, so that in separate methods that are called by\\ndifferent statements.\\n\\nIterable objects\\nIn Python, we have objects that can be iterated by default. For example, lists, tuples, sets,\\nand dictionaries can not only hold data in the structure we want but also be iterated over\\na for loop to get those values repeatedly.\\nHowever, the built-in iterable objects are not the only kind that we can have in a for loop.\\nWe could also create our own iterable, with the logic we define for iteration.\\nIn order to achieve this, we rely on, once again, magic methods.\\nIteration works in Python by its own protocol (namely the iteration protocol). When you try\\nto iterate an object in the form for e in myobject:..., what Python checks at a very\\nhigh level are the following two things, in order:\\nIf the object contains one of the iterator methods\\xe2\\x80\\x94__next__ or __iter__\\nIf the object is a sequence and has __len__ and __getitem__\\nTherefore, as a fallback mechanism, sequences can be iterated, and so there are two ways of\\ncustomizing our objects to be able to work on for loops.\\n\\nCreating iterable objects\\nWhen we try to iterate an object, Python will call the iter() function over it. One of the\\nfirst things this function checks for is the presence of the __iter__ method on that object,\\nwhich, if present, will be executed.\\n\\n[ 40 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThe following code creates an object that allows iterating over a range of dates, producing\\none day at a time on every round of the loop:\\nfrom datetime import timedelta\\nclass DateRangeIterable:\\n\"\"\"An iterable that contains its own iterator object.\"\"\"\\ndef __init__(self, start_date, end_date):\\nself.start_date = start_date\\nself.end_date = end_date\\nself._present_day = start_date\\ndef __iter__(self):\\nreturn self\\ndef __next__(self):\\nif self._present_day >= self.end_date:\\nraise StopIteration\\ntoday = self._present_day\\nself._present_day += timedelta(days=1)\\nreturn today\\n\\nThis object is designed to be created with a pair of dates, and when iterated, it will produce\\neach day in the interval of specified dates, which is shown in the following code:\\n>>> for day in DateRangeIterable(date(2018, 1, 1), date(2018, 1, 5)):\\n...\\nprint(day)\\n...\\n2018-01-01\\n2018-01-02\\n2018-01-03\\n2018-01-04\\n>>>\\n\\nHere, the for loop is starting a new iteration over our object. At this point, Python will call\\nthe iter() function on it, which in turn will call the __iter__ magic method. On this\\nmethod, it is defined to return self, indicating that the object is an iterable itself, so at that\\npoint every step of the loop will call the next() function on that object, which delegates to\\nthe __next__ method. In this method, we decide how to produce the elements and return\\none at a time. When there is nothing else to produce, we have to signal this to Python by\\nraising the StopIteration exception.\\n\\n[ 41 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThis means that what is actually happening is similar to Python calling next() every time\\non our object until there is a StopIteration exception, on which it knows it has to stop\\nthe for loop:\\n>>> r = DateRangeIterable(date(2018, 1, 1), date(2018, 1, 5))\\n>>> next(r)\\ndatetime.date(2018, 1, 1)\\n>>> next(r)\\ndatetime.date(2018, 1, 2)\\n>>> next(r)\\ndatetime.date(2018, 1, 3)\\n>>> next(r)\\ndatetime.date(2018, 1, 4)\\n>>> next(r)\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nFile ... __next__\\nraise StopIteration\\nStopIteration\\n>>>\\n\\nThis example works, but it has a small problem\\xe2\\x80\\x94once exhausted, the iterable will continue\\nto be empty, hence raising StopIteration. This means that if we use this on two or more\\nconsecutive for loops, only the first one will work, while the second one will be empty:\\n>>> r1 = DateRangeIterable(date(2018, 1, 1), date(2018, 1, 5))\\n>>> \", \".join(map(str, r1))\\n\\'2018-01-01, 2018-01-02, 2018-01-03, 2018-01-04\\'\\n>>> max(r1)\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nValueError: max() arg is an empty sequence\\n>>>\\n\\nThis is because of the way the iteration protocol works\\xe2\\x80\\x94 an iterable constructs an iterator,\\nand this one is the one being iterated over. In our example, __iter__ just returned self,\\nbut we can make it create a new iterator every time it is called. One way of fixing this\\nwould be to create new instances of DateRangeIterable, which is not a terrible issue, but\\nwe can make __iter__ use a generator (which are iterator objects), which is being created\\nevery time:\\nclass DateRangeContainerIterable:\\ndef __init__(self, start_date, end_date):\\nself.start_date = start_date\\nself.end_date = end_date\\n\\n[ 42 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\ndef __iter__(self):\\ncurrent_day = self.start_date\\nwhile current_day < self.end_date:\\nyield current_day\\ncurrent_day += timedelta(days=1)\\n\\nAnd this time, it works:\\n>>> r1 = DateRangeContainerIterable(date(2018, 1, 1), date(2018, 1, 5))\\n>>> \", \".join(map(str, r1))\\n\\'2018-01-01, 2018-01-02, 2018-01-03, 2018-01-04\\'\\n>>> max(r1)\\ndatetime.date(2018, 1, 4)\\n>>>\\n\\nThe difference is that each for loop is calling __iter__ again, and each one of those is\\ncreating the generator again.\\nThis is called a container iterable.\\nIn general, it is a good idea to work with container iterables when dealing\\nwith generators.\\n\\nDetails on generators will be explained in more detail in Chapter 7, Using Generators.\\n\\nCreating sequences\\nMaybe our object does not define the __iter__() method, but we still want to be able to\\niterate over it. If __iter__ is not defined on the object, the iter() function will look for\\nthe presence of __getitem__, and if this is not found, it will raise TypeError.\\nA sequence is an object that implements __len__ and __getitem__ and expects to be able\\nto get the elements it contains, one at a time, in order, starting at zero as the first index. This\\nmeans that you should be careful in the logic so that you correctly implement\\n__getitem__ to expect this type of index, or the iteration will not work.\\nThe example from the previous section had the advantage that it uses less memory. This\\nmeans that is only holding one date at a time, and knows how to produce the days one by\\none. However, it has the drawback that if we want to get the nth element, we have no way to\\ndo so but iterate n-times until we reach it. This is a typical trade-off in computer science\\nbetween memory and CPU usage.\\n\\n[ 43 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThe implementation with an iterable will use less memory, but it takes up to O(n) to get an\\nelement, whereas implementing a sequence will use more memory (because we have to\\nhold everything at once), but supports indexing in constant time, O(1).\\nThis is what the new implementation might look like:\\nclass DateRangeSequence:\\ndef __init__(self, start_date, end_date):\\nself.start_date = start_date\\nself.end_date = end_date\\nself._range = self._create_range()\\ndef _create_range(self):\\ndays = []\\ncurrent_day = self.start_date\\nwhile current_day < self.end_date:\\ndays.append(current_day)\\ncurrent_day += timedelta(days=1)\\nreturn days\\ndef __getitem__(self, day_no):\\nreturn self._range[day_no]\\ndef __len__(self):\\nreturn len(self._range)\\n\\nHere is how the object behaves:\\n>>> s1 = DateRangeSequence(date(2018, 1, 1), date(2018, 1, 5))\\n>>> for day in s1:\\n...\\nprint(day)\\n...\\n2018-01-01\\n2018-01-02\\n2018-01-03\\n2018-01-04\\n>>> s1[0]\\ndatetime.date(2018, 1, 1)\\n>>> s1[3]\\ndatetime.date(2018, 1, 4)\\n>>> s1[-1]\\ndatetime.date(2018, 1, 4)\\n\\nIn the preceding code, we can see that negative indices also work. This is because\\nthe DateRangeSequence object delegates all of the operations to its wrapped object (a\\nlist), which is the best way to maintain compatibility and a consistent behavior.\\n\\n[ 44 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nEvaluate the trade-off between memory and CPU usage when deciding\\nwhich one of the two possible implementations to use. In general, the\\niteration is preferable (and generators even more), but keep in mind the\\nrequirements of every case.\\n\\nContainer objects\\nContainers are objects that implement a __contains__ method (that usually returns a\\nBoolean value). This method is called in the presence of the in keyword of Python.\\nSomething like the following:\\nelement in container\\n\\nWhen used in Python becomes this:\\ncontainer.__contains__(element)\\n\\nYou can imagine how much more readable (and Pythonic!) the code can be when this\\nmethod is properly implemented.\\nLet\\'s say we have to mark some points on a map of a game that has two-dimensional\\ncoordinates. We might expect to find a function like the following:\\ndef mark_coordinate(grid, coord):\\nif 0 <= coord.x < grid.width and 0 <= coord.y < grid.height:\\ngrid[coord] = MARKED\\n\\nNow, the part that checks the condition of the first if statement seems convoluted; it\\ndoesn\\'t reveal the intention of the code, it\\'s not expressive, and worst of all it calls for code\\nduplication (every part of the code where we need to check the boundaries before\\nproceeding will have to repeat that if statement).\\nWhat if the map itself (called grid on the code) could answer this question? Even better,\\nwhat if the map could delegate this action to an even smaller (and hence more cohesive)\\nobject? Therefore, we can ask the map if it contains a coordinate, and the map itself can\\nhave information about its limit, and ask this object the following:\\nclass Boundaries:\\ndef __init__(self, width, height):\\nself.width = width\\nself.height = height\\ndef __contains__(self, coord):\\n\\n[ 45 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nx, y = coord\\nreturn 0 <= x < self.width and 0 <= y < self.height\\n\\nclass Grid:\\ndef __init__(self, width, height):\\nself.width = width\\nself.height = height\\nself.limits = Boundaries(width, height)\\ndef __contains__(self, coord):\\nreturn coord in self.limits\\n\\nThis code alone is a much better implementation. First, it is doing a simple composition and\\nit\\'s using delegation to solve the problem. Both objects are really cohesive, having the\\nminimal possible logic; the methods are short, and the logic speaks for itself\\xe2\\x80\\x94coord in\\nself.limits is pretty much a declaration of the problem to solve, expressing the intention\\nof the code.\\nFrom the outside, we can also see the benefits. It\\'s almost as if Python is solving the\\nproblem for us:\\ndef mark_coordinate(grid, coord):\\nif coord in grid:\\ngrid[coord] = MARKED\\n\\nDynamic attributes for objects\\nIt is possible to control the way attributes are obtained from objects by means of the\\n__getattr__ magic method. When we call something like <myobject>.<myattribute>,\\nPython will look for <myattribute> in the dictionary of the object, calling\\n__getattribute__ on it. If this is not found (namely, the object does not have the\\nattribute we are looking for), then the extra method, __getattr__, is called, passing the\\nname of the attribute (myattribute) as a parameter. By receiving this value, we can\\ncontrol the way things should be returned to our objects. We can even create new\\nattributes, and so on.\\nIn the following listing, the __getattr__ method is demonstrated:\\nclass DynamicAttributes:\\ndef __init__(self, attribute):\\nself.attribute = attribute\\n\\n[ 46 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\ndef __getattr__(self, attr):\\nif attr.startswith(\"fallback_\"):\\nname = attr.replace(\"fallback_\", \"\")\\nreturn f\"[fallback resolved] {name}\"\\nraise AttributeError(\\nf\"{self.__class__.__name__} has no attribute {attr}\"\\n)\\n\\nHere are some calls to an object of this class:\\n>>> dyn = DynamicAttributes(\"value\")\\n>>> dyn.attribute\\n\\'value\\'\\n>>> dyn.fallback_test\\n\\'[fallback resolved] test\\'\\n>>> dyn.__dict__[\"fallback_new\"] = \"new value\"\\n>>> dyn.fallback_new\\n\\'new value\\'\\n>>> getattr(dyn, \"something\", \"default\")\\n\\'default\\'\\n\\nThe first call is straightforward\\xe2\\x80\\x94we just request an attribute that the object has and get its\\nvalue as a result. The second is where this method takes action because the object does not\\nhave anything called fallback_test, so the __getattr__ will run with that value. Inside\\nthat method, we placed the code that returns a string, and what we get is the result of that\\ntransformation.\\nThe third example is interesting because there a new attribute named fallback_new is\\ncreated (actually, this call would be the same as running dyn.fallback_new = \"new\\nvalue\"), so when we request that attribute, notice that the logic we put\\nin __getattr__ does not apply, simply because that code is never called.\\nNow, the last example is the most interesting one. There is a subtle detail here that makes a\\nhuge difference. Take another look at the code in the __getattr__ method. Notice the\\nexception it raises when the value is not retrievable AttributeError. This is not only for\\nconsistency (as well as the message in the exception) but also required by the builtin getattr() function. Had this exception been any other, it would raise, and the default\\nvalue would not be returned.\\n\\n[ 47 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nBe careful when implementing a method so dynamic as __getattr__,\\nand use it with caution. When implementing __getattr__,\\nraise AttributeError.\\n\\nCallable objects\\nIt is possible (and often convenient) to define objects that can act as functions. One of the\\nmost common applications for this is to create better decorators, but it\\'s not limited to that.\\nThe magic method __call__ will be called when we try to execute our object as if it were a\\nregular function. Every argument passed to it will be passed along to the __call__\\nmethod.\\nThe main advantage of implementing functions this way, through objects, is that objects\\nhave states, so we can save and maintain information across calls.\\nWhen we have an object, a statement like this object(*args, **kwargs) is translated in\\nPython to object.__call__(*args, **kwargs).\\nThis method is useful when we want to create callable objects that will work as\\nparametrized functions, or in some cases functions with memory.\\nThe following listing uses this method to construct an object that when called with a\\nparameter returns the number of times it has been called with the very same value:\\nfrom collections import defaultdict\\nclass CallCount:\\ndef __init__(self):\\nself._counts = defaultdict(int)\\ndef __call__(self, argument):\\nself._counts[argument] += 1\\nreturn self._counts[argument]\\n\\nSome examples of this class in action are as follows:\\n>>> cc = CallCount()\\n>>> cc(1)\\n1\\n>>> cc(2)\\n1\\n\\n[ 48 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\n>>> cc(1)\\n2\\n>>> cc(1)\\n3\\n>>> cc(\"something\")\\n1\\n\\nLater in this book, we will find out that this method comes in handy when creating\\ndecorators.\\n\\nSummary of magic methods\\nWe can summarize the concepts we described in the previous sections in the form of a cheat\\nsheet like the one presented as follows. For each action in Python, the magic method\\ninvolved is presented, along with the concept that it represents:\\nStatement\\n\\nobj[key]\\nobj[i:j]\\nobj[i:j:k]\\nwith obj: ...\\nfor i in obj: ...\\nobj.<attribute>\\n\\nMagic method\\n\\nPython concept\\n\\n__getitem__(key)\\n\\nSubscriptable object\\n\\n__enter__ / __exit__\\n\\nContext manager\\n\\n__iter__ / __next__\\n__len__ / __getitem__\\n__getattr__\\n\\nIterable object\\nSequence\\n\\nobj(*args, **kwargs) __call__(*args, **kwargs)\\n\\nDynamic attribute retrieval\\nCallable object\\n\\nCaveats in Python\\nBesides understanding the main features of the language, being able to write idiomatic code\\nis also about being aware of the potential problems of some idioms, and how to avoid them.\\nIn this section, we will explore common issues that might cause you long debugging\\nsessions if they catch you off guard.\\nMost of the points discussed in this section are things to avoid entirely, and I will dare to\\nsay that there is almost no possible scenario that justifies the presence of the anti-pattern (or\\nidiom, in this case). Therefore, if you find this on the code base you are working on, feel\\nfree to refactor it in the way that is suggested. If you find these traits while doing a code\\nreview, this is a clear indication that something needs to change.\\n\\n[ 49 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nMutable default arguments\\nSimply put, don\\'t use mutable objects as the default arguments of functions. If you use\\nmutable objects as default arguments, you will get results that are not the expected ones.\\nConsider the following erroneous function definition:\\ndef wrong_user_display(user_metadata: dict = {\"name\": \"John\", \"age\": 30}):\\nname = user_metadata.pop(\"name\")\\nage = user_metadata.pop(\"age\")\\nreturn f\"{name} ({age})\"\\n\\nThis has two problems, actually. Besides the default mutable argument, the body of the\\nfunction is mutating a mutable object, hence creating a side effect. But the main problem is\\nthe default argument for user_medatada.\\nThis will actually only work the first time it is called without arguments. For the second\\ntime, we call it without explicitly passing something to user_metadata. It will fail with a\\nKeyError, like so:\\n>>> wrong_user_display()\\n\\'John (30)\\'\\n>>> wrong_user_display({\"name\": \"Jane\", \"age\": 25})\\n\\'Jane (25)\\'\\n>>> wrong_user_display()\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nFile ... in wrong_user_display\\nname = user_metadata.pop(\"name\")\\nKeyError: \\'name\\'\\n\\nThe explanation is simple\\xe2\\x80\\x94by assigning the dictionary with the default data to\\nuser_metadata on the definition of the function, this dictionary is actually created once\\nand the variable user_metadata points to it. The body of the function modifies this object,\\nwhich remains alive in memory so long as the program is running. When we pass a value\\nto it, this will take the place of the default argument we just created. When we don\\'t want\\nthis object it is called again, and it has been modified since the previous run; the next time\\nwe run it, will not contain the keys since they were removed on the previous call.\\n\\n[ 50 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nThe fix is also simple\\xe2\\x80\\x94we need to use None as a default sentinel value and assign the\\ndefault on the body of the function. Because each function has its own scope and life\\ncycle, user_metadata will be assigned to the dictionary every time None appears:\\ndef user_display(user_metadata: dict = None):\\nuser_metadata = user_metadata or {\"name\": \"John\", \"age\": 30}\\nname = user_metadata.pop(\"name\")\\nage = user_metadata.pop(\"age\")\\nreturn f\"{name} ({age})\"\\n\\nExtending built-in types\\nThe correct way of extending built-in types such as lists, strings, and dictionaries is by\\nmeans of the collections module.\\nIf you create a class that directly extends dict, for example, you will obtain results that are\\nprobably not what you are expecting. The reason for this is that in CPython the methods of\\nthe class don\\'t call each other (as they should), so if you override one of them, this will not\\nbe reflected by the rest, resulting in unexpected outcomes. For example, you might want to\\noverride __getitem__, and then when you iterate the object with a for loop, you will\\nnotice that the logic you have put on that method is not applied.\\nThis is all solved by using collections.UserDict, for example, which provides a\\ntransparent interface to actual dictionaries, and is more robust.\\nLet\\'s say we want a list that was originally created from numbers to convert the values to\\nstrings, adding a prefix. The first approach might look like it solves the problem, but it is\\nerroneous:\\nclass BadList(list):\\ndef __getitem__(self, index):\\nvalue = super().__getitem__(index)\\nif index % 2 == 0:\\nprefix = \"even\"\\nelse:\\nprefix = \"odd\"\\nreturn f\"[{prefix}] {value}\"\\n\\n[ 51 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nAt first sight, it looks like the object behaves as we want it to. But then, if we try to iterate it\\n(after all, it is a list), we find that we don\\'t get what we wanted:\\n>>> bl = BadList((0, 1, 2, 3, 4, 5))\\n>>> bl[0]\\n\\'[even] 0\\'\\n>>> bl[1]\\n\\'[odd] 1\\'\\n>>> \"\".join(bl)\\nTraceback (most recent call last):\\n...\\nTypeError: sequence item 0: expected str instance, int found\\n\\nThe join function will try to iterate (run a for loop over) the list, but expects values of\\ntype string. This should work because it is exactly the type of change we made to the list,\\nbut apparently when the list is being iterated, our changed version of the __getitem__ is\\nnot being called.\\nThis issue is actually an implementation detail of CPython (a C optimization), and in other\\nplatforms such as PyPy it doesn\\'t happen (see the differences between PyPy and CPython\\nin the references at the end of this chapter).\\nRegardless of this, we should write code that is portable and compatible in all\\nimplementations, so we will fix it by extending not from list but from UserList:\\nfrom collections import UserList\\nclass GoodList(UserList):\\ndef __getitem__(self, index):\\nvalue = super().__getitem__(index)\\nif index % 2 == 0:\\nprefix = \"even\"\\nelse:\\nprefix = \"odd\"\\nreturn f\"[{prefix}] {value}\"\\n\\nAnd now things look much better:\\n>>> gl = GoodList((0, 1, 2))\\n>>> gl[0]\\n\\'[even] 0\\'\\n>>> gl[1]\\n\\'[odd] 1\\'\\n>>> \"; \".join(gl)\\n\\'[even] 0; [odd] 1; [even] 2\\'\\n\\n[ 52 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nDon\\'t extend directly from dict, use collections.UserDict instead. For\\nlists, use collections.UserList, and for strings,\\nuse collections.UserString.\\n\\nSummary\\nIn this chapter, we have explored the main features of Python, with the goal of\\nunderstanding its most distinctive features, those that make Python a peculiar language\\ncompared to the rest. On this path, we have explored different methods of Python,\\nprotocols, and their internal mechanics.\\nAs opposed to the previous chapter, this one is more Python-focused. A key takeaway of\\nthe topics of this book is that clean code goes beyond following the formatting rules (which,\\nof course, are essential to a good code base). They are a necessary condition, but not\\nsufficient. Over the next few chapters, we will see ideas and principles that relate more to\\nthe code, with the goal of achieving a better design and implementation of our software\\nsolution.\\nWith the concepts and the ideas of this chapter, we explored the core of Python: its\\nprotocols and magic methods. It should be clear by now that the best way of having\\nPythonic, idiomatic code is not only by following the formatting conventions but also by\\ntaking full advantage of all the features Python has to offer. This means that you should\\nsometimes use a particular magic method, implement a context manager, and more.\\nIn the next chapter, we will put these concepts into action, relating general concepts of\\nsoftware engineering with the way they can be written in Python.\\n\\nReferences\\nThe reader will find more information about some of the topics that we have covered in this\\nchapter in the following references. The decision of how indices work in Python is based on\\n(EWD831), which analyzes several alternatives for ranges in math and programming\\nlanguages:\\nEWD831: Why numbering should start at zero (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bcs.\\xe2\\x80\\x8butexas.\\xe2\\x80\\x8bedu/\\nusers/\\xe2\\x80\\x8bEWD/\\xe2\\x80\\x8btranscriptions/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bEWD831.\\xe2\\x80\\x8bhtml)\\nPEP-343: The \"with\" Statement (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0343/\\xe2\\x80\\x8b)\\n\\n[ 53 ]\\n\\n\\x0cPythonic Code\\n\\nChapter 2\\n\\nCC08: The book written by Robert C. Martin named Clean Code: A Handbook of\\nAgile Software Craftsmanship\\nPython documentation, the iter() function (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\nlibrary/\\xe2\\x80\\x8bfunctions.\\xe2\\x80\\x8bhtml#iter)\\nDifferences between PyPy and CPython (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpypy.\\xe2\\x80\\x8breadthedocs.\\xe2\\x80\\x8bio/\\xe2\\x80\\x8ben/\\nlatest/\\xe2\\x80\\x8bcpython_\\xe2\\x80\\x8bdifferences.\\xe2\\x80\\x8bhtml#subclasses-\\xe2\\x80\\x8bof-\\xe2\\x80\\x8bbuilt-\\xe2\\x80\\x8bin-\\xe2\\x80\\x8btypes)\\n\\n[ 54 ]\\n\\n\\x0c3\\nGeneral Traits of Good Code\\nThis is a book about software construction with Python. Good software is built from a good\\ndesign. By saying things such as clean code, one might think that we will explore good\\npractices that relate only to the implementation details of the software, instead of its design.\\nHowever, this assumption would be wrong since the code is not something different from\\nthe design\\xe2\\x80\\x94the code is the design.\\nThe code is probably the most detailed representation of the design. In the first two\\nchapters, we discussed why structuring the code in a consistent way was important, and\\nwe have seen idioms for writing more compact and idiomatic code. Now it\\'s time to\\nunderstand that clean code is that, and much more\\xe2\\x80\\x94the ultimate goal is to make the code\\nas robust as possible, and to write it in a way that minimizes defects or makes them utterly\\nevident, should they occur.\\nThis chapter and the next one are focused on design principles at a higher level of\\nabstraction. These ideas not only relate to Python in particular but are instead general\\nprinciples of software engineering.\\nIn particular, for this chapter, we will review different principles that make for good\\nsoftware design. Good-quality software should be built around these ideas, and they will\\nserve as design tools. This does not mean that all of them should always be applied; in fact,\\nsome of them represent different points of view (such is the case with the Design by\\nContract (DbC) approach, as opposed to defensive programming). Some of them depend\\non the context and are not always applicable.\\nA high-quality code is a concept that has multiple dimensions. We can think of this\\nsimilarly to how we think about the quality attributes of a software architecture. For\\nexample, we want our software to be secure and to have good performance, reliability, and\\nmaintainability, just to name a few.\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThe goals of this chapter are as follows:\\nTo understand the concepts behind robust software\\nTo learn how to deal with erroneous data during the workflow of the application\\nTo design maintainable software that can easily be extended and adapted to new\\nrequirements\\nTo design reusable software\\nTo write effective code that will keep the productivity of the development team\\nhigh\\n\\nDesign by contract\\nSome parts of the software we are working on are not meant to be called directly by users,\\nbut instead by other parts of the code. Such is the case when we divide the responsibilities\\nof the application into different components or layers, and we have to think about the\\ninteraction between them.\\nWe will have to encapsulate some functionality behind each component, and expose an\\ninterface to clients who are going to use that functionality, namely an Application\\nProgramming Interface (API). The functions, classes, or methods we write for that\\ncomponent have a particular way of working under certain considerations that, if they are\\nnot met, will make our code crash. Conversely, clients calling that code expect a particular\\nresponse, and any failure of our function to provide this would represent a defect.\\nThat is to say that if, for example, we have a function that is expected to work with a series\\nof parameters of type integers, and some other function invokes our passing strings, it is\\nclear that it should not work as expected, but in reality, the function should not run at all\\nbecause it was called incorrectly (the client made a mistake). This error should not pass\\nsilently.\\nOf course, when designing an API, the expected input, output, and side-effects should be\\ndocumented. But documentation cannot enforce the behavior of the software at runtime.\\nThese rules, what every part of the code expects in order to work properly and what the\\ncaller is expecting from them, should be part of the design, and here is where the concept of\\na contract comes into place.\\nThe idea behind the DbC is that instead of implicitly placing in the code what every party is\\nexpecting, both parties agree on a contract that, if violated, will raise an exception, clearly\\nstating why it cannot continue.\\n\\n[ 56 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nIn our context, a contract is a construction that enforces some rules that must be honored\\nduring the communication of software components. A contract entails mainly\\npreconditions and postconditions, but in some cases, invariants, and side-effects are also\\ndescribed:\\nPreconditions: We can say that these are all the checks code will do before\\nrunning. It will check for all the conditions that have to be made before the\\nfunction can proceed. In general, it\\'s implemented by validating the data set\\nprovided in the parameters passed, but nothing should stop us from running all\\nsorts of validations (for example, validating a set in a database, a file, another\\nmethod that was called before, and so on) if we consider that their side-effects are\\novershadowed by the importance of such a validation. Notice that this imposes a\\nconstraint on the caller.\\nPostconditions: The opposite of preconditions, here, the validations are done\\nafter the function call is returned. Postcondition validations are run to validate\\nwhat the caller is expecting from this component.\\nInvariants: Optionally, it would be a good idea to document, in the docstring of a\\nfunction, the invariants, the things that are kept constant while the code of the\\nfunction is running, as an expression of the logic of the function to be correct.\\nSide-effects: Optionally, we can mention any side-effects of our code in the\\ndocstring.\\nWhile conceptually all of these items form part of the contract for a software component,\\nand this is what should go to the documentation of such piece, only the first two\\n(preconditions and postconditions) are to be enforced at a low level (code).\\nThe reason why we would design by contract is that if errors occur, they must be easy to\\nspot (and by noticing whether it was either the precondition or postcondition that failed,\\nwe will find the culprit much easily) so that they can be quickly corrected. More\\nimportantly, we want critical parts of the code to avoid being executed under the wrong\\nassumptions. This should help to clearly mark the limits for the responsibilities and errors if\\nthey occur, as opposed to something saying\\xe2\\x80\\x94this part of the application is failing... But the\\ncaller code provided the wrong arguments, so where should we apply the fix?\\nThe idea is that preconditions bind the client (they have an obligation to meet them if they\\nwant to run some part of the code), whereas postconditions bind the component in question\\nto some guarantees that the client can verify and enforce.\\nThis way, we can quickly identify responsibilities. If the precondition fails, we know it is\\ndue to a defect on the client. On the other hand, if the postcondition check fails, we know\\nthe problem is in the routine or class (supplier) itself.\\n\\n[ 57 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nSpecifically regarding preconditions, it is important to highlight that they can be checked at\\nruntime, and if they occur, the code that is being called should not be run at all (it does not\\nmake sense to run it because its conditions do not hold, and further more, doing so might\\nend up making things worse).\\n\\nPreconditions\\nPreconditions are all of the guarantees a function or method expects to receive in order to\\nwork correctly. In general programming terms, this usually means to provide data that is\\nproperly formed, for example, objects that are initialized, non-null values, and many more.\\nFor Python, in particular, being dynamically typed, this also means that sometimes we\\nneed to check for the exact type of data that are provided. This is not exactly the same as\\ntype checking, the kind mypy would do this, but rather verify for exact values that are\\nneeded.\\nPart of these checks can be detected early on by using static analysis tools, such as mypy,\\nwhich we already introduced in Chapter 1, Introduction, Code Formatting, and Tools, but\\nthese checks are not enough. A function should have proper validation for the information\\nthat it is going to handle.\\nNow, this poses the question of where to place the validation logic, depending on whether\\nwe let the clients validate all the data before calling the function, or allow this one to\\nvalidate everything that it received prior running its own logic. The former corresponds to\\na tolerant approach (because the function itself is still allowing any data, potentially\\nmalformed data as well), whereas the latter corresponds to a demanding approach.\\nFor the purposes of this analysis, we prefer a demanding approach when it comes to DbC,\\nbecause it is usually the safest choice in terms of robustness, and usually the most common\\npractice in the industry.\\nRegardless of the approach we decide to take, we should always keep in mind the nonredundancy principle, which states that the enforcement of each precondition for a function\\nshould be done by only one of the two parts of the contract, but not both. This means that\\nwe put the validation logic on the client, or we leave it to the function itself, but in no cases\\nshould we duplicate it (which also relates to the DRY principle, which we will discuss later\\non in this chapter).\\n\\n[ 58 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nPostconditions\\nPostconditions are the part of the contract that is responsible for enforcing the state after the\\nmethod or function has returned.\\nAssuming that the function or method has been called with the correct properties (that is,\\nwith its preconditions met), then the postconditions will guarantee that certain properties\\nare preserved.\\nThe idea is to use postconditions to check and validate for everything that a client might\\nneed. If the method executed properly, and the postcondition validations pass, then any\\nclient calling that code should be able to work with the returned object without problems,\\nas the contract has been fulfilled.\\n\\nPythonic contracts\\nAt the time of writing this book, a PEP-316, named Programming by Contract for Python, is\\ndeferred. This doesn\\'t mean that we cannot implement it in Python, because, as introduced\\nat the beginning of the chapter, this is a general design principle.\\nProbably the best way to enforce this is by adding control mechanisms to our methods,\\nfunctions, and classes, and if they rail raise a RuntimeError exception or ValueError. It\\'s\\nhard to devise a general rule for the correct type of exception, as that would pretty much\\ndepend on the application in particular. These previously mentioned exceptions are the\\nmost common types of exception, but if they don\\'t fit accurately with the problem, creating\\na custom exception would be the best choice.\\nWe would also like to keep the code as isolated as possible. That is, the code for the\\npreconditions in one part, the one for the postconditions in another, and the core of the\\nfunction separated. We could achieve this separation by creating smaller functions, but in\\nsome cases implementing a decorator would be an interesting alternative.\\n\\nDesign by contract \\xe2\\x80\\x93 conclusions\\nThe main value of this design principle is to effectively identify where the problem is. By\\ndefining a contract, when something fails at runtime it will be clear what part of the code is\\nbroken, and what broke the contract.\\nAs a result of following this principle, the code will be more robust. Each component is\\nenforcing its own constraints and maintaining some invariants, and the program can be\\nproven correct as long as these invariants are preserved.\\n\\n[ 59 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nIt also serves the purpose of clarifying the structure of the program better. Instead of trying\\nto run ad hoc validations, or trying to surmount all possible failure scenarios, the contracts\\nexplicitly specify what each function or method expects to work properly, and what is\\nexpected from them.\\nOf course, following these principles also adds extra work, because we are not just\\nprogramming the core logic of our main application, but also the contracts. In addition, we\\nmight also want to consider adding unit tests for these contracts as well. However, the\\nquality gained by this approach pays off in the long run; hence, it is a good idea to\\nimplement this principle for critical components of the application.\\nNonetheless, for this method to be effective, we should carefully think about what are we\\nwilling to validate, and this has to be a meaningful value. For example, it would not make\\nmuch sense to define contracts that only check for the correct data types of the parameters\\nprovided to a function. Many programmers would argue that this would be like trying to\\nmake Python a statically-typed language. Regardless of this, tools such as Mypy, in\\ncombination with the use of annotations, would serve this purpose much better and with\\nless effort. With that in mind, design contracts so that there is actually value on them,\\nchecking, for example, the properties of the objects being passed and returned, the\\nconditions they have to hold, and so on.\\n\\nDefensive programming\\nDefensive programming follows a somewhat different approach than DbC; instead of\\nstating all conditions that must be held in a contract, that if unmet will raise an exception\\nand make the program fail, this is more about making all parts of the code (objects,\\nfunctions, or methods) able to protect themselves against invalid inputs.\\nDefensive programming is a technique that has several aspects, and it is particularly useful\\nif it is combined with other design principles (this means that the fact that it follows a\\ndifferent philosophy than DbC does not mean that it is a case of either one or the other\\xe2\\x80\\x94it\\ncould mean that they might complement each other).\\nThe main ideas on the subject of defensive programming are how to handle errors for\\nscenarios that we might expect to occur, and how to deal with errors that should never\\noccur (when impossible conditions happen). The former will fall into error handling\\nprocedures, while the latter will be the case for assertions, both topics we will be exploring\\nin the following sections.\\n\\n[ 60 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nError handling\\nIn our programs, we resort to error handling procedures for situations that we anticipate as\\nprone to cause errors. This is usually the case for data input.\\nThe idea behind error handling is to gracefully respond to these expected errors in an\\nattempt to either continue our program execution or decide to fail if the error turns out to\\nbe insurmountable.\\nThere are different approaches by which we can handle errors on our programs, but not all\\nof them are always applicable. Some of these approaches are as follows:\\nValue substitution\\nError logging\\nException handling\\n\\nValue substitution\\nIn some scenarios, when there is an error and there is a risk of the software producing an\\nincorrect value or failing entirely, we might be able to replace the result with another, safer\\nvalue. We call this value substitution, since we are in fact replacing the actual erroneous\\nresult for a value that is to be considered non-disruptive (it could be a default, a wellknown constant, a sentinel value, or simply something that does not affect the result at all,\\nlike returning zero in a case where the result is intended to be applied to a sum).\\nValue substitution is not always possible, however. This strategy has to be carefully chosen\\nfor cases where the substituted value is actually a safe option. Making this decision is a\\ntrade-off between robustness and correctness. A software program is robust when it does\\nnot fail, even in the presence of an erroneous scenario. But this is not correct either.\\nThis might not be acceptable for some kinds of software. If the application is critical, or the\\ndata being handled is too sensitive, this is not an option, since we cannot afford to provide\\nusers (or other parts of the application) with erroneous results. In these cases, we opt\\nfor correctness, rather than let the program explode when yielding the wrong results.\\n\\n[ 61 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nA slightly different, and safer, version of this decision is to use default values for data that\\nis not provided. This can be the case for parts of the code that can work with a default\\nbehavior, for example, default values for environment variables that are not set, for missing\\nentries in configuration files, or for parameters of functions. We can find examples of\\nPython supporting this throughout different methods of its API, for example, dictionaries\\nhave a get method, whose (optional) second parameter allows you to indicate a default\\nvalue:\\n>>> configuration = {\"dbport\": 5432}\\n>>> configuration.get(\"dbhost\", \"localhost\")\\n\\'localhost\\'\\n>>> configuration.get(\"dbport\")\\n5432\\n\\nEnvironment variables have a similar API:\\n>>> import os\\n>>> os.getenv(\"DBHOST\")\\n\\'localhost\\'\\n>>> os.getenv(\"DPORT\", 5432)\\n5432\\n\\nIn both previous examples, if the second parameter is not provided, None will be returned,\\nbecause it\\'s the default value those functions are defined with. We can also define default\\nvalues for the parameters of our own functions:\\n>>> def connect_database(host=\"localhost\", port=5432):\\n...\\nlogger.info(\"connecting to database server at %s:%i\", host, port)\\n\\nIn general, replacing missing parameters with default values is acceptable, but substituting\\nerroneous data with legal close values is more dangerous and can mask some errors. Take\\nthis criterion into consideration when deciding on this approach.\\n\\nException handling\\nIn the presence of incorrect or missing input data, sometimes it is possible to correct the\\nsituation with some examples such as the ones mentioned in the previous section. In other\\ncases, however, it is better to stop the program from continuing to run with the wrong data\\nthan to leave it computing under erroneous assumptions. In those cases, failing and\\nnotifying the caller that something is wrong is a good approach, and this is the case for a\\nprecondition that was violated, as we saw in DbC.\\n\\n[ 62 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nNonetheless, erroneous input data is not the only possible way in which a function can go\\nwrong. After all, functions are not just about passing data around; they also have sideeffects and connect to external components.\\nIt could be possible that a fault in a function call is due to a problem on one of these\\nexternal components, and not in our function itself. If that is the case, our function should\\ncommunicate this properly. This will make it easier to debug. The function should clearly,\\nand unambiguously notify the rest of the application about errors that cannot be ignored so\\nthat they can be addressed accordingly.\\nThe mechanism for accomplishing this is an exception. It is important to emphasize that\\nthis is what exceptions should be used for\\xe2\\x80\\x94clearly announcing an exceptional situation,\\nnot altering the flow of the program according to business logic.\\nIf the code tries to use exceptions to handle expected scenarios or business logic, the flow of\\nthe program will become harder to read. This will lead to a situation where exceptions are\\nused as a sort of go-to statement, that (to make things worse) could span multiple levels\\non the call stack (up to caller functions), violating the encapsulation of the logic into its\\ncorrect level of abstraction. The case could get even worse if these except blocks are\\nmixing business logic with truly exceptional cases that the code is trying to defend against;\\nin that case, it will be harder to distinguish between the core logic we have to maintain and\\nerrors to be handled.\\nDo not use exceptions as a go-to mechanism for business logic. Raise\\nexceptions when there is actually something wrong with the code that\\ncallers need to be aware of.\\nThis last concept is an important one; exceptions are usually about notifying the caller\\nabout something that is amiss. This means that exceptions should be used carefully because\\nthey weaken encapsulation. The more exceptions a function has, the more the caller\\nfunction will have to anticipate, therefore knowing about the function it is calling. And if a\\nfunction raises too many exceptions, this means that is not so context-free, because every\\ntime we want to invoke it, we will have to keep all of its possible side-effects in mind.\\nThis can be used as a heuristic to tell when a function is not cohesive enough and has too\\nmany responsibilities. If it raises too many exceptions, it could be a sign that it has to be\\nbroken down into multiple, smaller ones.\\nHere are some recommendations that relate to exceptions in Python.\\n\\n[ 63 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nHandle exceptions at the right level of abstraction\\nExceptions are also part of the principal functions that do one thing, and one thing only.\\nThe exception the function is handling (or raising) has to be consistent with the logic\\nencapsulated on it.\\nIn this example, we can see what we mean by mixing different levels of abstractions.\\nImagine an object that acts as a transport for some data in our application. It connects to an\\nexternal component where the data is going to be sent upon decoding. In the following\\nlisting, we will focus on the deliver_event method:\\nclass DataTransport:\\n\"\"\"An example of an object handling exceptions of different levels.\"\"\"\\nretry_threshold: int = 5\\nretry_n_times: int = 3\\ndef __init__(self, connector):\\nself._connector = connector\\nself.connection = None\\ndef deliver_event(self, event):\\ntry:\\nself.connect()\\ndata = event.decode()\\nself.send(data)\\nexcept ConnectionError as e:\\nlogger.info(\"connection error detected: %s\", e)\\nraise\\nexcept ValueError as e:\\nlogger.error(\"%r contains incorrect data: %s\", event, e)\\nraise\\ndef connect(self):\\nfor _ in range(self.retry_n_times):\\ntry:\\nself.connection = self._connector.connect()\\nexcept ConnectionError as e:\\nlogger.info(\\n\"%s: attempting new connection in %is\",\\ne,\\nself.retry_threshold,\\n)\\ntime.sleep(self.retry_threshold)\\nelse:\\nreturn self.connection\\nraise ConnectionError(\\n\\n[ 64 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nf\"Couldn\\'t connect after {self.retry_n_times} times\"\\n)\\ndef send(self, data):\\nreturn self.connection.send(data)\\n\\nFor our analysis, let\\'s zoom in and focus on how the deliver_event() method handles\\nexceptions.\\nWhat does ValueError have to do with ConnectionError? Not much. By looking at\\nthese two highly different types of error, we can get an idea of how responsibilities should\\nbe divided. The ConnectionError should be handled inside the connect method. This\\nwill allow a clear separation of behavior. For example, if this method needs to support\\nretries, that would be a way of doing it. Conversely, ValueError belongs to the decode\\nmethod of the event. With this new implementation, this method does not need to catch\\nany exception\\xe2\\x80\\x94the exceptions it was worrying about before are either handled by internal\\nmethods or deliberately left to be raised.\\nWe should separate these fragments into different methods or functions. For connection\\nmanagement, a small function should be enough. This function will be in charge of trying\\nto establish the connection, catching exceptions (should they occur), and logging them\\naccordingly:\\ndef connect_with_retry(connector, retry_n_times, retry_threshold=5):\\n\"\"\"Tries to establish the connection of <connector> retrying\\n<retry_n_times>.\\nIf it can connect, returns the connection object.\\nIf it\\'s not possible after the retries, raises ConnectionError\\n:param connector: An object with a `.connect()` method.\\n:param retry_n_times int: The number of times to try to call\\n``connector.connect()``.\\n:param retry_threshold int: The time lapse between retry calls.\\n\"\"\"\\nfor _ in range(retry_n_times):\\ntry:\\nreturn connector.connect()\\nexcept ConnectionError as e:\\nlogger.info(\\n\"%s: attempting new connection in %is\", e, retry_threshold\\n)\\n\\n[ 65 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\ntime.sleep(retry_threshold)\\nexc = ConnectionError(f\"Couldn\\'t connect after {retry_n_times} times\")\\nlogger.exception(exc)\\nraise exc\\n\\nThen, we will call this function in our method. As for the ValueError exception on the\\nevent, we could separate it with a new object and do composition, but for this limited case\\nit would be overkill, so just moving the logic to a separate method would be enough. With\\nthese two considerations in place, the new version of the method looks much more compact\\nand easier to read:\\nclass DataTransport:\\n\"\"\"An example of an object that separates the exception handling by\\nabstraction levels.\\n\"\"\"\\nretry_threshold: int = 5\\nretry_n_times: int = 3\\ndef __init__(self, connector):\\nself._connector = connector\\nself.connection = None\\ndef deliver_event(self, event):\\nself.connection = connect_with_retry(\\nself._connector, self.retry_n_times, self.retry_threshold\\n)\\nself.send(event)\\ndef send(self, event):\\ntry:\\nreturn self.connection.send(event.decode())\\nexcept ValueError as e:\\nlogger.error(\"%r contains incorrect data: %s\", event, e)\\nraise\\n\\nDo not expose tracebacks\\nThis is a security consideration. When dealing with exceptions, it might be acceptable to let\\nthem propagate if the error is too important, and maybe even let the program fail if this is\\nthe decision for that particular scenario and correctness was favored over robustness.\\n\\n[ 66 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nWhen there is an exception that denotes a problem, it\\'s important to log in with as much\\ndetail as possible (including the traceback information, message, and all we can gather) so\\nthat the issue can be corrected efficiently. At the same time, we want to include as much\\ndetail as possible for ourselves\\xe2\\x80\\x94we definitely don\\'t want any of this becoming visible to\\nusers.\\nIn Python, tracebacks of exceptions contain very rich and useful debugging information.\\nUnfortunately, this information is also very useful for attackers or malicious users who\\nwant to try and harm the application, not to mention that the leak would represent an\\nimportant information disclosure, jeopardizing the intellectual property of your\\norganization (parts of the code will be exposed).\\nIf you choose to let exceptions propagate, make sure not to disclose any sensitive\\ninformation. Also, if you have to notify users about a problem, choose generic messages\\n(such as Something went wrong, or Page not found). This is a common technique used in\\nweb applications that display generic informative messages when an HTTP error occurs.\\n\\nAvoid empty except blocks\\nThis was even referred to as the most diabolical Python anti-pattern (REAL 01). While it is\\ngood to anticipate and defend our programs against some errors, being too defensive might\\nlead to even worse problems. In particular, the only problem with being too defensive is\\nthat there is an empty except block that silently passes without doing anything.\\nPython is so flexible that it allows us to write code that can be faulty and yet, will not raise\\nan error, like this:\\ntry:\\nprocess_data()\\nexcept:\\npass\\n\\nThe problem with this is that it will not fail, ever. Even when it should. It is also nonPythonic if you remember from the zen of Python that errors should never pass silently.\\nIf there is a true exception, this block of code will not fail, which might be what we wanted\\nin the first place. But what if there is a defect? We need to know if there is an error in our\\nlogic to be able to correct it. Writing blocks such as this one will mask problems, making\\nthings harder to maintain.\\n\\n[ 67 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThere are two alternatives:\\nCatch a more specific exception (not too broad, such as an Exception). In fact,\\nsome linting tools and IDEs will warn you in some cases when the code is\\nhandling too broad an exception.\\nDo some actual error handling on the except block.\\nThe best thing to do would be to apply both items simultaneously.\\nHandling a more specific exception (for example, AttributeError or KeyError) will\\nmake the program more maintainable because the reader will know what to expect, and\\ncan get an idea of the why of it. It will also leave other exceptions free to be raised, and if\\nthat happens, this probably means a bug, only this time it can be discovered.\\nHandling the exception itself can mean multiple things. In its simplest form, it could be just\\nabout logging the exception (make sure to use logger.exception or logger.error to\\nprovide the full context of what happened). Other alternatives could be to return a default\\nvalue (substitution, only that in this case after detecting an error, not prior to causing it), or\\nraising a different exception.\\nIf you choose to raise a different exception, to include the original\\nexception that caused the problem, which leads us to the next point.\\n\\nInclude the original exception\\nAs part of our error handling logic, we might decide to raise a different one, and maybe\\neven change its message. If that is the case, it is recommended to include the original\\nexception that led to that.\\nIn Python 3 (PEP-3134), we can now use the raise <e> from <original_exception>\\nsyntax. When using this construction, the original traceback will be embedded into the new\\nexception, and the original exception will be set in the __cause__ attribute of the resulting\\none.\\n\\n[ 68 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nFor example, if we desire to wrap default exceptions with custom ones internally to our\\nproject, we could still do that while including information about the root exception:\\nclass InternalDataError(Exception):\\n\"\"\"An exception with the data of our domain problem.\"\"\"\\n\\ndef process(data_dictionary, record_id):\\ntry:\\nreturn data_dictionary[record_id]\\nexcept KeyError as e:\\nraise InternalDataError(\"Record not present\") from e\\n\\nAlways use the raise <e> from <o> syntax when changing the type of\\nthe exception.\\n\\nUsing assertions in Python\\nAssertions are to be used for situations that should never happen, so the expression on\\nthe assert statement has to mean an impossible condition. Should this condition happen,\\nit means there is a defect in the software.\\nIn contrast with the error handling approach, here there is (or should not be) a possibility of\\ncontinuing the program. If such an error occurs, the program must stop. It makes sense to\\nstop the program because, as commented before, we are in the presence of a defect, so there\\nis no way to move forward by releasing a new version of the software that corrects this\\ndefect.\\nThe idea of using assertions is to prevent the program from causing further damage if such\\nan invalid scenario is presented. Sometimes, it is better to stop and let the program crash,\\nrather than let it continue processing under the wrong assumptions.\\nFor this reason, assertions should not be mixed with the business logic, or used as control\\nflow mechanisms for the software. The following example is a bad idea:\\ntry:\\nassert condition.holds(), \"Condition is not satisfied\"\\nexcept AssertionError:\\nalternative_procedure()\\n\\n[ 69 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nDo not catch the AssertionError exception.\\n\\nMake sure that the program terminates when an assertion fails.\\nInclude a descriptive error message in the assertion statement and log the errors to make\\nsure that you can properly debug and correct the problem later on.\\nAnother important reason why the previous code is a bad idea is that besides catching\\nAssertionError, the statement in the assertion is a function call. Function calls can have\\nside-effects, and they aren\\'t always repeatable (we don\\'t know if calling\\ncondition.holds() again will yield the same result). Moreover, if we stop the debugger\\nat that line, we might not be able to conveniently see the result that causes the error, and,\\nagain, even if we call that function again, we don\\'t know if that was the offending value.\\nA better alternative requires fewer lines of code and provides more useful information:\\nresult = condition.holds()\\nassert result > 0, \"Error with {0}\".format(result)\\n\\nSeparation of concerns\\nThis is a design principle that is applied at multiple levels. It is not just about the low-level\\ndesign (code), but it is also relevant at a higher level of abstraction, so it will come up later\\nwhen we talk about architecture.\\nDifferent responsibilities should go into different components, layers, or modules of the\\napplication. Each part of the program should only be responsible for a part of the\\nfunctionality (what we call its concerns) and should know nothing about the rest.\\nThe goal of separating concerns in software is to enhance maintainability by minimizing\\nripple effects. A ripple effect means the propagation of a change in the software from a\\nstarting point. This could be the case of an error or exception triggering a chain of other\\nexceptions, causing failures that will result in a defect on a remote part of the application. It\\ncan also be that we have to change a lot of code scattered through multiple parts of the code\\nbase, as a result of a simple change in a function definition.\\nClearly, we do not want these scenarios to happen. The software has to be easy to change. If\\nwe have to modify or refactor some part of the code that has to have a minimal impact on\\nthe rest of the application, the way to achieve this is through proper encapsulation.\\n\\n[ 70 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nIn a similar way, we want any potential errors to be contained so that they don\\'t cause\\nmajor damage.\\nThis concept is related to the DbC principle in the sense that each concern can be enforced\\nby a contract. When a contract is violated, and an exception is raised as a result of such a\\nviolation, we know what part of the program has the failure, and what responsibilities\\nfailed to be met.\\nDespite this similarity, separation of concerns goes further. We normally think of contracts\\nbetween functions, methods, or classes, and while this also applies to responsibilities that\\nhave to be separated, the idea of separation of concerns also applies to Python modules,\\npackages, and basically any software component.\\n\\nCohesion and coupling\\nThese are important concepts for good software design.\\nOn the one hand, cohesion means that objects should have a small and well-defined\\npurpose, and they should do as little as possible. It follows a similar philosophy as Unix\\ncommands that do only one thing and do it well. The more cohesive our objects are, the\\nmore useful and reusable they become, making our design better.\\nOn the other hand, coupling refers to the idea of how two or more objects depend on each\\nother. This dependency poses a limitation. If two parts of the code (objects or methods) are\\ntoo dependent on each other, they bring with them some undesired consequences:\\nNo code reuse: If one function depends too much on a particular object, or takes\\ntoo many parameters, it\\'s coupled with this object, which means that it will be\\nreally difficult to use that function in a different context (in order to do so, we\\nwill have to find a suitable parameter that complies with a very restrictive\\ninterface)\\nRipple effects: Changes in one of the two parts will certainly impact the other, as\\nthey are too close\\nLow level of abstraction: When two functions are so closely related, it is hard to\\nsee them as different concerns resolving problems at different levels of\\nabstraction\\nRule of thumb: Well-defined software will achieve high cohesion and low\\ncoupling.\\n\\n[ 71 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nAcronyms to live by\\nIn this section, we will review some principles that yield some good design ideas. The point\\nis to quickly relate to good software practices by acronyms that are easy to remember,\\nworking as a sort of mnemonic rule. If you keep these words in mind, you will be able to\\nassociate them with good practices more easily, and finding the right idea behind a\\nparticular line of code that you are looking at will be faster.\\nThese are by no means formal or academic definitions, but more like empirical ideas that\\nemerged from years of working in the software industry. Some of them do appear in books,\\nas they were coined by important authors (see the references to investigate them in more\\ndetail), and others have their roots probably in blog posts, papers, or conference talks.\\n\\nDRY/OAOO\\nThe ideas of Don\\'t Repeat Yourself (DRY) and Once and Only Once (OAOO) are closely\\nrelated, so they were included together here. They are self-explanatory, you should avoid\\nduplication at all costs.\\nThings in the code, knowledge, have to be defined only once and in a single place. When\\nyou have to make a change in the code, there should be only one rightful location to\\nmodify. Failure to do so is a sign of a poorly designed system.\\nCode duplication is a problem that directly impacts maintainability. It is very undesirable\\nto have code duplication because of its many negative consequences:\\nIt\\'s error prone: When some logic is repeated multiple times throughout the\\ncode, and this needs to change, it means we depend on efficiently correcting all\\nthe instances with this logic, without forgetting of any of them, because in that\\ncase there will be a bug.\\nIt\\'s expensive: Linked to the previous point, making a change in multiple places\\ntakes much more time (development and testing effort) than if it was defined\\nonly once. This will slow the team down.\\nIt\\'s unreliable: Also linked to the first point, when multiple places need to be\\nchanged for a single change in the context, you rely on the person who wrote the\\ncode to remember all the instances where the modification has to be made. There\\nis no single source of truth.\\nDuplication is often caused by ignoring (or forgetting) that code represents knowledge. By\\ngiving meaning to certain parts of the code, we are identifying and labeling that\\nknowledge.\\n\\n[ 72 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nLet\\'s see what this means with an example. Imagine that, in a study center, students are\\nranked by the following criteria: 11 points per exam passed, minus five points per exam\\nfailed, and minus two per year in the institution. The following is not actual code, but just a\\nrepresentation of how this might be scattered in a real code base:\\ndef process_students_list(students):\\n# do some processing...\\nstudents_ranking = sorted(\\nstudents, key=lambda s: s.passed * 11 - s.failed * 5 - s.years * 2\\n)\\n# more processing\\nfor student in students_ranking:\\nprint(\\n\"Name: {0}, Score: {1}\".format(\\nstudent.name,\\n(student.passed * 11 - student.failed * 5 - student.years *\\n2),\\n)\\n)\\n\\nNotice how the lambda which is in the key of the sorted function represents some valid\\nknowledge from the domain problem, yet it doesn\\'t reflect it (it doesn\\'t have a name, a\\nproper and rightful location, there is no meaning assigned to that code, nothing). This lack\\nof meaning in the code leads to the duplication we find when the score is printed out while\\nlisting the raking.\\nWe should reflect our knowledge of our domain problem in our code, and our code will\\nthen be less likely to suffer from duplication and will be easier to understand:\\ndef score_for_student(student):\\nreturn student.passed * 11 - student.failed * 5 - student.years * 2\\n\\ndef process_students_list(students):\\n# do some processing...\\nstudents_ranking = sorted(students, key=score_for_student)\\n# more processing\\nfor student in students_ranking:\\nprint(\\n\"Name: {0}, Score: {1}\".format(\\nstudent.name, score_for_student(student)\\n)\\n)\\n\\n[ 73 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nA fair disclaimer: this is just an analysis of one of the traits of code duplication. In reality,\\nthere are more cases, types, and taxonomies of code duplication, entire chapters could be\\ndedicated to this topic, but here we focus on one particular aspect to make the idea behind\\nthe acronym clear.\\nIn this example, we have taken what is probably the simplest approach to eliminating\\nduplication: creating a function. Depending on the case, the best solution would be\\ndifferent. In some cases, there might be an entirely new object that has to be created (maybe\\nan entire abstraction was missing). In other cases, we can eliminate duplication with a\\ncontext manager. Iterators or generators (described in Chapter 7, Using Generators) could\\nalso help to avoid repetition in the code, and decorators (explained in Chapter 5, Using\\nDecorators to Improve Our Code), will also help.\\nUnfortunately, there is no general rule or pattern to tell you which of the features of Python\\nare the most suitable to address code duplication, but hopefully, after seeing the examples\\nin this book, and how the elements of Python are used, the reader will be able to develop\\nhis/her own intuition.\\n\\nYAGNI\\nYAGNI (short for You Ain\\'t Gonna Need It) is an idea you might want to keep in mind\\nvery often when writing a solution if you do not want to over-engineer it.\\nWe want to be able to easily modify our programs, so we want to make them future-proof.\\nIn line with that, many developers think that they have to anticipate all future requirements\\nand create solutions that are very complex, and so create abstractions that are hard to read,\\nmaintain, and understand. Sometime later, it turns out that those anticipated requirements\\ndo not show up, or they do but in a different way (surprise!), and the original code that was\\nsupposed to handle precisely that does not work. The problem is that now it is even harder\\nto refactor and extend our programs. What happened was that the original solution did not\\nhandle the original requirements correctly, and neither do the current ones, simply because\\nit is the wrong abstraction.\\nHaving maintainable software is not about anticipating future requirements (do not do\\nfuturology!). It is about writing software that only addresses current requirements in such a\\nway that it will be possible (and easy) to change later on. In other words, when designing,\\nmake sure that your decisions don\\'t tie you down, and that you will be able to keep on\\nbuilding, but do not build more than what\\'s necessary.\\n\\n[ 74 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nKIS\\nKIS (stands for Keep It Simple) relates very much to the previous point. When you are\\ndesigning a software component, avoid over-engineering it; ask yourself if your solution is\\nthe minimal one that fits the problem.\\nImplement minimal functionality that correctly solves the problem and does not complicate\\nyour solution more than is necessary. Remember: the simpler the design, the more\\nmaintainable it will be.\\nThis design principle is an idea we will want to keep in mind at all levels of abstraction,\\nwhether we are thinking of a high-level design, or addressing a particular line of code.\\nAt a high-level, think on the components we are creating. Do we really need all of them?\\nDoes this module actually require being utterly extensible right now? Emphasize the last\\npart\\xe2\\x80\\x94maybe we want to make that component extensible, but now is not the right time, or\\nit is not appropriate to do so because we still do not have enough information to create the\\nproper abstractions, and trying to come up with generic interfaces at this point will only\\nlead to even worse problems.\\nIn terms of code, keeping it simple usually means using the smallest data structure that fits\\nthe problem. You will most likely find it in the standard library.\\nSometimes, we might over-complicate code, creating more functions or methods than\\nwhat\\'s necessary. The following class creates a namespace from a set of keyword arguments\\nthat have been provided, but it has a rather complicated code interface:\\nclass ComplicatedNamespace:\\n\"\"\"An convoluted example of initializing an object with some\\nproperties.\"\"\"\\nACCEPTED_VALUES = (\"id_\", \"user\", \"location\")\\n@classmethod\\ndef init_with_data(cls, **data):\\ninstance = cls()\\nfor key, value in data.items():\\nif key in cls.ACCEPTED_VALUES:\\nsetattr(instance, key, value)\\nreturn instance\\n\\n[ 75 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nHaving an extra class method for initializing the object doesn\\'t seem really necessary. Then,\\nthe iteration, and the call to setattr inside it, make things even more strange, and the\\ninterface that is presented to the user is not very clear:\\n>>> cn = ComplicatedNamespace.init_with_data(\\n...\\nid_=42, user=\"root\", location=\"127.0.0.1\", extra=\"excluded\"\\n... )\\n>>> cn.id_, cn.user, cn.location\\n(42, \\'root\\', \\'127.0.0.1\\')\\n>>> hasattr(cn, \"extra\")\\nFalse\\n\\nThe user has to know of the existence of this other method, which is not convenient. It\\nwould be better to keep it simple, and just initialize the object as we initialize any other\\nobject in Python (after all, there is a method for that) with the __init__ method:\\nclass Namespace:\\n\"\"\"Create an object from keyword arguments.\"\"\"\\nACCEPTED_VALUES = (\"id_\", \"user\", \"location\")\\ndef __init__(self, **data):\\naccepted_data = {\\nk: v for k, v in data.items() if k in self.ACCEPTED_VALUES\\n}\\nself.__dict__.update(accepted_data)\\n\\nRemember the zen of Python: simple is better than complex.\\n\\nEAFP/LBYL\\nEAFP (stands for Easier to Ask Forgiveness than Permission), while LBYL (stands for\\nLook Before You Leap).\\nThe idea of EAFP is that we write our code so that it performs an action directly, and then\\nwe take care of the consequences later in case it doesn\\'t work. Typically, this means\\ntry running some code, expecting it to work, but catching an exception if it doesn\\'t, and\\nthen handling the corrective code on the except block.\\n\\n[ 76 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThis is the opposite of LBYL. As its name says, in the look before you leap approach, we\\nfirst check what we are about to use. For example, we might want to check if a file is\\navailable before trying to operate with it:\\nif os.path.exists(filename):\\nwith open(filename) as f:\\n...\\n\\nThis might be good for other programming languages, but it is not the Pythonic way of\\nwriting code. Python was built with ideas such as EAFP, and it encourages you to follow\\nthem (remember, explicit is better than implicit). This code would instead be rewritten like\\nthis:\\ntry:\\nwith open(filename) as f:\\n...\\nexcept FileNotFoundError as e:\\nlogger.error(e)\\n\\nPrefer EAFP over LBYL.\\n\\nComposition and inheritance\\nIn object-oriented software design, there are often discussions as to how to address some\\nproblems by using the main ideas of the paradigm (polymorphism, inheritance, and\\nencapsulation).\\nProbably the most commonly used of these ideas is inheritance\\xe2\\x80\\x94developers often start by\\ncreating a class hierarchy with the classes they are going to need and decide the methods\\neach one should implement.\\nWhile inheritance is a powerful concept, it does come with its perils. The main one is that\\nevery time we extend a base class, we are creating a new one that is tightly coupled with\\nthe parent. As we have already discussed, coupling is one of the things we want to reduce\\nto a minimum when designing software.\\n\\n[ 77 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nOne of the main uses developers relate inheritance with is code reuse. While we should\\nalways embrace code reuse, it is not a good idea to force our design to use inheritance to\\nreuse code just because we get the methods from the parent class for free. The proper way\\nto reuse code is to have highly cohesive objects that can be easily composed and that could\\nwork on multiple contexts.\\n\\nWhen inheritance is a good decision\\nWe have to be careful when creating a derived class, because this is a double-edged\\nsword\\xe2\\x80\\x94on the one hand, it has the advantage that we get all the code of the methods from\\nthe parent class for free, but on the other hand, we are carrying all of them to a new class,\\nmeaning that we might be placing too much functionality in a new definition.\\nWhen creating a new subclass, we have to think if it is actually going to use all of the\\nmethods it has just inherited, as a heuristic to see if the class is correctly defined. If instead,\\nwe find out that we do not need most of the methods, and have to override or replace them,\\nthis is a design mistake that could be caused by several reasons:\\nThe superclass is vaguely defined and contains too much responsibility, instead\\nof a well-defined interface\\nThe subclass is not a proper specialization of the superclass it is trying to extend\\nA good case for using inheritance is the type of situation when you have a class that defines\\ncertain components with its behavior that are defined by the interface of this class (its\\npublic methods and attributes), and then you need to specialize this class in order to create\\nobjects that do the same but with something else added, or with some particular parts of its\\nbehavior changed.\\nYou can find examples of good uses of inheritance in the Python standard library itself. For\\nexample, in the http.server package (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bhttp.\\nserver.\\xe2\\x80\\x8bhtml#http.\\xe2\\x80\\x8bserver.\\xe2\\x80\\x8bBaseHTTPRequestHandler), we can find a base class such\\nas BaseHTTPRequestHandler, and subclasses such as SimpleHTTPRequestHandler that\\nextend this one by adding or changing part of its base interface.\\nSpeaking of interface definition, this is another good use for inheritance. When we want to\\nenforce the interface of some objects, we can create an abstract base class that does not\\nimplement the behavior itself, but instead just defines the interface\\xe2\\x80\\x94every class that\\nextends this one will have to implement these to be a proper subtype.\\n\\n[ 78 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nFinally, another good case for inheritance is exceptions. We can see that the standard\\nexception in Python derives from Exception. This is what allows you to have a generic\\nclause such as except Exception:, which will catch every possible error. The important\\npoint is the conceptual one, they are classes derived from Exception because they are\\nmore specific exceptions. This also works in well-known libraries such as requests, for\\ninstance, in which an HTTPError is RequestException, which in turn is an IOError.\\n\\nAnti-patterns for inheritance\\nIf the previous section had to be summarized into a single word, it would\\nbe specialization. The correct use for inheritance is to specialize objects and create more\\ndetailed abstractions starting from base ones.\\nThe parent (or base) class is part of the public definition of the new derived class. This is\\nbecause the methods that are inherited will be part of the interface of this new class. For this\\nreason, when we read the public methods of a class, they have to be consistent with what\\nthe parent class defines.\\nFor example, if we see that a class derived from BaseHTTPRequestHandler implements a\\nmethod named handle(), it would make sense because it is overriding one of the parents.\\nIf it had any other method whose name relates to an action that has to do with an HTTP\\nrequest, then we could also think that is correctly placed (but we would not think that if we\\nfound something called process_purchase() on that class).\\nThe previous illustration might seem obvious, but it is something that happens very often,\\nespecially when developers try to use inheritance with the sole goal of reusing code. In the\\nnext example, we will see a typical situation that represents a common anti-pattern in\\nPython\\xe2\\x80\\x94there is a domain problem that has to be represented, and a suitable data structure\\nis devised for that problem, but instead of creating an object that uses such a data structure,\\nthe object becomes the data structure itself.\\nLet\\'s see these problems more concretely through an example. Imagine we have a system\\nfor managing insurance, with a module in charge of applying policies to different clients.\\nWe need to keep in memory a set of customers that are being processed at the time in order\\nto apply those changes before further processing or persistence. The basic operations we\\nneed are to store a new customer with its records as satellite data, apply a change on a\\npolicy, or edit some of the data, just to name a few. We also need to support a batch\\noperation, that is, when something on the policy itself changes (the one this module is\\ncurrently processing), we have to apply these changes overall to customers on the current\\ntransaction.\\n\\n[ 79 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThinking in terms of the data structure we need, we realize that accessing the record for a\\nparticular customer in constant time is a nice trait. Therefore, something like\\npolicy_transaction[customer_id] looks like a nice interface. From this, we might\\nthink that a subscriptable object is a good idea, and further on, we might get carried away\\ninto thinking that the object we need is a dictionary:\\nclass TransactionalPolicy(collections.UserDict):\\n\"\"\"Example of an incorrect use of inheritance.\"\"\"\\ndef change_in_policy(self, customer_id, **new_policy_data):\\nself[customer_id].update(**new_policy_data)\\n\\nWith this code, we can get information about a policy for a customer by its identifier:\\n>>> policy = TransactionalPolicy({\\n...\\n\"client001\": {\\n...\\n\"fee\": 1000.0,\\n...\\n\"expiration_date\": datetime(2020, 1, 3),\\n...\\n}\\n... })\\n>>> policy[\"client001\"]\\n{\\'fee\\': 1000.0, \\'expiration_date\\': datetime.datetime(2020, 1, 3, 0, 0)}\\n>>> policy.change_in_policy(\"client001\", expiration_date=datetime(2020, 1,\\n4))\\n>>> policy[\"client001\"]\\n{\\'fee\\': 1000.0, \\'expiration_date\\': datetime.datetime(2020, 1, 4, 0, 0)}\\n\\nSure, we achieved the interface we wanted in the first place, but at what cost? Now, this\\nclass has a lot of extra behavior from carrying out methods that weren\\'t necessary:\\n>>> dir(policy)\\n[ # all magic and special method have been omitted for brevity...\\n\\'change_in_policy\\', \\'clear\\', \\'copy\\', \\'data\\', \\'fromkeys\\', \\'get\\', \\'items\\',\\n\\'keys\\', \\'pop\\', \\'popitem\\', \\'setdefault\\', \\'update\\', \\'values\\']\\n\\nThere are (at least) two major problems with this design. On the one hand, the hierarchy is\\nwrong. Creating a new class from a base one conceptually means that it\\'s a more specific\\nversion of the class it\\'s extending (hence the name). How is it that a\\nTransactionalPolicy is a dictionary? Does this make sense? Remember, this is part of\\nthe public interface of the object, so users will see this class, their hierarchy, and will notice\\nsuch an odd specialization, as well as its public methods.\\n\\n[ 80 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThis leads us to the second problem\\xe2\\x80\\x94coupling. The interface of the transactional policy\\nnow includes all methods from a dictionary. Does a transactional policy really need\\nmethods such as pop() or items()? However, there they are. They are also public, so any\\nuser of this interface is entitled to call them, with whatever undesired side-effect they may\\ncarry. More on this point\\xe2\\x80\\x94we don\\'t really gain much by extending a dictionary. The only\\nmethod it actually needs to update for all customers affected by a change in the current\\npolicy (change_in_policy()) is not on the base class, so we will have to define it\\nourselves either way.\\nThis is a problem of mixing implementation objects with domain objects. A dictionary is an\\nimplementation object, a data structure, suitable for certain kinds of operation, and with a\\ntrade-off like all data structures. A transactional policy should represent something in the\\ndomain problem, an entity that is part of the problem we are trying to solve.\\nHierarchies like this one are incorrect, and just because we get a few magic methods from a\\nbase class (to make the object subscriptable by extending a dictionary) is not reason enough\\nto create such an extension. Implementation classes should be extending solely when\\ncreating other, more specific, implementation classes. In other words, extend a dictionary if\\nyou want to create another (more specific, or slightly modified) dictionary. The same rule\\napplies to classes of the domain problem.\\nThe correct solution here is to use composition. TransactionalPolicy is not a\\ndictionary\\xe2\\x80\\x94it uses a dictionary. It should store a dictionary in a private attribute, and\\nimplement __getitem__() by proxying from that dictionary and then only implementing\\nthe rest of the public method it requires:\\nclass TransactionalPolicy:\\n\"\"\"Example refactored to use composition.\"\"\"\\ndef __init__(self, policy_data, **extra_data):\\nself._data = {**policy_data, **extra_data}\\ndef change_in_policy(self, customer_id, **new_policy_data):\\nself._data[customer_id].update(**new_policy_data)\\ndef __getitem__(self, customer_id):\\nreturn self._data[customer_id]\\ndef __len__(self):\\nreturn len(self._data)\\n\\n[ 81 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThis way is not only conceptually correct, but also more extensible. If the underlying data\\nstructure (which, for now, is a dictionary) is changed in the future, callers of this object will\\nnot be affected, so long as the interface is maintained. This reduces coupling, minimizes\\nripple effects, allows for better refactoring (unit tests ought not to be changed), and makes\\nthe code more maintainable.\\n\\nMultiple inheritance in Python\\nPython supports multiple inheritance. As inheritance, when improperly used, leads to\\ndesign problems, you could also expect that multiple inheritance will also yield even bigger\\nproblems when it\\'s not correctly implemented.\\nMultiple inheritance is, therefore, a double-edged sword. It can also be very beneficial in\\nsome cases. Just to be clear, there is nothing wrong with multiple inheritance\\xe2\\x80\\x94the only\\nproblem it has is that when it\\'s not implemented correctly, it will multiply the problems.\\nMultiple inheritance is a perfectly valid solution when used correctly, and this opens up\\nnew patterns (such as the adapter pattern we discussed in Chapter 9, Common Design\\nPatterns) and mixins.\\nOne of the most powerful applications of multiple inheritance is perhaps that which\\nenables the creation of mixins. Before exploring mixins, we need to understand how\\nmultiple inheritance works, and how methods are resolved in a complex hierarchy.\\n\\nMethod Resolution Order (MRO)\\nSome people don\\'t like multiple inheritance because of the constraints it has in other\\nprogramming languages, for instance, the so-called diamond problem. When a class\\nextends from two or more, and all of those classes also extend from other base classes, the\\nbottom ones will have multiple ways to resolve the methods coming from the top-level\\nclasses. The question is, which of these implementations is used?\\nConsider the following diagram, which has a structure with multiple inheritance. The toplevel class has a class attribute and implements the __str__ method. Think of any of the\\nconcrete classes, for example, ConcreteModuleA12\\xe2\\x80\\x94it extends from BaseModule1 and\\nBaseModule2, and each one of them will take the implementation of __str__ from\\nBaseModule. Which of these two methods is going to be the one for ConcreteModuleA12?\\n\\n[ 82 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nWith the value of the class attribute, this will become evident:\\nclass BaseModule:\\nmodule_name = \"top\"\\ndef __init__(self, module_name):\\nself.name = module_name\\ndef __str__(self):\\nreturn f\"{self.module_name}:{self.name}\"\\n\\nclass BaseModule1(BaseModule):\\nmodule_name = \"module-1\"\\n\\nclass BaseModule2(BaseModule):\\nmodule_name = \"module-2\"\\n\\nclass BaseModule3(BaseModule):\\nmodule_name = \"module-3\"\\n\\nclass ConcreteModuleA12(BaseModule1, BaseModule2):\\n\"\"\"Extend 1 & 2\"\"\"\\n\\n[ 83 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nclass ConcreteModuleB23(BaseModule2, BaseModule3):\\n\"\"\"Extend 2 & 3\"\"\"\\n\\nNow, let\\'s test this to see what method is being called:\\n>>> str(ConcreteModuleA12(\"test\"))\\n\\'module-1:test\\'\\n\\nThere is no collision. Python resolves this by using an algorithm called C3 linearization or\\nMRO, which defines a deterministic way in which methods are going to be called.\\nIn fact, we can specifically ask the class for its resolution order:\\n>>> [cls.__name__ for cls in ConcreteModuleA12.mro()]\\n[\\'ConcreteModuleA12\\', \\'BaseModule1\\', \\'BaseModule2\\', \\'BaseModule\\', \\'object\\']\\n\\nKnowing about how the method is going to be resolved in a hierarchy can be used to our\\nadvantage when designing classes because we can make use of mixins.\\n\\nMixins\\nA mixin is a base class that encapsulates some common behavior with the goal of reusing\\ncode. Typically, a mixin class is not useful on its own, and extending this class alone will\\ncertainly not work, because most of the time it depends on methods and properties that are\\ndefined in other classes. The idea is to use mixin classes along with other ones, through\\nmultiple inheritance, so that the methods or properties used on the mixin will be available.\\nImagine we have a simple parser that takes a string and provides iteration over it by its\\nvalues separated by hyphens (-):\\nclass BaseTokenizer:\\ndef __init__(self, str_token):\\nself.str_token = str_token\\ndef __iter__(self):\\nyield from self.str_token.split(\"-\")\\n\\nThis is quite straightforward:\\n>>> tk = BaseTokenizer(\"28a2320b-fd3f-4627-9792-a2b38e3c46b0\")\\n>>> list(tk)\\n[\\'28a2320b\\', \\'fd3f\\', \\'4627\\', \\'9792\\', \\'a2b38e3c46b0\\']\\n\\n[ 84 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nBut now we want the values to be sent in upper-case, without altering the base class. For\\nthis simple example, we could just create a new class, but imagine that a lot of classes are\\nalready extending from BaseTokenizer, and we don\\'t want to replace all of them. We can\\nmix a new class into the hierarchy that handles this transformation:\\nclass UpperIterableMixin:\\ndef __iter__(self):\\nreturn map(str.upper, super().__iter__())\\n\\nclass Tokenizer(UpperIterableMixin, BaseTokenizer):\\npass\\n\\nThe new Tokenizer class is really simple. It doesn\\'t need any code because it takes\\nadvantage of the mixin. This type of mixing acts as a sort of decorator. Based on what we\\njust saw, Tokenizer will take __iter__ from the mixin, and this one, in turn, delegates to\\nthe next class on the line (by calling super()), which is the BaseTokenizer, but it\\nconverts its values to uppercase, creating the desired effect.\\n\\nArguments in functions and methods\\nIn Python, functions can be defined to receive arguments in several different ways, and\\nthese arguments can also be provided by callers in multiple ways.\\nThere is also an industry-wide set of practices for defining interfaces in software\\nengineering that closely relates to the definition of arguments in functions.\\nIn this section, we will first explore the mechanics of arguments in Python functions and\\nafterwards review the general principles of software engineering that relate to good\\npractices on this subject to finally relate both concepts.\\n\\nHow function arguments work in Python\\nFirst, we will explore the particularities of how arguments are passed to functions in\\nPython, and then we will review the general theory of good software engineering practices\\nthat relate to these concepts.\\n\\n[ 85 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nBy first understanding the possibilities that Python offers for handling parameters, we will\\nbe able to assimilate general rules more easily, and the idea is that after having done so, we\\ncan easily draw conclusions on what good patterns or idioms are when handling\\narguments. Then, we can identify in which scenarios the Pythonic approach is the correct\\none, and in which cases we might be abusing the features of the language.\\n\\nHow arguments are copied to functions\\nThe first rule in Python is that all arguments are passed by a value. Always. This means\\nthat when passing values to functions, they are assigned to the variables on the signature\\ndefinition of the function to be later used on it. You will notice that a function changing\\narguments might depend on the type arguments\\xe2\\x80\\x94if we are passing mutable objects, and\\nthe body of the function modifies this, then, of course, we have side-effect that they will\\nhave been changed by the time the function returns.\\nIn the following we can see the difference:\\n>>> def function(argument):\\n...\\nargument += \" in function\"\\n...\\nprint(argument)\\n...\\n>>> immutable = \"hello\"\\n>>> function(immutable)\\nhello in function\\n>>> mutable = list(\"hello\")\\n>>> immutable\\n\\'hello\\'\\n>>> function(mutable)\\n[\\'h\\', \\'e\\', \\'l\\', \\'l\\', \\'o\\', \\' \\', \\'i\\', \\'n\\', \\' \\', \\'f\\', \\'u\\', \\'n\\', \\'c\\', \\'t\\', \\'i\\',\\n\\'o\\', \\'n\\']\\n>>> mutable\\n[\\'h\\', \\'e\\', \\'l\\', \\'l\\', \\'o\\', \\' \\', \\'i\\', \\'n\\', \\' \\', \\'f\\', \\'u\\', \\'n\\', \\'c\\', \\'t\\', \\'i\\',\\n\\'o\\', \\'n\\']\\n>>>\\n\\nThis might look like an inconsistency, but it\\'s not. When we pass the first argument, a\\nstring, this is assigned to the argument on the function. Since string objects are immutable,\\na statement like \"argument += <expression>\" will in fact create the new object,\\n\"argument + <expression>\", and assign that back to the argument. At that point, an\\nargument is just a local variable inside the scope of the function and has nothing to do with\\nthe original one in the caller.\\n\\n[ 86 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nOn the other hand, when we pass list, which is a mutable object, then that statement has\\na different meaning (it\\'s actually equivalent to calling .extend() on that list). This\\noperator acts by modifying the list in-place over a variable that holds a reference to the\\noriginal list object, hence modifying it.\\nWe have to be careful when dealing with these types of parameter because it can lead to\\nunexpected side-effects. Unless you are absolutely sure that it is correct to manipulate\\nmutable arguments in this way, we would recommend avoiding it and going for\\nalternatives without these problems.\\nDon\\'t mutate function arguments. In general, try to avoid side-effects in\\nfunctions as much as possible.\\n\\nArguments in Python can be passed by position, as in many other programming languages,\\nbut also by keyword. This means that we can explicitly tell the function which values we\\nwant for which of its parameters. The only caveat is that after a parameter is passed by\\nkeyword, the rest that follow must also be passed this way, otherwise, SyntaxError will\\nbe raised.\\n\\nVariable number of arguments\\nPython, as well as other languages, has built-in functions and constructions that can take a\\nvariable number of arguments. Consider for example string interpolation functions\\n(whether it be by using the % operator or the format method for strings), which follow a\\nsimilar structure to the printf function in C, a first positional parameter with the string\\nformat, followed by any number of arguments that will be placed on the markers of that\\nformatting string.\\nBesides taking advantage of these functions that are available in Python, we can also create\\nour own, which will work in a similar fashion. In this section, we will cover the basic\\nprinciples of functions with a variable number of arguments, along with some\\nrecommendations, so that in the next section, we can explore how to use these features to\\nour advantage when dealing with common problems, issues, and constraints that functions\\nmight have if they have too many arguments.\\nFor a variable number of positional arguments, the star symbol (*) is used, preceding the\\nname of the variable that is packing those arguments. This works through the packing\\nmechanism of Python.\\n\\n[ 87 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nLet\\'s say there is a function that takes three positional arguments. In one part of the code,\\nwe conveniently happen to have the arguments we want to pass to the function inside a list,\\nin the same order as they are expected by the function. Instead of passing them one by one\\nby the position (that is, list[0] to the first element, list[1] to the second, and so on),\\nwhich would be really un-Pythonic, we can use the packing mechanism and pass them all\\ntogether in a single instruction:\\n>>> def f(first, second, third):\\n...\\nprint(first)\\n...\\nprint(second)\\n...\\nprint(third)\\n...\\n>>> l = [1, 2, 3]\\n>>> f(*l)\\n1\\n2\\n3\\n\\nThe nice thing about the packing mechanism is that it also works the other way around. If\\nwe want to extract the values of a list to variables, by their respective position, we can\\nassign them like this:\\n>>>\\n>>>\\n1\\n>>>\\n2\\n>>>\\n3\\n\\na, b, c = [1, 2, 3]\\na\\nb\\nc\\n\\nPartial unpacking is also possible. Let\\'s say we are just interested in the first values of a\\nsequence (this can be a list, tuple, or something else), and after some point we just want the\\nrest to be kept together. We can assign the variables we need and leave the rest under a\\npackaged list. The order in which we unpack is not limited. If there is nothing to place in\\none of the unpacked subsections, the result will be an empty list. The reader is encouraged\\nto try examples such as those presented in the following listing on a Python terminal, and\\nalso to explore that unpacking works with generators as well:\\n>>> def show(e, rest):\\n...\\nprint(\"Element: {0} - Rest: {1}\".format(e, rest))\\n...\\n>>> first, *rest = [1, 2, 3, 4, 5]\\n>>> show(first, rest)\\nElement: 1 - Rest: [2, 3, 4, 5]\\n>>> *rest, last = range(6)\\n>>> show(last, rest)\\n\\n[ 88 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nElement: 5 - Rest: [0, 1, 2, 3, 4]\\n>>> first, *middle, last = range(6)\\n>>> first\\n0\\n>>> middle\\n[1, 2, 3, 4]\\n>>> last\\n5\\n>>> first, last, *empty = (1, 2)\\n>>> first\\n1\\n>>> last\\n2\\n>>> empty\\n[]\\n\\nOne of the best uses for unpacking variables can be found in iteration. When we have to\\niterate over a sequence of elements, and each element is, in turn, a sequence, it is a good\\nidea to unpack at the same time each element is being iterated over. To see an example of\\nthis in action, we are going to pretend that we have a function that receives a list of\\ndatabase rows, and that it is in charge of creating users out of that data. The first\\nimplementation takes the values to construct the user with from the position of each\\ncolumn in the row, which is not idiomatic at all. The second implementation uses\\nunpacking while iterating:\\nUSERS = [(i, f\"first_name_{i}\", \"last_name_{i}\") for i in range(1_000)]\\n\\nclass User:\\ndef __init__(self, user_id, first_name, last_name):\\nself.user_id = user_id\\nself.first_name = first_name\\nself.last_name = last_name\\n\\ndef bad_users_from_rows(dbrows) -> list:\\n\"\"\"A bad case (non-pythonic) of creating ``User``s from DB rows.\"\"\"\\nreturn [User(row[0], row[1], row[2]) for row in dbrows]\\n\\ndef users_from_rows(dbrows) -> list:\\n\"\"\"Create ``User``s from DB rows.\"\"\"\\nreturn [\\nUser(user_id, first_name, last_name)\\nfor (user_id, first_name, last_name) in dbrows\\n]\\n\\n[ 89 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nNotice that the second version is much easier to read. In the first version of the function\\n(bad_users_from_rows), we have data expressed in the form row[0], row[1], and\\nrow[2], which doesn\\'t tell us anything about what they are. On the other hand, variables\\nsuch as user_id, first_name, and last_name speak for themselves.\\nWe can leverage this kind of functionality to our advantage when designing our own\\nfunctions.\\nAn example of this that we can find in the standard library lies in the max function, which is\\ndefined as follows:\\nmax(...)\\nmax(iterable, *[, default=obj, key=func]) -> value\\nmax(arg1, arg2, *args, *[, key=func]) -> value\\nWith a single iterable argument, return its biggest item. The\\ndefault keyword-only argument specifies an object to return if\\nthe provided iterable is empty.\\nWith two or more arguments, return the largest argument.\\n\\nThere is a similar notation, with two stars (**) for keyword arguments. If we have a\\ndictionary and we pass it with a double star to a function, what it will do is pick the keys as\\nthe name for the parameter, and pass the value for that key as the value for that parameter\\nin that function.\\nFor instance, check this out:\\nfunction(**{\"key\": \"value\"})\\n\\nIt is the same as the following:\\nfunction(key=\"value\")\\n\\nConversely, if we define a function with a parameter starting with two-star symbols, the\\nopposite will happen\\xe2\\x80\\x94keyword-provided parameters will be packed into a dictionary:\\n>>> def function(**kwargs):\\n...\\nprint(kwargs)\\n...\\n>>> function(key=\"value\")\\n{\\'key\\': \\'value\\'}\\n\\n[ 90 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThe number of arguments in functions\\nIn this section, we agree on the idea that having functions or methods that take too many\\narguments is a sign of bad design (a code smell). Then, we propose ways of dealing with\\nthis issue.\\nThe first alternative is a more general principle of software design\\xe2\\x80\\x94reification (creating a\\nnew object for all of those arguments that we are passing, which is probably the abstraction\\nwe are missing). Compacting multiple arguments into a new object is not a solution specific\\nto Python, but rather something that we can apply in any programming language.\\nAnother option would be to use the Python-specific features we saw in the previous\\nsection, making use of variable positional and keyword arguments to create functions that\\nhave a dynamic signature. While this might be a Pythonic way of proceeding, we have to be\\ncareful not to abuse the feature, because we might be creating something that is so dynamic\\nthat it is hard to maintain. In this case, we should take a look at the body of the function.\\nRegardless of the signature, and whether the parameters seem to be correct, if the function\\nis doing too many different things responding to the values of the parameters, then it is a\\nsign that it has to be broken down into multiple smaller functions (remember, functions\\nshould do one thing, and one thing only!).\\n\\nFunction arguments and coupling\\nThe more arguments a function signature has, the more likely this one is going to be tightly\\ncoupled with the caller function.\\nLet\\'s say we have two functions, f1, and f2, and the latter takes five parameters. The more\\nparameters f2 takes, the more difficult it would be for anyone trying to call that function to\\ngather all that information and pass it along so that it can work properly.\\nNow, f1 seems to have all of this information because it can call it correctly. From this, we\\ncan derive two conclusions: first, f2 is probably a leaky abstraction, which means that\\nsince f1 knows everything that f2 requires, it can pretty much figure out what it is doing\\ninternally and will be able to do it by itself. So, all in all, f2 is not abstracting that much.\\nSecond, it looks like f2 is only useful to f1, and it is hard to imagine using this function in a\\ndifferent context, making it harder to reuse.\\nWhen functions have a more general interface and are able to work with higher-level\\nabstractions, they become more reusable.\\n\\n[ 91 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nThis applies to all sort of functions and object methods, including the __init__ method for\\nclasses. The presence of a method like this could generally (but not always) mean that a\\nnew higher-level abstraction should be passed instead, or that there is a missing object.\\nIf a function needs too many parameters to work properly, consider it a\\ncode smell.\\n\\nIn fact, this is such a design problem that static analysis tools such as pylint (discussed in\\nChapter 1, Introduction, Code Formatting, and Tools) will, by default, raise a warning about\\nwhen they encounter such a case. When this happens, don\\'t suppress the warning\\xe2\\x80\\x94refactor\\nit instead.\\n\\nCompact function signatures that take too many\\narguments\\nSuppose we find a function that requires too many parameters. We know that we cannot\\nleave the code base like that, and a refactor is imperative. But, what are the options?\\nDepending on the case, some of the following rules might apply. This is by no means\\nextensive, but it does provide an idea of how to solve some scenarios that occur quite often.\\nSometimes, there is an easy way to change parameters if we can see that most of them\\nbelong to a common object. For example, consider a function call like this one:\\ntrack_request(request.headers, request.ip_addr, request.request_id)\\n\\nNow, the function might or might not take additional arguments, but something is really\\nobvious here: all of the parameters depend upon request, so why not pass the\\nrequest object instead? This is a simple change, but it significantly improves the code. The\\ncorrect function call should be track_request(request)\\xe2\\x80\\x94not to mention that,\\nsemantically, it also makes much more sense.\\nWhile passing around parameters like this is encouraged, in all cases where we pass\\nmutable objects to functions, we must be really careful about side-effects. The function we\\nare calling should not make any modifications to the object we are passing because that will\\nmutate the object, creating an undesired side-effect. Unless this is actually the desired effect\\n(in which case, it must be made explicit), this kind of behavior is discouraged. Even when\\nwe actually want to change something on the object we are dealing with, a better\\nalternative would be to copy it and return a (new) modified version of it.\\n\\n[ 92 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nWork with immutable objects, and avoid side-effects as much as possible.\\n\\nThis brings us to a similar topic: grouping parameters. In the previous example, the\\nparameters were already grouped, but the group (in this case, the request object) was not\\nbeing used. But other cases are not as obvious as that one, and we might want to group all\\nthe data in the parameters in a single object that acts as a container. Needless to say, this\\ngrouping has to make sense. The idea here is to reify: create the abstraction that was missing\\nfrom our design.\\nIf the previous strategies don\\'t work, as a last resort we can change the signature of the\\nfunction to accept a variable number of arguments. If the number of arguments is too big,\\nusing *args or **kwargs will make things harder to follow, so we have to make sure that\\nthe interface is properly documented and correctly used, but in some cases this is worth\\ndoing.\\nIt\\'s true that a function defined with *args and **kwargs is really flexible and adaptable,\\nbut the disadvantage is that it loses its signature, and with that, part of its meaning, and\\nalmost all of its legibility. We have seen examples of how names for variables (including\\nfunction arguments) make the code much easier to read. If a function will take any number\\nof arguments (positional or keyword), we might find out that when we want to take a look\\nat that function in the future, we probably won\\'t know exactly what it was supposed to do\\nwith its parameters, unless it has a very good docstring.\\n\\nFinal remarks on good practices for\\nsoftware design\\nA good software design involves a combination of following good practices of software\\nengineering and taking advantage of most of the features of the language. There is a great\\nvalue in using everything that Python has to offer, but there is also a great risk of abusing\\nthis and trying to fit complex features into simple designs.\\nIn addition to this general principle, it would be good to add some final recommendations.\\n\\n[ 93 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nOrthogonality in software\\nThis word is very general and can have multiple meanings or interpretations. In math,\\northogonal means that two elements are independent. If two vectors are orthogonal, their\\nscalar product is zero. It also means they are not related at all: a change in one of them\\ndoesn\\'t affect the other one at all. That\\'s the way we should think about our software.\\nChanging a module, class, or function should have no impact on the outside world to that\\ncomponent that is being modified. This is of course highly desirable, but not always\\npossible. But even for cases where it\\'s not possible, a good design will try to minimize the\\nimpact as much as possible. We have seen ideas such as separation of concerns, cohesion,\\nand isolation of components.\\nIn terms of the runtime structure of software, orthogonality can be interpreted as the fact\\nthat makes changes (or side-effects) local. This means, for instance, that calling a method on\\nan object should not alter the internal state of other (unrelated) objects. We have already\\n(and will continue to do so) emphasized in this book the importance of minimizing sideeffects in our code.\\nIn the example with the mixin class, we created a tokenizer object that returned an\\niterable. The fact that the __iter__ method returned a new generator increases the chances\\nthat all three classes (the base, the mixing, and the concrete class) are orthogonal. If this had\\nreturned something in concrete (a list, let\\'s say), this would have created a dependency on\\nthe rest of the classes, because when we changed the list to something else, we might have\\nneeded to update other parts of the code, revealing that the classes were not as independent\\nas they should be.\\nLet\\'s show you a quick example. Python allows passing functions by parameter because\\nthey are just regular objects. We can use this feature to achieve some orthogonality. We\\nhave a function that calculates a price, including taxes and discounts, but afterward we\\nwant to format the final price that\\'s obtained:\\ndef calculate_price(base_price: float, tax: float, discount: float) ->\\nreturn (base_price * (1 + tax)) * (1 - discount)\\n\\ndef show_price(price: float) -> str:\\nreturn \"$ {0:,.2f}\".format(price)\\n\\ndef str_final_price(\\nbase_price: float, tax: float, discount: float, fmt_function=str\\n) -> str:\\nreturn fmt_function(calculate_price(base_price, tax, discount))\\n\\n[ 94 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nNotice that the top-level function is composing two orthogonal functions. One thing to\\nnotice is how we calculate the price, which is how the other one is going to be represented.\\nChanging one does not change the other. If we don\\'t pass anything in particular, it will use\\nstring conversion as the default representation function, and if we choose to pass a custom\\nfunction, the resulting string will change. However, changes in show_price do not affect\\ncalculate_price. We can make changes to either function, knowing that the other one\\nwill remain as it was:\\n>>> str_final_price(10, 0.2, 0.5)\\n\\'6.0\\'\\n>>> str_final_price(1000, 0.2, 0)\\n\\'1200.0\\'\\n>>> str_final_price(1000, 0.2, 0.1, fmt_function=show_price)\\n\\'$ 1,080.00\\'\\n\\nThere is an interesting quality aspect that relates to orthogonality. If two parts of the code\\nare orthogonal, it means one can change without affecting the other. This implies that the\\npart that changed has unit tests that are also orthogonal to the unit tests of the rest of the\\napplication. Under this assumption, if those tests pass, we can assume (up to a certain\\ndegree) that the application is correct without needing full regression testing.\\nMore broadly, orthogonality can be thought of in terms of features. Two functionalities of\\nthe application can be totally independent so that they can be tested and released without\\nhaving to worry that one might break the other (or the rest of the code, for that matter).\\nImagine that the project requires a new authentication mechanism (oauth2, let\\'s say, but\\njust for the sake of the example), and at the same time another team is also working on a\\nnew report. Unless there is something fundamentally wrong in that system, neither of those\\nfeatures should impact the other. Regardless of which one of those gets merged first, the\\nother one should not be affected at all.\\n\\nStructuring the code\\nThe way code is organized also impacts the performance of the team and its\\nmaintainability.\\nIn particular, having large files with lots of definitions (classes, functions, constants, and so\\non) is a bad practice and should be discouraged. This doesn\\'t mean going to the extreme of\\nplacing one definition per file, but a good code base will structure and arrange components\\nby similarity.\\n\\n[ 95 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nLuckily, most of the time, changing a large file into smaller ones is not a hard task in\\nPython. Even if multiple other parts of the code depend on definitions made on that file,\\nthis can be broken down into a package, and will maintain total compatibility. The idea\\nwould be to create a new directory with a __init__.py file on it (this will make it a\\nPython package). Alongside this file, we will have multiple files with all the particular\\ndefinitions each one requires (fewer functions and classes grouped by a certain criterion).\\nThen, the __init__.py file will import from all the other files the definitions it previously\\nhad (which is what guarantees its compatibility). Additionally, these definitions can be\\nmentioned in the __all__ variable of the module to make them exportable.\\nThere are many advantages of this. Other than the fact that each file will be easier to\\nnavigate, and things will be easier to find, we could argue that it will be more efficient\\nbecause of the following reasons:\\nIt contains fewer objects to parse and load into memory when the module is\\nimported\\nThe module itself will probably be importing fewer modules because it needs\\nfewer dependencies, like before\\nIt also helps to have a convention for the project. For example, instead of placing\\nconstants in all of the files, we can create a file specific to the constant values to be used in\\nthe project, and import it from there:\\nfrom mypoject.constants import CONNECTION_TIMEOUT\\n\\nCentralizing information like this makes it easier to reuse code and helps to avoid\\ninadvertent duplication.\\nMore details about separating modules and creating Python packages will be discussed in\\nChapter 10, Clean Architecture, when we explore this in the context of software architecture.\\n\\nSummary\\nIn this chapter, we have explored several principles to achieve a clean design.\\nUnderstanding that the code is part of the design is key to achieving high-quality software.\\nThis and the following chapter are focused precisely on that.\\nWith these ideas, we can now construct more robust code. For example, by applying DbC,\\nwe can create components that are guaranteed to work under their constraints. More\\nimportantly, if errors occur, this will not happen out of the blue, but instead, we will have a\\nclear idea of who the offender is and which part of the code broke the contract. This\\ncompartmentalization is clear to effective debugging.\\n\\n[ 96 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nAlong similar lines, each component can be made more robust if it defends itself from\\nmalicious intent or incorrect input. Although this idea goes in a different direction from\\ndesign by contract, it might complement it very well. Defensive programming is a good\\nidea, especially for critical parts of the application.\\nFor both approaches (design by contract and defensive programming), it\\'s important to\\ncorrectly handle assertions. Keep in mind how they should be used in Python, and don\\'t\\nuse assertions as part of the control flow logic of the program. Don\\'t catch this exception,\\neither.\\nSpeaking of exceptions, it\\'s important to know how and when to use them, and the most\\nimportant concept here is to avoid using exception as a control flow (go-to) kind of\\nconstruction.\\nWe have explored a recurrent topic in object-oriented design: deciding between using\\ninheritance or composition. The main lesson here is not to use one over the other, but to use\\nwhichever option is better; we should also avoid some common anti-patterns, which we\\nmight often see in Python (especially given its highly dynamic nature).\\nFinally, we discussed the number of arguments in functions, along with heuristics for a\\nclean design, always with the particularities of Python in mind.\\nThese concepts are fundamental design ideas that lay the foundations for what\\'s coming in\\nthe next chapter. We need to first understand these ideas so that we can move on to more\\nadvanced topics, such as SOLID principles.\\n\\nReferences\\nHere is a list of information you can refer to:\\nObject-Oriented Software Construction, Second Edition, written by Bertrand Meyer\\nThe Pragmatic Programmer: From Journeyman to Master, by Andrew Hunt and\\nDavid Thomas published by Addison-Wesley, 2000.\\nPEP-316: Programming by Contract for Python (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\npeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0316/\\xe2\\x80\\x8b)\\nREAL 01: The Most Diabolical Python Antipattern: https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8brealpython.\\xe2\\x80\\x8bcom/\\nblog/\\xe2\\x80\\x8bpython/\\xe2\\x80\\x8bthe-\\xe2\\x80\\x8bmost-\\xe2\\x80\\x8bdiabolical-\\xe2\\x80\\x8bpython-\\xe2\\x80\\x8bantipattern/\\xe2\\x80\\x8b\\n\\nPEP-3134: Exception Chaining and Embedded Tracebacks (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\norg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b3134/\\xe2\\x80\\x8b)\\n\\n[ 97 ]\\n\\n\\x0cGeneral Traits of Good Code\\n\\nChapter 3\\n\\nIdiomatic Python: EAFP versus LBYL (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bblogs.\\xe2\\x80\\x8bmsdn.\\xe2\\x80\\x8bmicrosoft.\\xe2\\x80\\x8bcom/\\npythonengineering/\\xe2\\x80\\x8b2016/\\xe2\\x80\\x8b06/\\xe2\\x80\\x8b29/\\xe2\\x80\\x8bidiomatic-\\xe2\\x80\\x8bpython-\\xe2\\x80\\x8beafp-\\xe2\\x80\\x8bversus-\\xe2\\x80\\x8blbyl/\\xe2\\x80\\x8b)\\nComposition vs. Inheritance: How to Choose? (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bthoughtworks.\\xe2\\x80\\x8bcom/\\ninsights/\\xe2\\x80\\x8bblog/\\xe2\\x80\\x8bcomposition-\\xe2\\x80\\x8bvs-\\xe2\\x80\\x8binheritance-\\xe2\\x80\\x8bhow-\\xe2\\x80\\x8bchoose)\\nPython HTTP (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bhttp.\\xe2\\x80\\x8bserver.\\xe2\\x80\\x8bhtml#http.\\nserver.\\xe2\\x80\\x8bBaseHTTPRequestHandler)\\nSource reference for exceptions in the requests library (http:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpythonrequests.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8ben/\\xe2\\x80\\x8bmaster/\\xe2\\x80\\x8b_\\xe2\\x80\\x8bmodules/\\xe2\\x80\\x8brequests/\\xe2\\x80\\x8bexceptions/\\xe2\\x80\\x8b)\\nCode Complete: A Practical Handbook of Software Construction, Second Edition,\\nwritten by Steve McConnell\\n\\n[ 98 ]\\n\\n\\x0c4\\nThe SOLID Principles\\nIn this chapter, we will continue to explore concepts of clean design applied to Python. In\\nparticular, we will review the so-called SOLID principles, and how to implement them in a\\nPythonic way. These principles entail a series of good practices to achieve better-quality\\nsoftware. In case some of us aren\\'t aware of what SOLID stands for, here it is:\\nS: Single responsibility principle\\nO: Open/closed principle\\nL: Liskov\\'s substitution principle\\nI: Interface segregation principle\\nD: Dependency inversion principle\\nThe goals of this chapter are as follows:\\nTo become acquainted with SOLID principles for software design\\nTo design software components that follow the single responsibility principle\\nTo achieve more maintainable code through the open/closed principle\\nTo implement proper class hierarchies in object-oriented design, by complying\\nwith Liskov\\'s substitution principle\\nTo design with interface segregation and dependency inversion\\n\\nSingle responsibility principle\\nThe single responsibility principle (SRP) states that a software component (in general, a\\nclass) must have only one responsibility. The fact that the class has a sole responsibility\\nmeans that it is in charge of doing just one concrete thing, and as a consequence of that, we\\ncan conclude that it must have only one reason to change.\\nOnly if one thing on the domain problem changes will the class have to be updated. If we\\nhave to make modifications to a class, for different reasons, it means the abstraction is\\nincorrect, and that the class has too many responsibilities.\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nAs introduced in Chapter 2, Pythonic Code, this design principle helps us build\\nmore cohesive abstractions; objects that do one thing, and just one thing, well, following the\\nUnix philosophy. What we want to avoid in all cases is having objects with multiple\\nresponsibilities (often called god-objects, because they know too much, or more than they\\nshould). These objects group different (mostly unrelated) behaviors, thus making them\\nharder to maintain.\\nAgain, the smaller the class, the better.\\nThe SRP is closely related to the idea of cohesion in software design, which we already\\nexplored in Chapter 3, General Traits of Good Code, when we discussed separation of\\nconcerns in software. What we strive to achieve here is that classes are designed in such a\\nway that most of their properties and their attributes are used by its methods, most of the\\ntime. When this happens, we know they are related concepts, and therefore it makes sense\\nto group them under the same abstraction.\\nIn a way, this idea is somehow similar to the concept of normalization on relational\\ndatabase design. When we detect that there are partitions on the attributes or methods of\\nthe interface of an object, they might as well be moved somewhere else\\xe2\\x80\\x94it is a sign that\\nthey are two or more different abstractions mixed into one.\\nThere is another way of looking at this principle. If, when looking at a class, we find\\nmethods that are mutually exclusive and do not relate to each other, they are the different\\nresponsibilities that have to be broken down into smaller classes.\\n\\nA class with too many responsibilities\\nIn this example, we are going to create the case for an application that is in charge of\\nreading information about events from a source (this could be log files, a database, or many\\nmore sources), and identifying the actions corresponding to each particular log.\\nA design that fails to conform to the SRP would look like this:\\n\\n[ 100 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nWithout considering the implementation, the code for the class might look in the following\\nlisting:\\n# srp_1.py\\nclass SystemMonitor:\\ndef load_activity(self):\\n\"\"\"Get the events from a source, to be processed.\"\"\"\\ndef identify_events(self):\\n\"\"\"Parse the source raw data into events (domain objects).\"\"\"\\ndef stream_events(self):\\n\"\"\"Send the parsed events to an external agent.\"\"\"\\n\\nThe problem with this class is that it defines an interface with a set of methods that\\ncorrespond to actions that are orthogonal: each one can be done independently of the rest.\\nThis design flaw makes the class rigid, inflexible, and error-prone because it is hard to\\nmaintain. In this example, each method represents a responsibility of the class. Each\\nresponsibility entails a reason why the class might need to be modified. In this case, each\\nmethod represents one of the various reasons why the class will have to be modified.\\nConsider the loader method, which retrieves the information from a particular source.\\nRegardless of how this is done (we can abstract the implementation details here), it is clear\\nthat it will have its own sequence of steps, for instance connecting to the data source,\\nloading the data, parsing it into the expected format, and so on. If any of this changes (for\\nexample, we want to change the data structure used for holding the data), the\\nSystemMonitor class will need to change. Ask yourself whether this makes sense. Does a\\nsystem monitor object have to change because we changed the representation of the data?\\nNo.\\nThe same reasoning applies to the other two methods. If we change how we fingerprint\\nevents, or how we deliver them to another data source, we will end up making changes to\\nthe same class.\\nIt should be clear by now that this class is rather fragile, and not very maintainable. There\\nare lots of different reasons that will impact on changes in this class. Instead, we want\\nexternal factors to impact our code as little as possible. The solution, again, is to create\\nsmaller and more cohesive abstractions.\\n\\n[ 101 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nDistributing responsibilities\\nTo make the solution more maintainable, we separate every method into a different class.\\nThis way, each class will have a single responsibility:\\n\\nThe same behavior is achieved by using an object that will interact with instances of these\\nnew classes, using those objects as collaborators, but the idea remains that each class\\nencapsulates a specific set of methods that are independent of the rest. The idea now is that\\nchanges on any of these classes do not impact the rest, and all of them have a clear and\\nspecific meaning. If we need to change something on how we load events from the data\\nsources, the alert system is not even aware of these changes, so we do not have to modify\\nanything on the system monitor (as long as the contract is still preserved), and the data\\ntarget is also unmodified.\\nChanges are now local, the impact is minimal, and each class is easier to maintain.\\nThe new classes define interfaces that are not only more maintainable but also reusable.\\nImagine that now, in another part of the application, we also need to read the activity from\\nthe logs, but for different purposes. With this design, we can simply use objects of type\\nActivityReader (which would actually be an interface, but for the purposes of this\\nsection, that detail is not relevant and will be explained later for the next principles). This\\nwould make sense, whereas it would not have made sense in the previous design, because\\nattempts to reuse the only class we had defined would have also carried extra methods\\n(such as identify_events(), or stream_events()) that were not needed at all.\\nOne important clarification is that the principle does not mean at all that each class must\\nhave a single method. Any of the new classes might have extra methods, as long as they\\ncorrespond to the same logic that that class is in charge of handling.\\n\\n[ 102 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nThe open/closed principle\\nThe open/closed principle (OCP) states that a module should be both open and closed (but\\nwith respect to different aspects).\\nWhen designing a class, for instance, we should carefully encapsulate the logic so that it has\\ngood maintenance, meaning that we will want it to be open to extension but closed for\\nmodification.\\nWhat this means in simple terms is that, of course, we want our code to be extensible, to\\nadapt to new requirements, or changes in the domain problem. This means that, when\\nsomething new appears on the domain problem, we only want to add new things to our\\nmodel, not change anything existing that is closed to modification.\\nIf, for some reason, when something new has to be added, we found ourselves modifying\\nthe code, then that logic is probably poorly designed. Ideally, when requirements change,\\nwe want to just have to extend the module with the new required behavior in order to\\ncomply with the new requirements, but without having to modify the code.\\nThis principle applies to several software abstractions. It could be a class or even a module.\\nIn the following two subsections, we will see examples of each one, respectively.\\n\\nExample of maintainability perils for not following\\nthe open/closed principle\\nLet\\'s begin with an example of a system that is designed in such a way that does not follow\\nthe open/closed principle, in order to see the maintainability problems this carries, and the\\ninflexibility of such a design.\\nThe idea is that we have a part of the system that is in charge of identifying events as they\\noccur in another system, which is being monitored. At each point, we want this component\\nto identify the type of event, correctly, according to the values of the data that was\\npreviously gathered (for simplicity, we will assume it is packaged into a dictionary, and\\nwas previously retrieved through another means such as logs, queries, and many more).\\nWe have a class that, based on this data, will retrieve the event, which is another type with\\nits own hierarchy.\\nA first attempt to solve this problem might look like this:\\n# openclosed_1.py\\nclass Event:\\ndef __init__(self, raw_data):\\n\\n[ 103 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nself.raw_data = raw_data\\n\\nclass UnknownEvent(Event):\\n\"\"\"A type of event that cannot be identified from its data.\"\"\"\\n\\nclass LoginEvent(Event):\\n\"\"\"A event representing a user that has just entered the system.\"\"\"\\n\\nclass LogoutEvent(Event):\\n\"\"\"An event representing a user that has just left the system.\"\"\"\\n\\nclass SystemMonitor:\\n\"\"\"Identify events that occurred in the system.\"\"\"\\ndef __init__(self, event_data):\\nself.event_data = event_data\\ndef identify_event(self):\\nif (\\nself.event_data[\"before\"][\"session\"] ==\\nand self.event_data[\"after\"][\"session\"]\\n):\\nreturn LoginEvent(self.event_data)\\nelif (\\nself.event_data[\"before\"][\"session\"] ==\\nand self.event_data[\"after\"][\"session\"]\\n):\\nreturn LogoutEvent(self.event_data)\\n\\n0\\n== 1\\n\\n1\\n== 0\\n\\nreturn UnknownEvent(self.event_data)\\n\\nThe following is the expected behavior of the preceding code:\\n>>> l1 = SystemMonitor({\"before\": {\"session\": 0}, \"after\": {\"session\": 1}})\\n>>> l1.identify_event().__class__.__name__\\n\\'LoginEvent\\'\\n>>> l2 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 0}})\\n>>> l2.identify_event().__class__.__name__\\n\\'LogoutEvent\\'\\n>>> l3 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 1}})\\n>>> l3.identify_event().__class__.__name__\\n\\'UnknownEvent\\'\\n\\n[ 104 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nWe can clearly notice the hierarchy of event types, and some business logic to construct\\nthem. For instance, when there was no previous flag for a session, but there is now, we\\nidentify that record as a login event. Conversely, when the opposite happens, it means that\\nit was a logout event. If it was not possible to identify an event, an event of type unknown\\nis returned. This is to preserve polymorphism by following the null object pattern (instead\\nof returning None, it retrieves an object of the corresponding type with some default logic).\\nThe null object pattern is described in Chapter 9, Common Design Patterns.\\nThis design has some problems. The first issue is that the logic for determining the types of\\nevents is centralized inside a monolithic method. As the number of events we want to\\nsupport grows, this method will as well, and it could end up being a very long method,\\nwhich is bad because, as we have already discussed, it will not be doing just one thing and\\none thing well.\\nOn the same line, we can see that this method is not closed for modification. Every time we\\nwant to add a new type of event to the system, we will have to change something in this\\nmethod (not to mention, that the chain of elif statements will be a nightmare to read!).\\nWe want to be able to add new types of event without having to change this method\\n(closed for modification). We also want to be able to support new types of event (open for\\nextension) so that when a new event is added, we only have to add code, not change the\\ncode that already exists.\\n\\nRefactoring the events system for extensibility\\nThe problem with the previous example was that the SystemMonitor class was interacting\\ndirectly with the concrete classes it was going to retrieve.\\nIn order to achieve a design that honors the open/closed principle, we have to design\\ntoward abstractions.\\nA possible alternative would be to think of this class as it collaborates with the events, and\\nthen we delegate the logic for each particular type of event to its corresponding class:\\n\\n[ 105 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nThen we have to add a new (polymorphic) method to each type of event with the single\\nresponsibility of determining if it corresponds to the data being passed or not, and we also\\nhave to change the logic to go through all events, finding the right one.\\nThe new code should look like this:\\n# openclosed_2.py\\nclass Event:\\ndef __init__(self, raw_data):\\nself.raw_data = raw_data\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn False\\n\\nclass UnknownEvent(Event):\\n\"\"\"A type of event that cannot be identified from its data\"\"\"\\n\\nclass LoginEvent(Event):\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn (\\nevent_data[\"before\"][\"session\"] == 0\\nand event_data[\"after\"][\"session\"] == 1\\n)\\n\\nclass LogoutEvent(Event):\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn (\\nevent_data[\"before\"][\"session\"] == 1\\nand event_data[\"after\"][\"session\"] == 0\\n)\\n\\nclass SystemMonitor:\\n\"\"\"Identify events that occurred in the system.\"\"\"\\ndef __init__(self, event_data):\\nself.event_data = event_data\\ndef identify_event(self):\\nfor event_cls in Event.__subclasses__():\\ntry:\\nif event_cls.meets_condition(self.event_data):\\n\\n[ 106 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nreturn event_cls(self.event_data)\\nexcept KeyError:\\ncontinue\\nreturn UnknownEvent(self.event_data)\\n\\nNotice how the interaction is now oriented toward an abstraction (in this case, it would be\\nthe generic base class Event, which might even be an abstract base class or an interface, but\\nfor the purposes of this example it is enough to have a concrete base class). The method no\\nlonger works with specific types of event, but just with generic events that follow a\\ncommon interface\\xe2\\x80\\x94they are all polymorphic with respect to the meets_condition\\nmethod.\\nNotice how events are discovered through the __subclasses__() method. Supporting\\nnew types of event is now just about creating a new class for that event that has to inherit\\nfrom Event and implement its own meets_condition() method, according to its specific\\nbusiness logic.\\n\\nExtending the events system\\nNow, let\\'s prove that this design is actually as extensible as we wanted it to be. Imagine that\\na new requirement arises, and we have to also support events that correspond to\\ntransactions that the user executed on the monitored system.\\nThe class diagram for the design has to include such a new event type, as in the following:\\n\\nOnly by adding the code to this new class does the logic keep working as expected:\\n# openclosed_3.py\\nclass Event:\\ndef __init__(self, raw_data):\\nself.raw_data = raw_data\\n@staticmethod\\n\\n[ 107 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\ndef meets_condition(event_data: dict):\\nreturn False\\n\\nclass UnknownEvent(Event):\\n\"\"\"A type of event that cannot be identified from its data\"\"\"\\n\\nclass LoginEvent(Event):\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn (\\nevent_data[\"before\"][\"session\"] == 0\\nand event_data[\"after\"][\"session\"] == 1\\n)\\n\\nclass LogoutEvent(Event):\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn (\\nevent_data[\"before\"][\"session\"] == 1\\nand event_data[\"after\"][\"session\"] == 0\\n)\\n\\nclass TransactionEvent(Event):\\n\"\"\"Represents a transaction that has just occurred on the system.\"\"\"\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn event_data[\"after\"].get(\"transaction\") is not None\\n\\nclass SystemMonitor:\\n\"\"\"Identify events that occurred in the system.\"\"\"\\ndef __init__(self, event_data):\\nself.event_data = event_data\\ndef identify_event(self):\\nfor event_cls in Event.__subclasses__():\\ntry:\\nif event_cls.meets_condition(self.event_data):\\nreturn event_cls(self.event_data)\\nexcept KeyError:\\ncontinue\\nreturn UnknownEvent(self.event_data)\\n\\n[ 108 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nWe can verify that the previous cases work as before and that the new event is also\\ncorrectly identified:\\n>>> l1 = SystemMonitor({\"before\": {\"session\": 0}, \"after\": {\"session\": 1}})\\n>>> l1.identify_event().__class__.__name__\\n\\'LoginEvent\\'\\n>>> l2 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 0}})\\n>>> l2.identify_event().__class__.__name__\\n\\'LogoutEvent\\'\\n>>> l3 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 1}})\\n>>> l3.identify_event().__class__.__name__\\n\\'UnknownEvent\\'\\n>>> l4 = SystemMonitor({\"after\": {\"transaction\": \"Tx001\"}})\\n>>> l4.identify_event().__class__.__name__\\n\\'TransactionEvent\\'\\n\\nNotice that the SystemMonitor.identify_event() method did not change at all when\\nwe added the new event type. We, therefore, say that this method is closed with respect to\\nnew types of event.\\nConversely, the Event class allowed us to add a new type of event when we were required\\nto do so. We then say that events are open for an extension with respect to new types.\\nThis is the true essence of this principle\\xe2\\x80\\x94when something new appears on the domain\\nproblem, we only want to add new code, not modify existing code.\\n\\nFinal thoughts about the OCP\\nAs you might have noticed, this principle is closely related to effective use of\\npolymorphism. We want to design toward abstractions that respect a polymorphic contract\\nthat the client can use, to a structure that is generic enough that extending the model is\\npossible, as long as the polymorphic relationship is preserved.\\nThis principle tackles an important problem in software engineering: maintainability .The\\nperils of not following the OCP are ripple effects and problems in the software where a\\nsingle change triggers changes all over the code base, or risks breaking other parts of the\\ncode.\\n\\n[ 109 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nOne important final note is that, in order to achieve this design in which we do not change\\nthe code to extend behavior, we need to be able to create proper closure against the\\nabstractions we want to protect (in this example, new types of event). This is not always\\npossible in all programs, as some abstractions might collide (for example, we might have a\\nproper abstraction that provides closure against a requirement, but does not work for other\\ntypes of requirements). In these cases, we need to be selective and apply a strategy that\\nprovides the best closure for the types of requirement that require to be the most extensible.\\n\\nLiskov\\'s substitution principle\\nLiskov\\'s substitution principle (LSP) states that there is a series of properties that an object\\ntype must hold to preserve reliability on its design.\\nThe main idea behind LSP is that, for any class, a client should be able to use any of its\\nsubtypes indistinguishably, without even noticing, and therefore without compromising\\nthe expected behavior at runtime. This means that clients are completely isolated and\\nunaware of changes in the class hierarchy.\\nMore formally, this is the original definition (LISKOV 01) of Liskov\\'s substitution principle:\\nif S is a subtype of T, then objects of type T may be replaced by objects of type S, without\\nbreaking the program.\\nThis can be understood with the help of a generic diagram such as the following one.\\nImagine that there is some client class that requires (includes) objects of another type.\\nGenerally speaking, we will want this client to interact with objects of some type, namely, it\\nwill work through an interface.\\nNow, this type might as well be just a generic interface definition, an abstract class or an\\ninterface, not a class with the behavior itself. There may be several subclasses extending this\\ntype (described in the diagram with the name Subtype, up to N). The idea behind this\\nprinciple is that, if the hierarchy is correctly implemented, the client class has to be able to\\nwork with instances of any of the subclasses without even noticing. These objects should be\\ninterchangeable, as shown here:\\n\\n[ 110 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nThis is related to other design principles we have already visited, like designing to\\ninterfaces. A good class must define a clear and concise interface, and as long as subclasses\\nhonor that interface, the program will remain correct.\\nAs a consequence of this, the principle also relates to the ideas behind designing by\\ncontract. There is a contract between a given type and a client. By following the rules of\\nLSP, the design will make sure that subclasses respect the contracts as they are defined by\\nparent classes.\\n\\nDetecting LSP issues with tools\\nThere are some scenarios so notoriously wrong with respect to the LSP that they can be\\neasily identified by the tools we have learned to configure in Chapter 1, Introduction, Code\\nFormatting, and Tools (mainly Mypy and Pylint).\\n\\nDetecting incorrect datatypes in method signatures\\nwith Mypy\\nBy using type annotations (as recommended previously in Chapter 1, Introduction, Code\\nFormatting, and Tools), throughout our code, and configuring Mypy, we can quickly detect\\nsome basic errors early, and check basic compliance with LSP for free.\\n\\n[ 111 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nIf one of the subclasses of the Event class were to override a method in an incompatible\\nfashion, Mypy would notice this by inspecting the annotations:\\nclass Event:\\n...\\ndef meets_condition(self, event_data: dict) -> bool:\\nreturn False\\nclass LoginEvent(Event):\\ndef meets_condition(self, event_data: list) -> bool:\\nreturn bool(event_data)\\n\\nWhen we run Mypy on this file, we will get an error message saying the following:\\nerror: Argument 1 of \"meets_condition\" incompatible with supertype \"Event\"\\n\\nThe violation to LSP is clear\\xe2\\x80\\x94since the derived class is using a type for the event_data\\nparameter which is different from the one defined on the base class, we cannot expect them\\nto work equally. Remember that, according to this principle, any caller of this hierarchy has\\nto be able to work with Event or LoginEvent transparently, without noticing any\\ndifference. Interchanging objects of these two types should not make the application fail.\\nFailure to do so would break the polymorphism on the hierarchy.\\nThe same error would have occurred if the return type was changed for something other\\nthan a Boolean value. The rationale is that clients of this code are expecting a Boolean value\\nto work with. If one of the derived classes changes this return type, it would be breaking\\nthe contract, and again, we cannot expect the program to continue working normally.\\nA quick note about types that are not the same but share a common interface: even though\\nthis is just a simple example to demonstrate the error, it is still true that both dictionaries\\nand lists have something in common; they are both iterables. This means that in some cases,\\nit might be valid to have a method that expects a dictionary and another one expecting to\\nreceive a list, as long as both treat the parameters through the iterable interface. In this case,\\nthe problem would not lie in the logic itself (LSP might still apply), but in the definition of\\nthe types of the signature, which should read neither list nor dict, but a union of both.\\nRegardless of the case, something has to be modified, whether it is the code of the method,\\nthe entire design, or just the type annotations, but in no case should we silence the warning\\nand ignore the error given by Mypy.\\nDo not ignore errors such as this by using # type: ignore or something\\nsimilar. Refactor or change the code to solve the real problem. The tools\\nare reporting an actual design flaw for a valid reason.\\n\\n[ 112 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nDetecting incompatible signatures with Pylint\\nAnother strong violation of LSP is when, instead of varying the types of the parameters on\\nthe hierarchy, the signatures of the methods differ completely. This might seem like quite a\\nblunder, but detecting it would not always be so easy to remember; Python is interpreted,\\nso there is no compiler to detect these type of error early on, and therefore they will not be\\ncaught until runtime. Luckily, we have static code analyzers such as Mypy and Pylint to\\ncatch errors such as this one early on.\\nWhile Mypy will also catch these type of error, it is not bad to also run Pylint to gain more\\ninsight.\\nIn the presence of a class that breaks the compatibility defined by the hierarchy (for\\nexample, by changing the signature of the method, adding an extra parameter, and so on)\\nshown as follows:\\n# lsp_1.py\\nclass LogoutEvent(Event):\\ndef meets_condition(self, event_data: dict, override: bool) -> bool:\\nif override:\\nreturn True\\n...\\n\\nPylint will detect it, printing an informative error:\\nParameters differ from overridden \\'meets_condition\\' method (argumentsdiffer)\\n\\nOnce again, like in the previous case, do not suppress these errors. Pay attention to the\\nwarnings and errors the tools give and adapt the code accordingly.\\n\\nMore subtle cases of LSP violations\\nIn other cases, however, the way LSP is broken is not so clear or obvious that a tool can\\nautomatically identify it for us, and we have to rely upon careful code inspection when\\ndoing a code review.\\nCases where contracts are modified are particularly harder to detect automatically. Given\\nthat the entire idea of LSP is that subclasses can be used by clients just like their parent\\nclass, it must also be true that contracts are correctly preserved on the hierarchy.\\n\\n[ 113 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nRemember from Chapter 3, General Traits of Good Code, that, when designing by contract,\\nthe contract between the client and supplier sets some rules\\xe2\\x80\\x94the client must provide the\\npreconditions to the method, which the supplier might validate, and it returns some result\\nto the client that it will check in the form of postconditions.\\nThe parent class defines a contract with its clients. Subclasses of this one must respect such\\na contract. This means that, for example:\\nA subclass can never make preconditions stricter than they are defined on the\\nparent class\\nA subclass can never make postconditions weaker than they are defined on the\\nparent class\\nConsider the example of the events hierarchy defined in the previous section, but now with\\na change to illustrate the relationship between LSP and DbC.\\nThis time, we are going to assume a precondition for the method that checks the criteria\\nbased on the data, that the provided parameter must be a dictionary that contains both keys\\n\"before\" and \"after\", and that their values are also nested dictionaries. This allows us to\\nencapsulate even further, because now the client does not need to catch the KeyError\\nexception, but instead just calls the precondition method (assuming that is acceptable to fail\\nif the system is operating under the wrong assumptions). As a side note, it is good that we\\ncan remove this from the client, as now, SystemMonitor does not require to know which\\ntypes of exceptions the methods of the collaborator class might raise (remember that\\nexception weaken encapsulation, as they require the caller to know something extra about\\nthe object they are calling).\\nSuch a design might be represented with the following changes in the code:\\n# lsp_2.py\\nclass Event:\\ndef __init__(self, raw_data):\\nself.raw_data = raw_data\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn False\\n@staticmethod\\ndef meets_condition_pre(event_data: dict):\\n\"\"\"Precondition of the contract of this interface.\\nValidate that the ``event_data`` parameter is properly formed.\\n\"\"\"\\n\\n[ 114 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nassert isinstance(event_data, dict), f\"{event_data!r} is not a\\ndict\"\\nfor moment in (\"before\", \"after\"):\\nassert moment in event_data, f\"{moment} not in {event_data}\"\\nassert isinstance(event_data[moment], dict)\\n\\nAnd now the code that tries to detect the correct event type just checks the precondition\\nonce, and proceeds to find the right type of event:\\n# lsp_2.py\\nclass SystemMonitor:\\n\"\"\"Identify events that occurred in the system.\"\"\"\\ndef __init__(self, event_data):\\nself.event_data = event_data\\ndef identify_event(self):\\nEvent.meets_condition_pre(self.event_data)\\nevent_cls = next(\\n(\\nevent_cls\\nfor event_cls in Event.__subclasses__()\\nif event_cls.meets_condition(self.event_data)\\n),\\nUnknownEvent,\\n)\\nreturn event_cls(self.event_data)\\n\\nThe contract only states that the top-level keys \"before\" and \"after\" are mandatory and\\nthat their values should also be dictionaries. Any attempt in the subclasses to demand a\\nmore restrictive parameter will fail.\\nThe class for the transaction event was originally correctly designed. Look at how the code\\ndoes not impose a restriction on the internal key named \"transaction\"; it only uses its\\nvalue if it is there, but this is not mandatory:\\n# lsp_2.py\\nclass TransactionEvent(Event):\\n\"\"\"Represents a transaction that has just occurred on the system.\"\"\"\\n@staticmethod\\ndef meets_condition(event_data: dict):\\nreturn event_data[\"after\"].get(\"transaction\") is not None\\n\\n[ 115 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nHowever, the original two methods are not correct, because they demand the presence of a\\nkey named \"session\", which is not part of the original contract. This breaks the contract,\\nand now the client cannot use these classes in the same way it uses the rest of them because\\nit will raise KeyError.\\nAfter fixing this (changing the square brackets for the .get() method), the order on the\\nLSP has been reestablished, and polymorphism prevails:\\n>>> l1 = SystemMonitor({\"before\": {\"session\": 0}, \"after\": {\"session\": 1}})\\n>>> l1.identify_event().__class__.__name__\\n\\'LoginEvent\\'\\n>>> l2 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 0}})\\n>>> l2.identify_event().__class__.__name__\\n\\'LogoutEvent\\'\\n>>> l3 = SystemMonitor({\"before\": {\"session\": 1}, \"after\": {\"session\": 1}})\\n>>> l3.identify_event().__class__.__name__\\n\\'UnknownEvent\\'\\n>>> l4 = SystemMonitor({\"before\": {}, \"after\": {\"transaction\": \"Tx001\"}})\\n>>> l4.identify_event().__class__.__name__\\n\\'TransactionEvent\\'\\n\\nIt is unreasonable to expect automated tools (regardless of how good and helpful they are),\\nwill detect cases such as this one. We have to be careful when designing classes that we do\\nnot accidentally change the input or output of the methods in a way that would be\\nincompatible with what the clients are originally expecting.\\n\\nRemarks on the LSP\\nThe LSP is fundamental to a good object-oriented software design because it emphasizes\\none of its core traits\\xe2\\x80\\x94polymorphism. It is about creating correct hierarchies so that classes\\nderived from a base one are polymorphic along the parent one, with respect to the methods\\non their interface.\\nIt is also interesting to notice how this principle relates to the previous one\\xe2\\x80\\x94if we attempt\\nto extend a class with a new one that is incompatible, it will fail, the contract with the client\\nwill be broken, and as a result such an extension will not be possible (or, to make it\\npossible, we would have to break the other end of the principle and modify code in the\\nclient that should be closed for modification, which is completely undesirable and\\nunacceptable).\\n\\n[ 116 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nCarefully thinking about new classes in the way that LSP suggests helps us to extend the\\nhierarchy correctly. We could then say that LSP contributes to the OCP.\\n\\nInterface segregation\\nThe interface segregation principle (ISP) provides some guidelines over an idea that we\\nhave revisited quite repeatedly already: that interfaces should be small.\\nIn object-oriented terms, an interface is represented by the set of methods an object\\nexposes. This is to say that all the messages that an object is able to receive or interpret\\nconstitute its interface, and this is what other clients can request. The interface separates the\\ndefinition of the exposed behavior for a class from its implementation.\\nIn Python, interfaces are implicitly defined by a class according to its methods. This is\\nbecause Python follows the so-called duck typing principle.\\nTraditionally, the idea behind duck typing was that any object is really represented by the\\nmethods it has, and by what it is capable of doing. This means that, regardless of the type of\\nthe class, its name, its docstring, class attributes, or instance attributes, what ultimately\\ndefines the essence of the object are the methods it has. The methods defined on a class\\n(what it knows how to do) are what determines what that object will actually be. It was\\ncalled duck typing because of the idea that \"If it walks like a duck, and quacks like a duck,\\nit must be a duck.\"\\nFor a long time, duck typing was the sole way interfaces were defined in Python. Later on,\\nPython 3 (PEP-3119) introduced the concept of abstract base classes as a way to define\\ninterfaces in a different way. The basic idea of abstract base classes is that they define a\\nbasic behavior or interface that some derived classes are responsible for implementing. This\\nis useful in situations where we want to make sure that certain critical methods are actually\\noverridden, and it also works as a mechanism for overriding or extending the functionality\\nof methods such as isinstance().\\nThis module also contains a way of registering some types as part of a hierarchy, in what is\\ncalled a virtual subclass. The idea is that this extends the concept of duck typing a little bit\\nfurther by adding a new criterion\\xe2\\x80\\x94walks like a duck, quacks like a duck, or... it says it is a\\nduck.\\nThese notions of how Python interprets interfaces are important for understanding this\\nprinciple and the next one.\\n\\n[ 117 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nIn abstract terms, this means that the ISP states that, when we define an interface that\\nprovides multiple methods, it is better to instead break it down into multiple ones, each one\\ncontaining fewer methods (preferably just one), with a very specific and accurate scope. By\\nseparating interfaces into the smallest possible units, to favor code reusability, each class\\nthat wants to implement one of these interfaces will most likely be highly cohesive given\\nthat it has a quite definite behavior and set of responsibilities.\\n\\nAn interface that provides too much\\nNow, we want to be able to parse an event from several data sources, in different formats\\n(XML and JSON, for instance). Following good practice, we decide to target an interface as\\nour dependency instead of a concrete class, and something like the following is devised:\\n\\nIn order to create this as an interface in Python, we would use an abstract base class and\\ndefine the methods (from_xml() and from_json()) as abstract, to force derived classes to\\nimplement them. Events that derive from this abstract base class and implement these\\nmethods would be able to work with their corresponding types.\\nBut what if a particular class does not need the XML method, and can only be constructed\\nfrom a JSON? It would still carry the from_xml() method from the interface, and since it\\ndoes not need it, it will have to pass. This is not very flexible as it creates coupling and\\nforces clients of the interface to work with methods that they do not need.\\n\\nThe smaller the interface, the better\\nIt would be better to separate this into two different interfaces, one for each method:\\n\\n[ 118 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nWith this design, objects that derive from XMLEventParser and implement the\\nfrom_xml() method will know how to be constructed from an XML, and the same for a\\nJSON file, but most importantly, we maintain the orthogonality of two independent\\nfunctions, and preserve the flexibility of the system without losing any functionality that\\ncan still be achieved by composing new smaller objects.\\nThere is some resemblance to the SRP, but the main difference is that here we are talking\\nabout interfaces, so it is an abstract definition of behavior. There is no reason to change\\nbecause there is nothing there until the interface is actually implemented. However, failure\\nto comply with this principle will create an interface that will be coupled with orthogonal\\nfunctionality, and this derived class will also fail to comply with the SRP (it will have more\\nthan one reason to change).\\n\\nHow small should an interface be?\\nThe point made in the previous section is valid, but it also needs a warning\\xe2\\x80\\x94avoid a\\ndangerous path if it\\'s misunderstood or taken to the extreme.\\nA base class (abstract or not) defines an interface for all the other classes to extend it. The\\nfact that this should be as small as possible has to be understood in terms of cohesion\\xe2\\x80\\x94it\\nshould do one thing. That doesn\\'t mean it must necessarily have one method. In the\\nprevious example, it was by coincidence that both methods were doing totally disjoint\\nthings, hence it made sense to separate them into different classes.\\nBut it could be the case that more than one method rightfully belongs to the same class.\\nImagine that you want to provide a mixin class that abstracts certain logic in a context\\nmanager so that all classes derived from that mixin gain that context manager logic for free.\\nAs we already know, a context manager entails two methods: __enter__ and __exit__.\\nThey must go together, or the outcome will not be a valid context manager at all!\\nFailure to place both methods in the same class will result in a broken component that is\\nnot only useless, but also misleadingly dangerous. Hopefully, this exaggerated example\\nworks as a counter-balance to the one in the previous section, and together the reader can\\nget a more accurate picture about designing interfaces.\\n\\nDependency inversion\\nThis is a really powerful idea that will come up again later when we explore some design\\npatterns in Chapter 9, Common Design Patterns, and Chapter 10, Clean Architecture.\\n\\n[ 119 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nThe dependency inversion principle (DIP) proposes an interesting design principle by\\nwhich we protect our code by making it independent of things that are fragile, volatile, or\\nout of our control. The idea of inverting dependencies is that our code should not adapt to\\ndetails or concrete implementations, but rather the other way around: we want to force\\nwhatever implementation or detail to adapt to our code via a sort of API.\\nAbstractions have to be organized in such a way that they do not depend on details, but\\nrather the other way around\\xe2\\x80\\x94the details (concrete implementations) should depend on\\nabstractions.\\nImagine that two objects in our design need to collaborate, A and B. A works with an\\ninstance of B, but as it turns out, our module doesn\\'t control B directly (it might be an\\nexternal library, or a module maintained by another team, and so on). If our code heavily\\ndepends on B, when this changes the code will break. To prevent this, we have to invert the\\ndependency: make B have to adapt to A. This is done by presenting an interface and forcing\\nour code not to depend on the concrete implementation of B, but rather on the interface we\\nhave defined. It is then B\\'s responsibility to comply with that interface.\\nIn line with the concepts explored in previous sections, abstractions also come in the form\\nof interfaces (or abstract base classes in Python).\\nIn general, we could expect concrete implementations to change much more frequently\\nthan abstract components. It is for this reason that we place abstractions (interfaces) as\\nflexibility points where we expect our system to change, be modified, or extended without\\nthe abstraction itself having to be changed.\\n\\nA case of rigid dependencies\\nThe last part of our event\\'s monitoring system is to deliver the identified events to a data\\ncollector to be further analyzed. A naive implementation of such an idea would consist of\\nhaving an event streamer class that interacts with a data destination, for example, Syslog:\\n\\n[ 120 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nHowever, this design is not very good, because we have a high-level class\\n(EventStreamer) depending on a low-level one (Syslog is an implementation detail). If\\nsomething changes in the way we want to send data to Syslog, EventStreamer will have\\nto be modified. If we want to change the data destination for a different one or add new\\nones at runtime, we are also in trouble because we will find ourselves constantly modifying\\nthe stream() method to adapt it to these requirements.\\n\\nInverting the dependencies\\nThe solution to these problems is to make EventStreamer work with an interface, rather\\nthan a concrete class. This way, implementing this interface is up to the low-level classes\\nthat contain the implementation details:\\n\\nNow there is an interface that represents a generic data target where data is going to be sent\\nto. Notice how the dependencies have now been inverted since EventStreamer does not\\ndepend on a concrete implementation of a particular data target, it does not have to change\\nin line with changes on this one, and it is up to every particular data target; to implement\\nthe interface correctly and adapt to changes if necessary.\\nIn other words, the original EventStreamer of the first implementation only worked with\\nobjects of type Syslog, which was not very flexible. Then we realized that it could work\\nwith any object that could respond to a .send() message, and identified this method as the\\ninterface that it needed to comply with. Now, in this version, Syslog is actually extending\\nthe abstract base class named DataTargetClient, which defines the send() method.\\nFrom now on, it is up to every new type of data target (email, for instance) to extend this\\nabstract base class and implement the send() method.\\n\\n[ 121 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nWe can even modify this property at runtime for any other object that implements a\\nsend() method, and it will still work. This is the reason why it is often called dependency\\ninjection: because the dependency can be provided dynamically.\\nThe avid reader might be wondering why this is actually necessary. Python is flexible\\nenough (sometimes too flexible), and will allow us to provide an object like\\nEventStreamer with any particular data target object, without this one having to comply\\nwith any interface because it is dynamically typed. The question is this: why do we need to\\ndefine the abstract base class (interface) at all when we can simply pass an object with a\\nsend() method to it?\\nIn all fairness, this is true; there is actually no need to do that, and the program will work\\njust the same. After all, polymorphism does not mean (or require) inheritance to work.\\nHowever, defining the abstract base class is a good practice that comes with some\\nadvantages, the first one being duck typing. Together with as duck typing, we can mention\\nthe fact that the models become more readable\\xe2\\x80\\x94remember that inheritance follows the rule\\nof is a, so by declaring the abstract base class and extending from it, we are saying that, for\\ninstance, Syslog is DataTargetClient, which is something users of your code can read\\nand understand (again, this is duck typing).\\nAll in all, it is not mandatory to define the abstract base class, but it is desirable in order to\\nachieve a cleaner design. This is one of the things this book is for\\xe2\\x80\\x94to help programmers\\navoid easy-to-make mistakes, just because Python is too flexible and we can get away with\\nit.\\n\\nSummary\\nThe SOLID principles are key guidelines for good object-oriented software design.\\nBuilding software is an incredibly hard task\\xe2\\x80\\x94the logic of the code is complex, its behavior\\nat runtime is hard (if even possible, sometimes) to predict, requirements change constantly\\nas well as the environment, and there are multiple things that can go wrong.\\nIn addition, there are multiple ways of constructing software with different techniques,\\nparadigms, and a lot of different designs, which can work together to solve a particular\\nproblem in a specific manner. However, not all of these approaches will prove to be correct\\nas time passes, and requirements change or evolve. However, by this time, it will already\\nbe too late to do something about an incorrect design, as it is rigid, inflexible, and therefore\\nhard to change a refactor into the proper solution.\\n\\n[ 122 ]\\n\\n\\x0cThe SOLID Principles\\n\\nChapter 4\\n\\nThis means that, if we get the design wrong, it will costs us a lot in the future. How can we\\nthen achieve a good design that will eventually pay off? The answer is that we do not know\\nfor sure. We are dealing with the future, and the future is uncertain\\xe2\\x80\\x94there is no way to\\ndetermine if our design will be correct and if our software will be flexible and adaptable in\\nyears to come. It is precisely for that reason that we have to stick to principles.\\nThis is where the SOLID principles come into play. They are not a magic rule (after all,\\nthere are no silver bullets in software engineering), but they provide good guidelines to\\nfollow that have been proven to work in past projects and will make our software much\\nmore likely to succeed.\\nIn this chapter, we have explored the SOLID principles with the goal of understanding\\nclean design. In the following chapters, we will continue to explore details of the language,\\nand see in some cases how these tools and features can be used with these principles.\\n\\nReferences\\nHere is a list of information you may refer to:\\nSRP 01: The Single Responsibility Principle (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8b8thlight.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bblog/\\nuncle-\\xe2\\x80\\x8bbob/\\xe2\\x80\\x8b2014/\\xe2\\x80\\x8b05/\\xe2\\x80\\x8b08/\\xe2\\x80\\x8bSingleReponsibilityPrinciple.\\xe2\\x80\\x8bhtml)\\nPEP-3119: Introducing Abstract Base Classes (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\npeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b3119/\\xe2\\x80\\x8b)\\nLISKOV 01: A paper written by Barbara Liskov named Data Abstraction and\\nHierarchy\\n\\n[ 123 ]\\n\\n\\x0c5\\nUsing Decorators to Improve\\nOur Code\\nIn this chapter, we will explore decorators and see how they are useful in many situations\\nwhere we want to improve our design. We will start by first exploring what decorators are,\\nhow they work, and how they are implemented.\\nWith this knowledge, we will then revisit concepts that we learned in previous chapters\\nregarding general good practices for software design, and see how decorators can help us\\ncomply with each principle.\\nThe goals of this chapter are as follows:\\nTo understand how decorators work in Python\\nTo learn how to implement decorators that apply to functions and classes\\nTo effectively implement decorators, avoiding common implementation mistakes\\nTo analyze how to avoid code duplication (the DRY principle) with decorators\\nTo study how decorators contribute to separation of concerns\\nTo analyze examples of good decorators\\nTo review common situations, idioms, or patterns for when decorators are the\\nright choice\\n\\nWhat are decorators in Python?\\nDecorators were introduced in Python a long time ago, in (PEP-318), as a mechanism to\\nsimplify the way functions and methods are defined when they have to be modified after\\ntheir original definition.\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nOne of the original motivations for this was because functions such as classmethod and\\nstaticmethod were used to transform the original definition of the method, but they\\nrequired an extra line, modifying the original definition of the function.\\nMore generally speaking, every time we had to apply a transformation to a function, we\\nhad to call it with the modifier function, and then reassign it to the same name the\\nfunction was originally defined with.\\nFor instance, if we have a function called original, and then we have a function that\\nchanges the behavior of original on top of it, called modifier, we have to write\\nsomething like the following:\\ndef original(...):\\n...\\noriginal = modifier(original)\\n\\nNotice how we change the function and reassign it to the same name. This is confusing,\\nerror-prone (imagine that someone forgets to reassign the function, or does reassign that\\nbut not in the line immediately after the function definition, but much farther away), and\\ncumbersome. For this reason, some syntax support was added to the language.\\nThe previous example could be rewritten like so:\\n@modifier\\ndef original(...):\\n...\\n\\nThis means that decorators are just syntax sugar for calling whatever is after the decorator\\nas a first parameter of the decorator itself, and the result would be whatever the decorator\\nreturns.\\nIn line with the Python terminology, and our example, modifier is what we call\\nthe decorator, and original is the decorated function, often also called a wrapped object.\\nWhile the functionality was originally thought for methods and functions, the actual syntax\\nallows any kind of object to be decorated, so we are going to explore decorators applied to\\nfunctions, methods, generators, and classes.\\nOne final note is that, while the name of a decorator is correct (after all, the decorator is in\\nfact, making changes, extending, or working on top of the wrapped function), it is not to be\\nconfused with the decorator design pattern.\\n\\n[ 125 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nDecorate functions\\nFunctions are probably the simplest representation of a Python object that can be decorated.\\nWe can use decorators on functions to apply all sorts of logic to them\\xe2\\x80\\x94we can validate\\nparameters, check preconditions, change the behavior entirely, modify its signature, cache\\nresults (create a memorized version of the original function), and more.\\nAs an example, we will create a basic decorator that implements a retry mechanism,\\ncontrolling a particular domain-level exception and retrying a certain number of times:\\n# decorator_function_1.py\\nclass ControlledException(Exception):\\n\"\"\"A generic exception on the program\\'s domain.\"\"\"\\ndef retry(operation):\\n@wraps(operation)\\ndef wrapped(*args, **kwargs):\\nlast_raised = None\\nRETRIES_LIMIT = 3\\nfor _ in range(RETRIES_LIMIT):\\ntry:\\nreturn operation(*args, **kwargs)\\nexcept ControlledException as e:\\nlogger.info(\"retrying %s\", operation.__qualname__)\\nlast_raised = e\\nraise last_raised\\nreturn wrapped\\n\\nThe use of @wraps can be ignored for now, as it will be covered in the section\\nnamed Effective decorators - avoiding common mistakes. The use of _ in the for loop, means that\\nthe number is assigned to a variable we are not interested in at the moment, because it\\'s not\\nused inside the for loop (it\\'s a common idiom in Python to name _ values that are ignored).\\nThe retry decorator doesn\\'t take any parameters, so it can be easily applied to any\\nfunction, as follows:\\n@retry\\ndef run_operation(task):\\n\"\"\"Run a particular task, simulating some failures on its execution.\"\"\"\\nreturn task.run()\\n\\n[ 126 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nAs explained at the beginning, the definition of @retry on top of run_operation is just\\nsyntactic sugar that Python provides to actually execute run_operation =\\nretry(run_operation).\\nIn this limited example, we can see how decorators can be used to create a generic retry\\noperation that, under certain conditions (in this case, represented as exceptions that could\\nbe related to timeouts, for example), will allow calling the decorated code multiple times.\\n\\nDecorate classes\\nClasses can also be decorated (PEP-3129) with the same as can be applied to syntax\\nfunctions. The only difference is that when writing the code for this decorator, we have to\\ntake into consideration that we are receiving a class, not a function.\\nSome practitioners might argue that decorating a class is something rather convoluted and\\nthat such a scenario might jeopardize readability because we would be declaring some\\nattributes and methods in the class, but behind the scenes, the decorator might be applying\\nchanges that would render a completely different class.\\nThis assessment is true, but only if this technique is heavily abused. Objectively, this is no\\ndifferent from decorating functions; after all, classes are just another type of object in the\\nPython ecosystem, as functions are. We will review the pros and cons of this issue with\\ndecorators in the section titled Decorators and separation of concerns, but for now, we\\'ll\\nexplore the benefits of decorators that apply particularly to classes:\\nAll the benefits of reusing code and the DRY principle. A valid case of a class\\ndecorator would be to enforce that multiple classes conform to a certain interface\\nor criteria (by making this checks only once in the decorator that is going to be\\napplied to those many classes).\\nWe could create smaller or simpler classes that will be enhanced later on by\\ndecorators\\nThe transformation logic we need to apply to a certain class will be much easier\\nto maintain if we use a decorator, as opposed to more complicated (and often\\nrightfully discouraged) approaches such as metaclasses\\nAmong all possible applications of decorators, we will explore a simple example to give an\\nidea of the sorts of things they can be useful for. Keep in mind that this is not the only\\napplication type for class decorators, but also that the code we show you could have many\\nother multiple solutions as well, all with their pros and cons, but we chose decorators with\\nthe purpose of illustrating their usefulness.\\n\\n[ 127 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nRecalling our event systems for the monitoring platform, we now need to transform the\\ndata for each event and send it to an external system. However, each type of event might\\nhave its own particularities when selecting how to send its data.\\nIn particular, the event for a login might contain sensitive information such as credentials\\nthat we want to hide. Other fields such as timestamp might also require some\\ntransformations since we want to show them in a particular format. A first attempt at\\ncomplying with these requirements would be as simple as having a class that maps to each\\nparticular event and knows how to serialize it:\\nclass LoginEventSerializer:\\ndef __init__(self, event):\\nself.event = event\\ndef serialize(self) -> dict:\\nreturn {\\n\"username\": self.event.username,\\n\"password\": \"**redacted**\",\\n\"ip\": self.event.ip,\\n\"timestamp\": self.event.timestamp.strftime(\"%Y-%m-%d\\n%H:%M\"),\\n}\\nclass LoginEvent:\\nSERIALIZER = LoginEventSerializer\\ndef __init__(self, username, password, ip, timestamp):\\nself.username = username\\nself.password = password\\nself.ip = ip\\nself.timestamp = timestamp\\ndef serialize(self) -> dict:\\nreturn self.SERIALIZER(self).serialize()\\n\\nHere, we declare a class that is going to map directly with the login event, containing the\\nlogic for it\\xe2\\x80\\x94hide the password field, and format the timestamp as required.\\nWhile this works and might look like a good option to start with, as time passes and we\\nwant to extend our system, we will find some issues:\\nToo many classes: As the number of events grows, the number of serialization\\nclasses will grow in the same order of magnitude, because they are mapped one\\nto one.\\n\\n[ 128 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nThe solution is not flexible enough: If we need to reuse parts of the components\\n(for example, we need to hide the password in another type of event that also\\nhas it), we will have to extract this into a function, but also call it repeatedly from\\nmultiple classes, meaning that we are not reusing that much code after all.\\nBoilerplate: The serialize() method will have to be present in all event\\nclasses, calling the same code. Although we can extract this into another class\\n(creating a mixin), it does not seem like a good use of inheritance.\\nAn alternative solution is to be able to dynamically construct an object that, given a set of\\nfilters (transformation functions) and an event instance, is able to serialize it by applying\\nthe filters to its fields. We then only need to define the functions to transform each type of\\nfield, and the serializer is created by composing many of these functions.\\nOnce we have this object, we can decorate the class in order to add the serialize()\\nmethod, which will just call these Serialization objects with itself:\\ndef hide_field(field) -> str:\\nreturn \"**redacted**\"\\n\\ndef format_time(field_timestamp: datetime) -> str:\\nreturn field_timestamp.strftime(\"%Y-%m-%d %H:%M\")\\n\\ndef show_original(event_field):\\nreturn event_field\\n\\nclass EventSerializer:\\ndef __init__(self, serialization_fields: dict) -> None:\\nself.serialization_fields = serialization_fields\\ndef serialize(self, event) -> dict:\\nreturn {\\nfield: transformation(getattr(event, field))\\nfor field, transformation in\\nself.serialization_fields.items()\\n}\\n\\nclass Serialization:\\ndef __init__(self, **transformations):\\nself.serializer = EventSerializer(transformations)\\ndef __call__(self, event_class):\\ndef serialize_method(event_instance):\\n\\n[ 129 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nreturn self.serializer.serialize(event_instance)\\nevent_class.serialize = serialize_method\\nreturn event_class\\n\\n@Serialization(\\nusername=show_original,\\npassword=hide_field,\\nip=show_original,\\ntimestamp=format_time,\\n)\\nclass LoginEvent:\\ndef __init__(self, username, password, ip, timestamp):\\nself.username = username\\nself.password = password\\nself.ip = ip\\nself.timestamp = timestamp\\n\\nNotice how the decorator makes it easier for the user to know how each field is going to be\\ntreated without having to look into the code of another class. Just by reading the arguments\\npassed to the class decorator, we know that the username and IP address will be left\\nunmodified, the password will be hidden, and the timestamp will be formatted.\\nNow, the code of the class does not need the serialize() method defined, nor does it\\nneed to extend from a mixin that implements it, since the decorator will add it. In fact, this\\nis probably the only part that justifies the creation of the class decorator, because otherwise,\\nthe Serialization object could have been a class attribute of LoginEvent, but the fact\\nthat it is altering the class by adding a new method to it makes it impossible.\\nMoreover, we could have another class decorator that, just by defining the attributes of the\\nclass, implements the logic of the init method, but this is beyond the scope of this\\nexample. This is what libraries such as attrs (ATTRS 01) do, and a similar functionality is\\nproposed in (PEP-557) for the Standard library.\\nBy using this class decorator from (PEP-557), in Python 3.7+, the previous example could be\\nrewritten in a more compact way, without the boilerplate code of the init, as shown here:\\nfrom dataclasses import dataclass\\nfrom datetime import datetime\\n@Serialization(\\nusername=show_original,\\npassword=hide_field,\\nip=show_original,\\ntimestamp=format_time,\\n\\n[ 130 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\n)\\n@dataclass\\nclass LoginEvent:\\nusername: str\\npassword: str\\nip: str\\ntimestamp: datetime\\n\\nOther types of decorator\\nNow that we know what the @ syntax for decorators actually means, we can conclude that\\nit isn\\'t just functions, methods, or classes that can be decorated; actually, anything that can\\nbe defined, such as generators, coroutines, and even objects that have already been\\ndecorated, can be decorated, meaning that decorators can be stacked.\\nThe previous example showed how decorators can be chained. We first defined the class,\\nand then applied @dataclass to it, which converted it into a data class, acting as a\\ncontainer for those attributes. After that, the @Serialization will apply the logic to that\\nclass, resulting in a new class with the new serialize() method added to it.\\nAnother good use of decorators is for generators that are supposed to be used as\\ncoroutines. We will explore the details of generators and coroutines in Chapter 7, Using\\nGenerators, but the main idea is that, before sending any data to a newly created generator,\\nthe latter has to be advanced up to their next yield statement by calling next() on it. This\\nis a manual process that every user will have to remember and hence is error-prone. We\\ncould easily create a decorator that takes a generator as a parameter, calls next() to it, and\\nthen returns the generator.\\n\\nPassing arguments to decorators\\nAt this point, we already regard decorators as a powerful tool in Python. However, they\\ncould be even more powerful if we could just pass parameters to them so that their logic is\\nabstracted even more.\\nThere are several ways of implementing decorators that can take arguments, but we will go\\nover the most common ones. The first one is to create decorators as nested functions with a\\nnew level of indirection, making everything in the decorator fall one level deeper. The\\nsecond approach is to use a class for the decorator.\\n\\n[ 131 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nIn general, the second approach favors readability more, because it is easier to think in\\nterms of an object than three or more nested functions working with closures. However, for\\ncompleteness, we will explore both, and the reader can decide what is best for the problem\\nat hand.\\n\\nDecorators with nested functions\\nRoughly speaking, the general idea of a decorator is to create a function that returns a\\nfunction (often called a higher-order function). The internal function defined in the body of\\nthe decorator is going to be the one actually being called.\\nNow, if we wish to pass parameters to it, we then need another level of indirection. The\\nfirst one will take the parameters, and inside that function, we will define a new function,\\nwhich will be the decorator, which in turn will define yet another new function, namely the\\none to be returned as a result of the decoration process. This means that we will have at\\nleast three levels of nested functions.\\nDon\\'t worry if this didn\\'t seem clear so far. After reviewing the examples that are about to\\ncome, everything will become clear.\\nOne of the first examples we saw of decorators implemented the retry functionality over\\nsome functions. This is a good idea, except it has a problem; our implementation did not\\nallow us to specify the numbers of retries, and instead, this was a fixed number inside the\\ndecorator.\\nNow, we want to be able to indicate how many retries each instance is going to have, and\\nperhaps we could even add a default value to this parameter. In order to do this, we need\\nanother level of nested functions\\xe2\\x80\\x94first for the parameters, and then for the decorator itself.\\nThis is because we are now going to have something in the form of the following:\\n@retry(arg1, arg2,... )\\n\\nAnd that has to return a decorator because the @ syntax will apply the result of that\\ncomputation to the object to be decorated. Semantically, it would translate to something\\nlike the following:\\n<original_function> = retry(arg1, arg2, ....)(<original_function>)\\n\\n[ 132 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nBesides the number of desired retries, we can also indicate the types of exception we wish\\nto control. The new version of the code supporting the new requirements might look like\\nthis:\\nRETRIES_LIMIT = 3\\n\\ndef with_retry(retries_limit=RETRIES_LIMIT, allowed_exceptions=None):\\nallowed_exceptions = allowed_exceptions or (ControlledException,)\\ndef retry(operation):\\n@wraps(operation)\\ndef wrapped(*args, **kwargs):\\nlast_raised = None\\nfor _ in range(retries_limit):\\ntry:\\nreturn operation(*args, **kwargs)\\nexcept allowed_exceptions as e:\\nlogger.info(\"retrying %s due to %s\", operation, e)\\nlast_raised = e\\nraise last_raised\\nreturn wrapped\\nreturn retry\\n\\nHere are some examples of how this decorator can be applied to functions, showing the\\ndifferent options it accepts:\\n# decorator_parametrized_1.py\\n@with_retry()\\ndef run_operation(task):\\nreturn task.run()\\n\\n@with_retry(retries_limit=5)\\ndef run_with_custom_retries_limit(task):\\nreturn task.run()\\n\\n@with_retry(allowed_exceptions=(AttributeError,))\\ndef run_with_custom_exceptions(task):\\nreturn task.run()\\n\\n@with_retry(\\nretries_limit=4, allowed_exceptions=(ZeroDivisionError, AttributeError)\\n\\n[ 133 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\n)\\ndef run_with_custom_parameters(task):\\nreturn task.run()\\n\\nDecorator objects\\nThe previous example requires three levels of nested functions. The first it is going to be a\\nfunction that receives the parameters of the decorator we want to use. Inside this function,\\nthe rest of the functions are closures that use these parameters along with the logic of the\\ndecorator.\\nA cleaner implementation of this would be to use a class to define the decorator. In this\\ncase, we can pass the parameters in the __init__ method, and then implement the logic of\\nthe decorator on the magic method named __call__.\\nThe code for the decorator will look like it does in the following example:\\nclass WithRetry:\\ndef __init__(self, retries_limit=RETRIES_LIMIT,\\nallowed_exceptions=None):\\nself.retries_limit = retries_limit\\nself.allowed_exceptions = allowed_exceptions or\\n(ControlledException,)\\ndef __call__(self, operation):\\n@wraps(operation)\\ndef wrapped(*args, **kwargs):\\nlast_raised = None\\nfor _ in range(self.retries_limit):\\ntry:\\nreturn operation(*args, **kwargs)\\nexcept self.allowed_exceptions as e:\\nlogger.info(\"retrying %s due to %s\", operation, e)\\nlast_raised = e\\nraise last_raised\\nreturn wrapped\\n\\nAnd this decorator can be applied pretty much like the previous one, like so:\\n@WithRetry(retries_limit=5)\\ndef run_with_custom_retries_limit(task):\\nreturn task.run()\\n\\n[ 134 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nIt is important to note how the Python syntax takes effect here. First, we create the object, so\\nbefore the @ operation is applied, the object is created with its parameters passed to it. This\\nwill create a new object and initialize it with these parameters, as defined in the init\\nmethod. After this, the @ operation is invoked, so this object will wrap the function named\\nrun_with_custom_reries_limit, meaning that it will be passed to the call magic\\nmethod.\\nInside this call magic method, we defined the logic of the decorator as we normally\\ndo\\xe2\\x80\\x94we wrap the original function, returning a new one with the logic we want instead.\\n\\nGood uses for decorators\\nIn this section, we will take a look at some common patterns that make good use of\\ndecorators. These are common situations for when decorators are a good choice.\\nFrom all the countless applications decorators can be used for, we will enumerate a few, the\\nmost common or relevant:\\nTransforming parameters: Changing the signature of a function to expose a nicer\\nAPI, while encapsulating details on how the parameters are treated and\\ntransformed underneath\\nTracing code: Logging the execution of a function with its parameters\\nValidate parameters\\nImplement retry operations\\nSimplify classes by moving some (repetitive) logic into decorators\\nLet\\'s discuss the first two applications in detail in the following section.\\n\\nTransforming parameters\\nWe have mentioned before that decorators can be used to validate parameters (and even\\nenforce some preconditions or postconditions under the idea of DbC), so from this you\\nprobably have got the idea that it is somehow common to use decorators when dealing\\nwith or manipulating parameters.\\n\\n[ 135 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nIn particular, there are some cases on which we find ourselves repeatedly creating similar\\nobjects, or applying similar transformations that we would wish to abstract away. Most of\\nthe time, we can achieve this by simply using a decorator.\\n\\nTracing code\\nWhen talking about tracing in this section, we will refer to something more general that has\\nto do with dealing with the execution of a function that we wish to monitor. This could\\nrefer to scenarios in which we want to:\\nActually trace the execution of a function (for example, by logging the lines it\\nexecutes)\\nMonitor some metrics over a function (such as CPU usage or memory footprint)\\nMeasure the running time of a function\\nLog when a function was called, and the parameters that were passed to it\\nIn the next section, we will explore a simple example of a decorator that logs the execution\\nof a function, including its name and the time it took to run.\\n\\nEffective decorators \\xe2\\x80\\x93 avoiding common\\nmistakes\\nWhile decorators are a great feature of Python, they are not exempt from issues if used\\nincorrectly. In this section, we will see some common issues to avoid in order to create\\neffective decorators.\\n\\nPreserving data about the original wrapped\\nobject\\nOne of the most common problems when applying a decorator to a function is that some of\\nthe properties or attributes of the original function are not maintained, leading to\\nundesired, and hard-to-track, side-effects.\\n\\n[ 136 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nTo illustrate this we show a decorator that is in charge of logging when the function is\\nabout to run:\\n# decorator_wraps_1.py\\ndef trace_decorator(function):\\ndef wrapped(*args, **kwargs):\\nlogger.info(\"running %s\", function.__qualname__)\\nreturn function(*args, **kwargs)\\nreturn wrapped\\n\\nNow, let\\'s imagine we have a function with this decorator applied to it. We might initially\\nthink that nothing of that function is modified with respect to its original definition:\\n@trace_decorator\\ndef process_account(account_id):\\n\"\"\"Process an account by Id.\"\"\"\\nlogger.info(\"processing account %s\", account_id)\\n...\\n\\nBut maybe there are changes.\\nThe decorator is not supposed to alter anything from the original function, but, as it turns\\nout since it contains a flaw it\\'s actually modifying its name and docstring, among other\\nproperties.\\nLet\\'s try to get help for this function:\\n>>> help(process_account)\\nHelp on function wrapped in module decorator_wraps_1:\\nwrapped(*args, **kwargs)\\n\\nAnd let\\'s check how it\\'s called:\\n>>> process_account.__qualname__\\n\\'trace_decorator.<locals>.wrapped\\'\\n\\nWe can see that, since the decorator is actually changing the original function for a new one\\n(called wrapped), what we actually see are the properties of this function instead of those\\nfrom the original function.\\nIf we apply a decorator like this one to multiple functions, all with different names, they\\nwill all end up being called wrapped, which is a major concern (for example, if we want to\\nlog or trace the function, this will make debugging even harder).\\n\\n[ 137 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nAnother problem is that, in case we placed docstrings with tests on these functions, they\\nwill be overridden by those of the decorator. As a result, the docstrings with the test we\\nwant will not run when we call our code with the doctest module (as we have seen in\\nChapter 1, Introduction, Code Formatting, and Tools).\\nThe fix is simple, though. We just have to apply the wraps decorator in the internal\\nfunction (wrapped), telling it that it is actually wrapping function:\\n# decorator_wraps_2.py\\ndef trace_decorator(function):\\n@wraps(function)\\ndef wrapped(*args, **kwargs):\\nlogger.info(\"running %s\", function.__qualname__)\\nreturn function(*args, **kwargs)\\nreturn wrapped\\n\\nNow, if we check the properties, we will obtain what we expected in the first place.\\nCheck help for the function, like so:\\n>>> Help on function process_account in module decorator_wraps_2:\\nprocess_account(account_id)\\nProcess an account by Id.\\n\\nAnd verify that its qualified name is correct, like so:\\n>>> process_account.__qualname__\\n\\'process_account\\'\\n\\nMost importantly, we recovered the unit tests we might have had on the docstrings! By\\nusing the wraps decorator, we can also access the original, unmodified function under the\\n__wrapped__ attribute. Although it should not be used in production, it might come in\\nhandy in some unit tests when we want to check the unmodified version of the function.\\nIn general, for simple decorators, the way we would use functools.wraps would\\ntypically follow the general formula or structure:\\ndef decorator(original_function):\\n@wraps(original_function)\\ndef decorated_function(*args, **kwargs):\\n# modifications done by the decorator ...\\nreturn original_function(*args, **kwargs)\\nreturn decorated_function\\n\\n[ 138 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nAlways use functools.wraps applied over the wrapped function, when\\ncreating a decorator, as shown in the preceding formula.\\n\\nDealing with side-effects in decorators\\nIn this section, we will learn that it is advisable to avoid side-effects in the body of the\\ndecorator. There are cases where this might be acceptable, but the bottom line is that, if in\\ncase of doubt, decide against it, for the reasons that are explained ahead. Everything that\\nthe decorator needs to do aside from the function that it\\'s decorating should be placed in\\nthe innermost function definition, or there will be problems when it comes to importing.\\nNonetheless, sometimes these side-effects are required (or even desired) to run at import\\ntime, and the obverse applies.\\nWe will see examples of both, and where each one applies. If in doubt, err on the side of\\ncaution, and delay all side-effects until the very latest, right after the wrapped function is\\ngoing to be called.\\nNext, we will see when it\\'s not a good idea to place extra logic outside the wrapped\\nfunction.\\n\\nIncorrect handling of side-effects in a decorator\\nLet\\'s imagine the case of a decorator that was created with the goal of logging when a\\nfunction started running and then logging its running time:\\ndef traced_function_wrong(function):\\nlogger.info(\"started execution of %s\", function)\\nstart_time = time.time()\\n@functools.wraps(function)\\ndef wrapped(*args, **kwargs):\\nresult = function(*args, **kwargs)\\nlogger.info(\\n\"function %s took %.2fs\",\\nfunction,\\ntime.time() - start_time\\n)\\nreturn result\\nreturn wrapped\\n\\n[ 139 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nNow we will apply the decorator to a regular function, thinking that it will work just fine:\\n@traced_function_wrong\\ndef process_with_delay(callback, delay=0):\\ntime.sleep(delay)\\nreturn callback()\\n\\nThis decorator has a subtle, yet critical bug in it.\\nFirst, let\\'s import the function, call it several times, and see what happens:\\n>>> from decorator_side_effects_1 import process_with_delay\\nINFO:started execution of <function process_with_delay at 0x...>\\n\\nJust by importing the function, we will notice that something\\'s amiss. The logging line\\nshould not be there, because the function was not invoked.\\nNow, what happens if we run the function, and see how long it takes to run? Actually, we\\nwould expect that calling the same function multiple times will give similar results:\\n>>> main()\\n...\\nINFO:function <function process_with_delay at 0x> took 8.67s\\n>>> main()\\n...\\nINFO:function <function process_with_delay at 0x> took 13.39s\\n>>> main()\\n...\\nINFO:function <function process_with_delay at 0x> took 17.01s\\n\\nEvery time we run the same function, it takes longer! At this point, you have probably\\nalready noticed the (now obvious) error.\\nRemember the syntax for decorators. @traced_function_wrong actually means the\\nfollowing:\\nprocess_with_delay = traced_function_wrong(process_with_delay)\\n\\nAnd this will run when the module is imported. Therefore, the time that is set in the\\nfunction will be the one at the time the module was imported. Successive calls will compute\\nthe time difference from the running time until that original starting time. It will also log at\\nthe wrong moment, and not when the function is actually called.\\n\\n[ 140 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nLuckily, the fix is also very simple\\xe2\\x80\\x94we just have to move the code inside the wrapped\\nfunction in order to delay its execution:\\ndef traced_function(function):\\n@functools.wraps(function)\\ndef wrapped(*args, **kwargs):\\nlogger.info(\"started execution of %s\", function.__qualname__)\\nstart_time = time.time()\\nresult = function(*args, **kwargs)\\nlogger.info(\\n\"function %s took %.2fs\",\\nfunction.__qualname__,\\ntime.time() - start_time\\n)\\nreturn result\\nreturn wrapped\\n\\nWith this new version, the previous problems are resolved.\\nIf the actions of the decorator had been different, the results could have been much more\\ndisastrous. For instance, if it requires that you log events and send them to an external\\nservice, it will certainly fail unless the configuration has been run right before this has been\\nimported, which we cannot guarantee. Even if we could, it would be bad practice. The\\nsame applies if the decorator has any other sort of side-effect, such as reading from a file,\\nparsing a configuration, and many more.\\n\\nRequiring decorators with side-effects\\nSometimes, side-effects on decorators are necessary, and we should not delay their\\nexecution until the very last possible time, because that\\'s part of the mechanism which is\\nrequired for them to work.\\nOne common scenario for when we don\\'t want to delay the side-effect of decorators is\\nwhen we need to register objects to a public registry that will be available in the module.\\nFor instance, going back to our previous event system example, we now want to only\\nmake some events available in the module, but not all of them. In the hierarchy of events,\\nwe might want to have some intermediate classes that are not actual events we want to\\nprocess on the system, but some of their derivative classes instead.\\nInstead of flagging each class based on whether it\\'s going to be processed or not, we could\\nexplicitly register each class through a decorator.\\n\\n[ 141 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nIn this case, we have a class for all events that relate to the activities of a user. However, this\\nis just an intermediate table for the types of event we actually want, namely\\nUserLoginEvent and UserLogoutEvent:\\nEVENTS_REGISTRY = {}\\n\\ndef register_event(event_cls):\\n\"\"\"Place the class for the event into the registry to make it\\naccessible in\\nthe module.\\n\"\"\"\\nEVENTS_REGISTRY[event_cls.__name__] = event_cls\\nreturn event_cls\\n\\nclass Event:\\n\"\"\"A base event object\"\"\"\\n\\nclass UserEvent:\\nTYPE = \"user\"\\n\\n@register_event\\nclass UserLoginEvent(UserEvent):\\n\"\"\"Represents the event of a user when it has just accessed the\\nsystem.\"\"\"\\n\\n@register_event\\nclass UserLogoutEvent(UserEvent):\\n\"\"\"Event triggered right after a user abandoned the system.\"\"\"\\n\\nWhen we look at the preceding code, it seems that EVENTS_REGISTRY is empty, but after\\nimporting something from this module, it will get populated with all of the classes that are\\nunder the register_event decorator:\\n>>> from decorator_side_effects_2 import EVENTS_REGISTRY\\n>>> EVENTS_REGISTRY\\n{\\'UserLoginEvent\\': decorator_side_effects_2.UserLoginEvent,\\n\\'UserLogoutEvent\\': decorator_side_effects_2.UserLogoutEvent}\\n\\nThis might seem like it\\'s hard to read, or even misleading, because EVENTS_REGISTRY will\\nhave its final value at runtime, right after the module was imported, and we cannot easily\\npredict its value by just looking at the code.\\n\\n[ 142 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nWhile that is true, in some cases this pattern is justified. In fact, many web frameworks or\\nwell-known libraries use this to work and expose objects or make them available.\\nIt is also true that in this case, the decorator is not changing the wrapped object, nor altering\\nthe way it works in any way. However, the important note here is that, if we were to do\\nsome modifications and define an internal function that modifies the wrapped object, we\\nwould still probably want the code that registers the resulting object outside it.\\nNotice the use of the word outside. It does not necessarily mean before, it\\'s just not part of\\nthe same closure; but it\\'s in the outer scope, so it\\'s not delayed until runtime.\\n\\nCreating decorators that will always work\\nThere are several different scenarios to which decorators might apply. It can also be the\\ncase that we need to use the same decorator for objects that fall into these different multiple\\nscenarios, for instance, if we want to reuse our decorator and apply it to a function, a class,\\na method, or a static method.\\nIf we create the decorator, just thinking about supporting only the first type of object we\\nwant to decorate, we might notice that the same decorator does not work equally well on a\\ndifferent type of object. The typical example is where we create a decorator to be used on a\\nfunction, and then we want to apply it to a method of a class, only to realize that it does not\\nwork. A similar scenario might occur if we designed our decorator for a method, and then\\nwe want it to also apply for static methods or class methods.\\nWhen designing decorators, we typically think about reusing code, so we will want to use\\nthat decorator for functions and methods as well.\\nDefining our decorators with the signature *args, and **kwargs, will make them work in\\nall cases, because it\\'s the most generic kind of signature that we can have. However,\\nsometimes we might want not to use this, and instead define the decorator wrapping\\nfunction according to the signature of the original function, mainly because of two reasons:\\nIt will be more readable since it resembles the original function.\\nIt actually needs to do something with the arguments, so receiving *args and\\n**kwargs wouldn\\'t be convenient.\\nConsider the case on which we have many functions in our code base that require a\\nparticular object to be created from a parameter. For instance, we pass a string, and\\ninitialize a driver object with it, repeatedly. Then we think we can remove the duplication\\nby using a decorator that will take care of converting this parameter accordingly.\\n\\n[ 143 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nIn the next example, we pretend that DBDriver is an object that knows how to connect and\\nrun operations on a database, but it needs a connection string. The methods we have in our\\ncode, are designed to receive a string with the information of the database and require to\\ncreate an instance of DBDriver always. The idea of the decorator is that it\\'s going to take\\nplace of this conversion automatically\\xe2\\x80\\x94the function will continue to receive a string, but\\nthe decorator will create a DBDriver and pass it to the function, so internally we can\\nassume that we receive the object we need directly.\\nAn example of using this in a function is shown in the next listing:\\nimport logging\\nfrom functools import wraps\\nlogger = logging.getLogger(__name__)\\n\\nclass DBDriver:\\ndef __init__(self, dbstring):\\nself.dbstring = dbstring\\ndef execute(self, query):\\nreturn f\"query {query} at {self.dbstring}\"\\n\\ndef inject_db_driver(function):\\n\"\"\"This decorator converts the parameter by creating a ``DBDriver``\\ninstance from the database dsn string.\\n\"\"\"\\n@wraps(function)\\ndef wrapped(dbstring):\\nreturn function(DBDriver(dbstring))\\nreturn wrapped\\n\\n@inject_db_driver\\ndef run_query(driver):\\nreturn driver.execute(\"test_function\")\\n\\nIt\\'s easy to verify that if we pass a string to the function, we get the result done by an\\ninstance of DBDriver, so the decorator works as expected:\\n>>> run_query(\"test_OK\")\\n\\'query test_function at test_OK\\'\\n\\n[ 144 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nBut now, we want to reuse this same decorator in a class method, where we find the same\\nproblem:\\nclass DataHandler:\\n@inject_db_driver\\ndef run_query(self, driver):\\nreturn driver.execute(self.__class__.__name__)\\n\\nWe try to use this decorator, only to realize that it doesn\\'t work:\\n>>> DataHandler().run_query(\"test_fails\")\\nTraceback (most recent call last):\\n...\\nTypeError: wrapped() takes 1 positional argument but 2 were given\\n\\nWhat is the problem?\\nThe method in the class is defined with an extra argument\\xe2\\x80\\x94self.\\nMethods are just a particular kind of function that receives self (the object they\\'re defined\\nupon) as the first parameter.\\nTherefore, in this case, the decorator (designed to work with only one parameter, named\\ndbstring), will interpret that self is said parameter, and call the method passing the\\nstring in the place of self, and nothing in the place for the second parameter, namely the\\nstring we are passing.\\nTo fix this issue, we need to create a decorator that will work equally for methods and\\nfunctions, and we do so by defining this as a decorator object, that also implements the\\nprotocol descriptor.\\nDescriptors are fully explained in Chapter 7, Using Generators, so, for now, we can just take\\nthis as a recipe that will make the decorator work.\\nThe solution is to implement the decorator as a class object and make this object a\\ndescription, by implementing the __get__ method.\\nfrom functools import wraps\\nfrom types import MethodType\\n\\nclass inject_db_driver:\\n\"\"\"Convert a string to a DBDriver instance and pass this to the\\nwrapped function.\"\"\"\\ndef __init__(self, function):\\nself.function = function\\n\\n[ 145 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nwraps(self.function)(self)\\ndef __call__(self, dbstring):\\nreturn self.function(DBDriver(dbstring))\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn self.__class__(MethodType(self.function, instance))\\n\\nDetails on descriptors will be explained in Chapter 6, Getting More Out of Our Objects with\\nDescriptors, but for the purposes of this example, we can now say that what it does is\\nactually rebinding the callable it\\'s decorating to a method, meaning that it will bind the\\nfunction to the object, and then recreate the decorator with this new callable.\\nFor functions, it still works, because it won\\'t call the __get__ method at all.\\n\\nThe DRY principle with decorators\\nWe have seen how decorators allow us to abstract away certain logic into a separate\\ncomponent. The main advantage of this is that we can then apply the decorator multiple\\ntimes into different objects in order to reuse code. This follows the Don\\'t Repeat Yourself\\n(DRY) principle since we define certain knowledge once and only once.\\nThe retry mechanism implemented in the previous sections is a good example of a\\ndecorator that can be applied multiple times to reuse code. Instead of making each\\nparticular function include its retry logic, we create a decorator and apply it several times.\\nThis makes sense once we have made sure that the decorator can work with methods and\\nfunctions equally.\\nThe class decorator that defined how events are to be represented also complies with the\\nDRY principle in the sense that it defines one specific place for the logic for serializing an\\nevent, without needing to duplicate code scattered among different classes. Since we expect\\nto reuse this decorator and apply it to many classes, its development (and complexity) pay\\noff.\\nThis last remark is important to bear in mind when trying to use decorators in order to\\nreuse code\\xe2\\x80\\x94we have to be absolutely sure that we will actually be saving code.\\n\\n[ 146 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nAny decorator (especially if it is not carefully designed) adds another level of indirection to\\nthe code, and hence more complexity. Readers of the code might want to follow the path of\\nthe decorator to fully understand the logic of the function (although these considerations\\nare addressed in the following section), so keep in mind that this complexity has to pay off.\\nIf there is not going to be too much reuse, then do not go for a decorator and opt for a\\nsimpler option (maybe just a separate function or another small class is enough).\\nBut how do we know what too much reuse is? Is there a rule to determine when to refactor\\nexisting code into a decorator? There is nothing specific to decorators in Python, but we\\ncould apply a general rule of thumb in software engineering (GLASS 01) that states that a\\ncomponent should be tried out at least three times before considering creating a generic\\nabstraction in the sort of a reusable component. From the same reference (GLASS 01); (we\\nencourage all readers to read Facts and Fallacies of Software Engineering because it is a great\\nreference) also comes the idea that creating reusable components is three times harder than\\ncreating simple ones.\\nThe bottom line is that reusing code through decorators is acceptable, but only when you\\ntake into account the following considerations:\\nDo not create the decorator in the first place from scratch. Wait until the pattern\\nemerges and the abstraction for the decorator becomes clear, and then refactor.\\nConsider that the decorator has to be applied several times (at least three times)\\nbefore implementing it.\\nKeep the code in the decorators to a minimum.\\n\\nDecorators and separation of concerns\\nThe last point on the previous list is so important that it deserves a section of its own. We\\nhave already explored the idea of reusing code and noticed that a key element of reusing\\ncode is having components that are cohesive. This means that they should have the\\nminimum level of responsibility\\xe2\\x80\\x94do one thing, one thing only, and do it well. The smaller\\nour components, the more reusable, and the more they can be applied in a different context\\nwithout carrying extra behavior that will cause coupling and dependencies, which will\\nmake the software rigid.\\nTo show you what this means, let\\'s reprise one of the decorators that we used in a previous\\nexample. We created a decorator that traced the execution of certain functions with code\\nsimilar to the following:\\ndef traced_function(function):\\n@functools.wraps(function)\\n\\n[ 147 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\ndef wrapped(*args, **kwargs):\\nlogger.info(\"started execution of %s\", function.__qualname__)\\nstart_time = time.time()\\nresult = function(*args, **kwargs)\\nlogger.info(\\n\"function %s took %.2fs\",\\nfunction.__qualname__,\\ntime.time() - start_time\\n)\\nreturn result\\nreturn wrapped\\n\\nNow, this decorator, while it works, has a problem\\xe2\\x80\\x94it is doing more than one thing. It logs\\nthat a particular function was just invoked, and also logs how much time it took to run.\\nEvery time we use this decorator, we are carrying these two responsibilities, even if we only\\nwanted one of them.\\nThis should be broken down into smaller decorators, each one with a more specific and\\nlimited responsibility:\\ndef log_execution(function):\\n@wraps(function)\\ndef wrapped(*args, **kwargs):\\nlogger.info(\"started execution of %s\", function.__qualname__)\\nreturn function(*kwargs, **kwargs)\\nreturn wrapped\\n\\ndef measure_time(function):\\n@wraps(function)\\ndef wrapped(*args, **kwargs):\\nstart_time = time.time()\\nresult = function(*args, **kwargs)\\nlogger.info(\"function %s took %.2f\", function.__qualname__,\\ntime.time() - start_time)\\nreturn result\\nreturn wrapped\\n\\nNotice that the same functionality that we had previously can be achieved by simply\\ncombining both of them:\\n@measure_time\\n@log_execution\\ndef operation():\\n....\\n\\n[ 148 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nNotice how the order in which the decorators are applied is also important.\\nDo not place more than one responsibility in a decorator. The SRP applies\\nto decorators as well.\\n\\nAnalyzing good decorators\\nAs a closing note for this chapter, let\\'s review some examples of good decorators and how\\nthey are used both in Python itself, as well as in popular libraries. The idea is to get\\nguidelines on how good decorators are created.\\nBefore jumping into examples, let\\'s first identify traits that good decorators should have:\\nEncapsulation, or separation of concerns: A good decorator should effectively\\nseparate different responsibilities between what it does and what it is decorating.\\nIt cannot be a leaky abstraction, meaning that a client of the decorator should\\nonly invoke it in black box mode, without knowing how it is actually\\nimplementing its logic.\\nOrthogonality: What the decorator does should be independent, and as\\ndecoupled as possible from the object it is decorating.\\nReusability: It is desirable that the decorator can be applied to multiple types,\\nand not that it just appears on one instance of one function, because that means\\nthat it could just have been a function instead. It has to be generic enough.\\nA nice example of decorators can be found in the Celery project, where a task is defined by\\napplying the decorator of the task from the application to a function:\\n@app.task\\ndef mytask():\\n....\\n\\nOne of the reasons why this is a good decorator is because it is very good at\\nsomething\\xe2\\x80\\x94encapsulation. The user of the library only needs to define the function body\\nand the decorator will convert that into a task automatically. The \"@app.task\" decorator\\nsurely wraps a lot of logic and code, but none of that is relevant to the body of\\n\"mytask()\". It is complete encapsulation and separation of concerns\\xe2\\x80\\x94nobody will have to\\ntake a look at what that decorator does, so it is a correct abstraction that does not leak any\\ndetails.\\n\\n[ 149 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nAnother common use of decorators is in web frameworks (Pyramid, Flask, and Sanic, just\\nto name a few), on which the handlers for views are registered to the URLs through\\ndecorators:\\n@route(\"/\", method=[\"GET\"])\\ndef view_handler(request):\\n...\\n\\nThese sorts of decorator have the same considerations as before; they also provide total\\nencapsulation because a user of the web framework rarely (if ever) needs to know what\\nthe \"@route\" decorator is doing. In this case, we know that the decorator is doing\\nsomething more, such as registering these functions to a mapper to the URL, and also that it\\nis changing the signature of the original function to provide us with a nicer interface that\\nreceives a request object with all the information already set.\\nThe previous two examples are enough to make us notice something else about this use of\\ndecorators. They conform to an API. These libraries of frameworks are exposing their\\nfunctionality to users through decorators, and it turns out that decorators are an excellent\\nway of defining a clean programming interface.\\nThis is probably the best way we should think about to decorators. Much like in the\\nexample of the class decorator that tells us how the attributes of the event are going to be\\nhandled, a good decorator should provide a clean interface so that users of the code know\\nwhat to expect from the decorator, without needing to know how it works, or any of its\\ndetails for that matter.\\n\\nSummary\\nDecorators are powerful tools in Python that can be applied to many things such as classes,\\nmethods, functions, generators, and many more. We have demonstrated how to create\\ndecorators in different ways, and for different purposes, and drew some conclusions along\\nthe way.\\nWhen creating a decorator for functions, try to make its signature match the original\\nfunction being decorated. Instead of using the generic *args, and **kwargs, making the\\nsignature match the original one will make it easier to read, and maintain, and it will\\nresemble the original function more closely, so it will be more familiar to readers of that\\ncode.\\n\\n[ 150 ]\\n\\n\\x0cUsing Decorators to Improve Our Code\\n\\nChapter 5\\n\\nDecorators are a very useful tool for reusing code and following the DRY principle.\\nHowever, their usefulness comes at a cost, and if they are not used wisely, the complexity\\ncan do more harm than good. For that reason, we emphasize that decorators should be\\nused when they are actually going to be applied multiple times (three or more times). In the\\nsame way as the DRY principle, we find the ideas of separation of concerns, with the goal\\nof keeping the decorators as small as possible.\\nAnother good use of decorators is to create cleaner interfaces, for instance, simplifying the\\ndefinition of a class by extracting part of its logic into a decorator. In this sense, decorators\\nalso help readability by providing the users with information about what that particular\\ncomponent will be doing, without needing to know how (encapsulation).\\nIn the next chapter, we will take a look at another advanced feature of Python\\xe2\\x80\\x94descriptors.\\nIn particular, we will see how with the help of descriptors we can create even better\\ndecorators, and solve some of the issues we encountered in this chapter.\\n\\nReferences\\nHere is a list of information you can refer to:\\nPEP-318: Decorators for Functions and Methods (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\npeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0318/\\xe2\\x80\\x8b)\\nPEP-3129: Class Decorators (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b3129/\\xe2\\x80\\x8b)\\nWRAPT 01: https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpypi.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bproject/\\xe2\\x80\\x8bwrapt/\\xe2\\x80\\x8b\\nWRAPT 02: https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwrapt.\\xe2\\x80\\x8breadthedocs.\\xe2\\x80\\x8bio/\\xe2\\x80\\x8ben/\\xe2\\x80\\x8blatest/\\xe2\\x80\\x8bdecorators.\\nhtml#universal-\\xe2\\x80\\x8bdecorators\\n\\nThe Functools module: The wraps function in the functools module of Python\\'s\\nstandard library (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bfunctools.\\nhtml#functools.\\xe2\\x80\\x8bwrap)\\nATTRS 01: The attrs library (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpypi.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bproject/\\xe2\\x80\\x8battrs/\\xe2\\x80\\x8b)\\nPEP-557: Data Classes (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0557/\\xe2\\x80\\x8b)\\nGLASS 01: The book written by Robert L. Glass named Facts and Fallacies of\\nSoftware Engineering\\n\\n[ 151 ]\\n\\n\\x0c6\\nGetting More Out of Our\\nObjects with Descriptors\\nThis chapter introduces a new concept that is more advanced in Python development since\\nit features descriptors. Moreover, descriptors are not something programmers of other\\nlanguages are familiar with, so there are no easy analogies or parallelisms to make.\\nDescriptors are another distinctive feature of Python that takes object-oriented\\nprogramming to another level, and their potential allows users to build more powerful and\\nreusable abstractions. Most of the time, the full potential of descriptors is observed in\\nlibraries or frameworks.\\nIn this chapter, we will achieve the following goals that relate to descriptors:\\nUnderstand what descriptors are, how they work, and how to implement them\\neffectively\\nAnalyze the two types of descriptors (data and non-data descriptors), in term of\\ntheir conceptual differences and implementation details\\nReuse code effectively through descriptors\\nAnalyze examples of good uses of descriptors, and how to take advantage of\\nthem for our own libraries of APIs\\n\\nA first look at descriptors\\nFirst, we will explore the main idea behind descriptors to understand their mechanics and\\ninternal workings. Once this is clear, it will be easier to assimilate how the different types of\\ndescriptors work, which we will explore in the next section.\\nOnce we have a first understanding of the idea behind descriptors, we will look at an\\nexample where their use gives us a cleaner and more Pythonic implementation.\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThe machinery behind descriptors\\nThe way descriptors work is not all that complicated, but the problem with them is that\\nthere are a lot of caveats to take into consideration, so the implementation details are of the\\nutmost importance here.\\nIn order to implement descriptors, we need at least two classes. For the purposes of this\\ngeneric example, we are going to call the client class to the one that is going to take\\nadvantage of the functionality we want to implement in the descriptor (this class is\\ngenerally just a domain model one, a regular abstraction we create for our solution), and we\\nare going to call the descriptor class to the one that implements the logic of the\\ndescriptor.\\nA descriptor is, therefore, just an object that is an instance of a class that implements the\\ndescriptor protocol. This means that this class must have its interface containing at least one\\nof the following magic methods (part of the descriptor protocol as of Python 3.6+):\\n__get__\\n__set__\\n__delete__\\n__set_name__\\n\\nFor the purposes of this initial high-level introduction, the following naming convention\\nwill be used:\\nName\\n\\nClientClass\\n\\nMeaning\\n\\nThe domain-level abstraction that will take advantage of the\\nfunctionality to be implemented by the descriptor. This class is said to\\nbe a client of the descriptor.\\nThis class contains a class attribute (named descriptor by this\\nconvention), which is an instance of DescriptorClass.\\n\\nThe class that implements the descriptor itself. This class should\\nDescriptorClass implement some of the aforementioned magic methods that entail the\\nclient\\n\\ndescriptor\\n\\ndescriptor protocol.\\nAn instance of ClientClass.\\nclient = ClientClass()\\n\\nAn instance of DescriptorClass.\\ndescriptor = DescriptorClass().\\nThis object is a class attribute that is placed in ClientClass.\\n\\n[ 153 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThis relationship is illustrated in the following diagram:\\n\\nA very important observation to keep in mind is that for this protocol to work, the\\ndescriptor object has to be defined as a class attribute. Creating this object as an instance\\nattribute will not work, so it must be in the body of the class, and not in the init method.\\nAlways place the descriptor object as a class attribute!\\n\\nOn a slightly critical note, readers can also note that it is possible to implement the\\ndescriptor protocol partially\\xe2\\x80\\x94not all methods must always be defined; instead, we can\\nimplement only those we need, as we will see shortly.\\nSo, now we have the structure in place\\xe2\\x80\\x94we know what elements are set and how they\\ninteract. We need a class for the descriptor, another class that will consume the logic of\\nthe descriptor, which, in turn, will have a descriptor object (an instance of the\\nDescriptorClass) as a class attribute, and instances of ClientClass that will follow the\\ndescriptor protocol when we call for the attribute named descriptor. But now what?\\nHow does all of this fit into place at runtime?\\nNormally, when we have a regular class and we access its attributes, we simply obtain the\\nobjects as we expect them, and even their properties, as in the following example:\\n>>> class Attribute:\\n...\\nvalue = 42\\n...\\n>>> class Client:\\n...\\nattribute = Attribute()\\n...\\n>>> Client().attribute\\n<__main__.Attribute object at 0x7ff37ea90940>\\n>>> Client().attribute.value\\n42\\n\\n[ 154 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nBut, in the case of descriptors, something different happens. When an object is defined as a\\nclass attribute (and this one is a descriptor), when a client requests this attribute,\\ninstead of getting the object itself (as we would expect from the previous example), we get\\nthe result of having called the __get__ magic method.\\nLet\\'s start with some simple code that only logs information about the context, and returns\\nthe same client object:\\nclass DescriptorClass:\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nlogger.info(\"Call: %s.__get__(%r, %r)\",\\nself.__class__.__name__,instance, owner)\\nreturn instance\\n\\nclass ClientClass:\\ndescriptor = DescriptorClass()\\n\\nWhen running this code, and requesting the descriptor attribute of an instance of\\nClientClass, we will discover that we are, in fact, not getting an instance of\\nDescriptorClass, but whatever its __get__() method returns instead:\\n>>> client = ClientClass()\\n>>> client.descriptor\\nINFO:Call: DescriptorClass.__get__(<ClientClass object at 0x...>, <class\\n\\'ClientClass\\'>)\\n<ClientClass object at 0x...>\\n>>> client.descriptor is client\\nINFO:Call: DescriptorClass.__get__(ClientClass object at 0x...>, <class\\n\\'ClientClass\\'>)\\nTrue\\n\\nNotice how the logging line, placed under the __get__ method, was called instead of just\\nreturning the object we created. In this case, we made that method return the client itself,\\nhence making true a comparison of the last statement. The parameters of this method are\\nexplained in more detail in the following subsections when we explore each method in\\nmore detail.\\n\\n[ 155 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nStarting from this simple, yet demonstrative example, we can start creating more complex\\nabstractions and better decorators, because the important note here is that we have a new\\n(powerful) tool to work with. Notice how this changes the control flow of the program in a\\ncompletely different way. With this tool, we can abstract all sorts of logic behind the\\n__get__ method, and make the descriptor transparently run all sorts of transformations\\nwithout clients even noticing. This takes encapsulation to a new level.\\n\\nExploring each method of the descriptor protocol\\nUp until now, we have seen quite a few examples of descriptors in action, and we got the\\nidea of how they work. These examples gave us a first glimpse of the power of descriptors,\\nbut you might be wondering about some implementation details and idioms whose\\nexplanation we failed to address.\\nSince descriptors are just objects, these methods take self as the first parameter. For all of\\nthem, this just means the descriptor object itself.\\nIn this section, we will explore each method of the descriptor protocol, in full detail,\\nexplaining what each parameter signifies, and how they are intended to be used.\\n\\n__get__(self, instance, owner)\\nThe first parameter, instance, refers to the object from which the descriptor is being\\ncalled. In our first example, this would mean the client object.\\nThe owner parameter is a reference to the class of that object, which following our example\\n(from the previous class diagram in The machinery behind descriptors section) would be\\nClientClass.\\nFrom the previous paragraph we conclude that the parameter named instance in the\\nsignature of __get__ is the object over which the descriptor is taking action, and owner is\\nthe class of instance. The avid reader might be wondering why is the signature define like\\nthis, after all the class can be taken from instance directly (owner =\\ninstance.__class__). There is an edge case\\xe2\\x80\\x94when the descriptor is called from the\\nclass (ClientClass), not from the instance (client), then the value of instance is None,\\nbut we might still want to do some processing in that case.\\n\\n[ 156 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nWith the following simple code we can demonstrate the difference of when a descriptor is\\nbeing called from the class, or from an instance. In this case, the __get__ method is doing\\ntwo separate things for each case.\\n# descriptors_methods_1.py\\nclass DescriptorClass:\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn f\"{self.__class__.__name__}.{owner.__name__}\"\\nreturn f\"value for {instance}\"\\n\\nclass ClientClass:\\ndescriptor = DescriptorClass()\\n\\nWhen we call it from ClientClass directly it will do one thing, which is composing a\\nnamespace with the names of the classes:\\n>>> ClientClass.descriptor\\n\\'DescriptorClass.ClientClass\\'\\n\\nAnd then if we call it from an object we have created, it will return the other message\\ninstead:\\n>>> ClientClass().descriptor\\n\\'value for <descriptors_methods_1.ClientClass object at 0x...>\\'\\n\\nIn general, unless we really need to do something with the owner parameter, the most\\ncommon idiom, is to just return the descriptor itself, when instance is None.\\n\\n__set__(self, instance, value)\\nThis method is called when we try to assign something to a descriptor. It is activated\\nwith statements such as the following, in which a descriptor is an object that implements\\n__set__ (). The instance parameter, in this case, would be client, and\\nthe value would be the \"value\" string:\\nclient.descriptor = \"value\"\\n\\nIf client.descriptor doesn\\'t implement __set__(), then \"value\" will override the\\ndescriptor entirely.\\n\\n[ 157 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nBe careful when assigning a value to an attribute that is a descriptor. Make\\nsure it implements the __set__ method, and that we are not causing an\\nundesired side effect.\\nBy default, the most common use of this method is just to store data in an object.\\nNevertheless, we have seen how powerful descriptors are so far, and that we can take\\nadvantage of them, for example, if we were to create generic validation objects that can be\\napplied multiple times (again, this is something that if we don\\'t abstract, we might end up\\nrepeating multiple times in setter methods of properties).\\nThe following listing illustrates how we can take advantage of this method in order to\\ncreate generic validation objects for attributes, which can be created dynamically with\\nfunctions to validate on the values before assigning them to the object:\\nclass Validation:\\ndef __init__(self, validation_function, error_msg: str):\\nself.validation_function = validation_function\\nself.error_msg = error_msg\\ndef __call__(self, value):\\nif not self.validation_function(value):\\nraise ValueError(f\"{value!r} {self.error_msg}\")\\n\\nclass Field:\\ndef __init__(self, *validations):\\nself._name = None\\nself.validations = validations\\ndef __set_name__(self, owner, name):\\nself._name = name\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn instance.__dict__[self._name]\\ndef validate(self, value):\\nfor validation in self.validations:\\nvalidation(value)\\ndef __set__(self, instance, value):\\nself.validate(value)\\n\\n[ 158 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\ninstance.__dict__[self._name] = value\\n\\nclass ClientClass:\\ndescriptor = Field(\\nValidation(lambda x: isinstance(x, (int, float)), \"is not a\\nnumber\"),\\nValidation(lambda x: x >= 0, \"is not >= 0\"),\\n)\\n\\nWe can see this object in action in the following listing:\\n>>> client = ClientClass()\\n>>> client.descriptor = 42\\n>>> client.descriptor\\n42\\n>>> client.descriptor = -42\\nTraceback (most recent call last):\\n...\\nValueError: -42 is not >= 0\\n>>> client.descriptor = \"invalid value\"\\n...\\nValueError: \\'invalid value\\' is not a number\\n\\nThe idea is that something that we would normally place in a property can be abstracted\\naway into a descriptor, and reuse it multiple times. In this case, the __set__() method\\nwould be doing what the @property.setter would have been doing.\\n\\n__delete__(self, instance)\\nThis method is called upon with the following statement, in which self would be the\\ndescriptor attribute, and instance would be the client object in this example:\\n>>> del client.descriptor\\n\\nIn the following example, we use this method to create a descriptor with the goal of\\npreventing you from removing attributes from an object without the required\\nadministrative privileges. Notice how, in this case, that the descriptor has logic that is\\nused to predicate with the values of the object that is using it, instead of different related\\nobjects:\\n# descriptors_methods_3.py\\nclass ProtectedAttribute:\\ndef __init__(self, requires_role=None) -> None:\\nself.permission_required = requires_role\\n\\n[ 159 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nself._name = None\\ndef __set_name__(self, owner, name):\\nself._name = name\\ndef __set__(self, user, value):\\nif value is None:\\nraise ValueError(f\"{self._name} can\\'t be set to None\")\\nuser.__dict__[self._name] = value\\ndef __delete__(self, user):\\nif self.permission_required in user.permissions:\\nuser.__dict__[self._name] = None\\nelse:\\nraise ValueError(\\nf\"User {user!s} doesn\\'t have {self.permission_required} \"\\n\"permission\"\\n)\\n\\nclass User:\\n\"\"\"Only users with \"admin\" privileges can remove their email\\naddress.\"\"\"\\nemail = ProtectedAttribute(requires_role=\"admin\")\\ndef __init__(self, username: str, email: str, permission_list: list =\\nNone) -> None:\\nself.username = username\\nself.email = email\\nself.permissions = permission_list or []\\ndef __str__(self):\\nreturn self.username\\n\\nBefore seeing examples of how this object works, it\\'s important to remark some of the\\ncriteria of this descriptor. Notice the User class requires the username and email as\\nmandatory parameters. According to its __init__ method, it cannot be a user if it doesn\\'t\\nhave an email attribute. If we were to delete that attribute, and extract it from the object\\nentirely we would be creating an inconsistent object, with some invalid intermediate state\\nthat does not correspond to the interface defined by the class User. Details like this one are\\nreally important, in order to avoid issues. Some other object is expecting to work with this\\nUser, and it also expects that it has an email attribute.\\n\\n[ 160 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nFor this reason, it was decided that the \"deletion\" of an email will just simply set it to None,\\nand that is the part of the code listing that is in bold. For the same reason, we must forbid\\nsomeone trying to set a None value to it, because that would bypass the mechanism we\\nplaced in the __delete__ method.\\nHere, we can see it in action, assuming a case where only users with \"admin\" privileges\\ncan remove their email address:\\n>>> admin = User(\"root\", \"root@d.com\", [\"admin\"])\\n>>> user = User(\"user\", \"user1@d.com\", [\"email\", \"helpdesk\"])\\n>>> admin.email\\n\\'root@d.com\\'\\n>>> del admin.email\\n>>> admin.email is None\\nTrue\\n>>> user.email\\n\\'user1@d.com\\'\\n>>> user.email = None\\n...\\nValueError: email can\\'t be set to None\\n>>> del user.email\\n...\\nValueError: User user doesn\\'t have admin permission\\n\\nHere, in this simple descriptor, we see that we can delete the email from users that\\ncontain the \"admin\" permission only. As for the rest, when we try to call del on that\\nattribute, we will get a ValueError exception.\\nIn general, this method of the descriptor is not as commonly used as the two previous ones,\\nbut it is worth showing it for completeness.\\n\\n__set_name__(self, owner, name)\\nWhen we create the descriptor object in the class that is going to use it, we generally\\nneed the descriptor to know the name of the attribute it is going to be handling.\\nThis attribute name is the one we use to read from and write to __dict__ in the __get__\\nand __set__ methods, respectively.\\nBefore Python 3.6, the descriptor couldn\\'t take this name automatically, so the most general\\napproach was to just pass it explicitly when initializing the object. This works fine, but it\\nhas an issue in that it requires that we duplicate the name every time we want to use the\\ndescriptor for a new attribute.\\n\\n[ 161 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThis is what a typical descriptor would look like if we didn\\'t have this method:\\nclass DescriptorWithName:\\ndef __init__(self, name):\\nself.name = name\\ndef __get__(self, instance, value):\\nif instance is None:\\nreturn self\\nlogger.info(\"getting %r attribute from %r\", self.name, instance)\\nreturn instance.__dict__[self.name]\\ndef __set__(self, instance, value):\\ninstance.__dict__[self.name] = value\\n\\nclass ClientClass:\\ndescriptor = DescriptorWithName(\"descriptor\")\\n\\nWe can see how the descriptor uses this value:\\n>>> client = ClientClass()\\n>>> client.descriptor = \"value\"\\n>>> client.descriptor\\nINFO:getting \\'descriptor\\' attribute from <ClientClass object at 0x...>\\n\\'value\\'\\n\\nNow, if we wanted to avoid writing the name of the attribute twice (once for the variable\\nassigned inside the class, and once again as the name of the first parameter of the\\ndescriptor), we have to resort to a few tricks, like using a class decorator, or (even worse)\\nusing a metaclass.\\nIn Python 3.6, the new method __set_name__ was added, and it receives the class where\\nthat descriptor is being created, and the name that is being given to that descriptor. The\\nmost common idiom is to use this method for the descriptor so that it can store the required\\nname in this method.\\nFor compatibility, it is generally a good idea to keep a default value in the __init__\\nmethod but still take advantage of __set_name__.\\n\\n[ 162 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nWith this method, we can rewrite the previous descriptors as follows:\\nclass DescriptorWithName:\\ndef __init__(self, name=None):\\nself.name = name\\ndef __set_name__(self, owner, name):\\nself.name = name\\n...\\n\\nTypes of descriptors\\nBased on the methods we have just explored, we can make an important distinction among\\ndescriptors in terms of how they work. Understanding this distinction plays an important\\nrole in working effectively with descriptors, and will also help to avoid caveats or common\\nerrors at runtime.\\nIf a descriptor implements the __set__ or __delete__ methods, it is called a data\\ndescriptor. Otherwise, a descriptor that solely implements __get__ is a non-data\\ndescriptor. Notice that __set_name__ does not affect this classification at all.\\nWhen trying to resolve an attribute of an object, a data descriptor will always take\\nprecedence over the dictionary of the object, whereas a non-data descriptor will not. This\\nmeans that in a non-data descriptor if the object has a key on its dictionary with the same\\nname as the descriptor, this one will always be called, and the descriptor itself will never\\nrun. Conversely, in a data descriptor, even if there is a key in the dictionary with the same\\nname as the descriptor, this one will never be used since the descriptor itself will always\\nend up being called.\\nThe following two sections explain this in more detail, with examples, in order to get a\\ndeeper idea of what to expect from each type of descriptor.\\n\\nNon-data descriptors\\nWe will start with a descriptor that only implements the __get__ method, and see how\\nit is used:\\nclass NonDataDescriptor:\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn 42\\n\\n[ 163 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nclass ClientClass:\\ndescriptor = NonDataDescriptor()\\n\\nAs usual, if we ask for the descriptor, we get the result of its __get__ method:\\n>>> client = ClientClass()\\n>>> client.descriptor\\n42\\n\\nBut if we change the descriptor attribute to something else, we lose access to this value,\\nand get what was assigned to it instead:\\n>>> client.descriptor = 43\\n>>> client.descriptor\\n43\\n\\nNow, if we delete the descriptor, and ask for it again, let\\'s see what we get:\\n>>> del client.descriptor\\n>>> client.descriptor\\n42\\n\\nLet\\'s rewind what just happened. When we first created the client object, the\\ndescriptor attribute lay in the class, not the instance, so if we ask for the dictionary of the\\nclient object, it will be empty:\\n>>> vars(client)\\n{}\\n\\nAnd then, when we request the .descriptor attribute, it doesn\\'t find any key in\\nclient.__dict__ named \"descriptor\", so it goes to the class, where it will find it ... but\\nonly as a descriptor, hence why it returns the result of the __get__ method.\\nBut then, we change the value of the .descriptor attribute to something else, and what\\nthis does is set this into the dictionary of the instance, meaning that this time it won\\'t be\\nempty:\\n>>> client.descriptor = 99\\n>>> vars(client)\\n{\\'descriptor\\': 99}\\n\\nSo, when we ask for the .descriptor attribute here, it will look for it in the object (and\\nthis time it will find it, because there is a key named descriptor in the __dict__ attribute\\nof the object, as the vars result is showing us), and return it without having to look for it in\\nthe class. For this reason, the descriptor protocol is never invoked, and the next time we ask\\nfor this attribute, it will instead return the value we have overridden it with (99).\\n\\n[ 164 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nAfterward, we delete this attribute by calling del, and what this does is remove the key\\n\"descriptor\" from the dictionary of the object, leaving us back in the first scenario, where\\nit\\'s going to default to the class where the descriptor protocol will be activated:\\n>>> del client.descriptor\\n>>> vars(client)\\n{}\\n>>> client.descriptor\\n42\\n\\nThis means that if we set the attribute of the descriptor to something else, we might be\\naccidentally breaking it. Why? Because the descriptor doesn\\'t handle the delete action\\n(some of them don\\'t need to).\\nThis is called a non-data descriptor because it doesn\\'t implement the __set__ magic\\nmethod, as we will see in the next example.\\n\\nData descriptors\\nNow, let\\'s look at the difference of using a data descriptor. For this, we are going to create\\nanother simple descriptor that implements the __set__ method:\\nclass DataDescriptor:\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn 42\\ndef __set__(self, instance, value):\\nlogger.debug(\"setting %s.descriptor to %s\", instance, value)\\ninstance.__dict__[\"descriptor\"] = value\\n\\nclass ClientClass:\\ndescriptor = DataDescriptor()\\n\\nLet\\'s see what the value of the descriptor returns:\\n>>> client = ClientClass()\\n>>> client.descriptor\\n42\\n\\n[ 165 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nNow, let\\'s try to change this value to something else, and see what it returns instead:\\n>>> client.descriptor = 99\\n>>> client.descriptor\\n42\\n\\nThe value returned by the descriptor didn\\'t change. But when we assign a different value\\nto it, it must be set to the dictionary of the object (as it was previously):\\n>>> vars(client)\\n{\\'descriptor\\': 99}\\n>>> client.__dict__[\"descriptor\"]\\n99\\n\\nSo, the __set__() method was called, and indeed it did set the value to the dictionary of\\nthe object, only this time, when we request this attribute, instead of using the __dict__\\nattribute of the dictionary, the descriptor takes precedence (because it\\'s an overriding\\ndescriptor).\\nOne more thing\\xe2\\x80\\x94deleting the attribute will not work anymore:\\n>>> del client.descriptor\\nTraceback (most recent call last):\\n...\\nAttributeError: __delete__\\n\\nThe reason is as follows\\xe2\\x80\\x94given that now, the descriptor always takes place, calling\\ndel on an object doesn\\'t try to delete the attribute from the dictionary (__dict__) of the\\nobject, but instead it tries to call the __delete__() method of the descriptor (which is\\nnot implemented in this example, hence the attribute error).\\nThis is the difference between data and non-data descriptors. If the descriptor implements\\n__set__(), then it will always take precedence, no matter what attributes are present in\\nthe dictionary of the object. If this method is not implemented, then the dictionary will be\\nlooked up first, and then the descriptor will run.\\nAn interesting observation you might have noticed is this line on the set method:\\ninstance.__dict__[\"descriptor\"] = value\\n\\nThere are a lot of things to question about that line, but let\\'s break it down into parts.\\n\\n[ 166 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nFirst, why is it altering just the name of a \"descriptor\" attribute? This is just a\\nsimplification for this example, but, as it transpires when working with descriptors, it\\ndoesn\\'t know at this point the name of the parameter it was assigned to, so we just used the\\none from the example, knowing that it was going to be \"descriptor\".\\nIn a real example, you would do one of two things\\xe2\\x80\\x94either receive the name as a parameter\\nand store it internally in the init method, so that this one will just use the internal\\nattribute, or, even better, use the __set_name__ method.\\nWhy is it accessing the __dict__ attribute of the instance directly? Another good question,\\nwhich also has at least two explanations. First, you might be thinking why not just do the\\nfollowing:\\nsetattr(instance, \"descriptor\", value)\\n\\nRemember that this method (__set__) is called when we try to assign something to the\\nattribute that is a descriptor. So, using setattr() will call this descriptor again,\\nwhich, in turn, will call it again, and so on and so forth. This will end up in an infinite\\nrecursion.\\nDo not use setattr() or the assignment expression directly on the\\ndescriptor inside the __set__ method because that will trigger an infinite\\nrecursion.\\nWhy, then, is the descriptor not able to book-keep the values of the properties for all of its\\nobjects?\\nThe client class already has a reference to the descriptor. If we add a reference from the\\ndescriptor to the client object, we are creating circular dependencies, and these objects\\nwill never be garbage-collected. Since they are pointing at each other, their reference counts\\nwill never drop below the threshold for removal.\\nA possible alternative here is to use weak references, with the weakref module, and create\\na weak reference key dictionary if we want to do that. This implementation is explained\\nlater on in this chapter, but for the implementations within this book, we prefer to use this\\nidiom, since it is fairly common and accepted when writing descriptors.\\n\\n[ 167 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nDescriptors in action\\nNow that we have seen what descriptors are, how they work, and what the main ideas\\nbehind them are, we can see them in action. In this section, we will be exploring some\\nsituations that can be elegantly addressed through descriptors.\\nHere, we will look at some examples of working with descriptors, and we will also cover\\nimplementation considerations for them (different ways of creating them, with their pros\\nand cons), and finally we will discuss what the most suitable scenarios for descriptors are.\\n\\nAn application of descriptors\\nWe will start with a simple example that works, but that will lead to some code duplication.\\nIt is not very clear how this issue will be addressed. Later on, we will devise a way of\\nabstracting the repeated logic into a descriptor, which will address the duplication\\nproblem, and we will notice that the code on our client classes will be reduced drastically.\\n\\nA first attempt without using descriptors\\nThe problem we want to solve now is that we have a regular class with some attributes, but\\nwe wish to track all of the different values a particular attribute has over time, for example,\\nin a list. The first solution that comes to our mind is to use a property, and every time a\\nvalue is changed for that attribute in the setter method of the property, we add it to an\\ninternal list that will keep this trace as we want it.\\nImagine that our class represents a traveler in our application that has a current city, and\\nwe want to keep track of all the cities that user has visited throughout the running of the\\nprogram. The following code is a possible implementation that addresses these\\nrequirements:\\nclass Traveller:\\ndef __init__(self, name, current_city):\\nself.name = name\\nself._current_city = current_city\\nself._cities_visited = [current_city]\\n@property\\ndef current_city(self):\\nreturn self._current_city\\n@current_city.setter\\n\\n[ 168 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\ndef current_city(self, new_city):\\nif new_city != self._current_city:\\nself._cities_visited.append(new_city)\\nself._current_city = new_city\\n@property\\ndef cities_visited(self):\\nreturn self._cities_visited\\n\\nWe can easily check that this code works according to our requirements:\\n>>>\\n>>>\\n>>>\\n>>>\\n\\nalice = Traveller(\"Alice\", \"Barcelona\")\\nalice.current_city = \"Paris\"\\nalice.current_city = \"Brussels\"\\nalice.current_city = \"Amsterdam\"\\n\\n>>> alice.cities_visited\\n[\\'Barcelona\\', \\'Paris\\', \\'Brussels\\', \\'Amsterdam\\']\\n\\nSo far, this is all we need and nothing else has to be implemented. For the purposes of this\\nproblem, the property would be more than enough. What happens if we need the exact\\nsame logic in multiple places of the application? This would mean that this is actually an\\ninstance of a more generic problem\\xe2\\x80\\x94tracing all the values of an attribute in another one.\\nWhat would happen if we want to do the same with other attributes, such as keeping track\\nof all tickets Alice bought, or all the countries she has been in? We would have to repeat the\\nlogic in all of these places.\\nMoreover, what would happen if we need this same behavior in different classes? We\\nwould have to repeat the code or come up with a generic solution (maybe a decorator, a\\nproperty builder, or a descriptor). Since property builders are a particular (and more\\nconvoluted) case of descriptors, they are beyond of the scope of this book, and instead,\\ndescriptors are suggested as a cleaner way of proceeding.\\n\\nThe idiomatic implementation\\nWe will now look at how to address the questions of the previous section by using a\\ndescriptor that is generic enough as to be applied in any class. Again, this example is not\\nreally needed because the requirements do not specify such generic behavior (we haven\\'t\\neven followed the rule of three instances of the similar pattern previously creating the\\nabstraction), but it is shown with the goal of portraying descriptors in action.\\n\\n[ 169 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nDo not implement a descriptor unless there is actual evidence of the\\nrepetition we are trying to solve, and the complexity is proven to have\\npaid off.\\nNow, we will create a generic descriptor that, given a name for the attribute to hold the\\ntraces of another one, will store the different values of the attribute in a list.\\nAs we mentioned previously, the code is more than what we need for the problem, but its\\nintention is just to show how a descriptor would help us in this case. Given the generic\\nnature of descriptors, the reader will notice that the logic on it (the name of their method,\\nand attributes) does not relate to the domain problem at hand (a traveler object). This is\\nbecause the idea of the descriptor is to be able to use it in any type of class, probably on\\ndifferent projects, with the same outcomes.\\nIn order to address this gap, some parts of the code are annotated, and the respective\\nexplanation for each section (what it does, and how it relates to the original problem) is\\ndescribed in the following code:\\nclass HistoryTracedAttribute:\\ndef __init__(self, trace_attribute_name) -> None:\\nself.trace_attribute_name = trace_attribute_name\\nself._name = None\\n\\n# [1]\\n\\ndef __set_name__(self, owner, name):\\nself._name = name\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn instance.__dict__[self._name]\\ndef __set__(self, instance, value):\\nself._track_change_in_value_for_instance(instance, value)\\ninstance.__dict__[self._name] = value\\ndef _track_change_in_value_for_instance(self, instance, value):\\nself._set_default(instance)\\n# [2]\\nif self._needs_to_track_change(instance, value):\\ninstance.__dict__[self.trace_attribute_name].append(value)\\ndef _needs_to_track_change(self, instance, value) -> bool:\\ntry:\\ncurrent_value = instance.__dict__[self._name]\\nexcept KeyError:\\n# [3]\\nreturn True\\n\\n[ 170 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\nreturn value != current_value\\n\\nChapter 6\\n# [4]\\n\\ndef _set_default(self, instance):\\ninstance.__dict__.setdefault(self.trace_attribute_name, [])\\n\\n# [6]\\n\\nclass Traveller:\\ncurrent_city = HistoryTracedAttribute(\"cities_visited\")\\n\\n# [1]\\n\\ndef __init__(self, name, current_city):\\nself.name = name\\nself.current_city = current_city # [5]\\n\\nSome annotations and comments on the code are as follows (numbers in the list correspond\\nto the number annotations in the previous listing):\\n1. The name of the attribute is one of the variables assigned to the descriptor, in\\nthis case, current_city. We pass to the descriptor the name of the variable in\\nwhich it will store the trace for the variable of the descriptor. In this example,\\nwe are telling our object to keep track of all the values that current_city has\\nhad in the attribute named cities_visited.\\n2. The first time we call the descriptor, in the init, the attribute for tracing\\nvalues will not exist, in which case we initialize it to an empty list to later append\\nvalues to it.\\n3. In the init method, the name of the attribute current_city will not exist\\neither, so we want to keep track of this change as well. This is the equivalent of\\ninitializing the list with the first value in the previous example.\\n4. Only track changes when the new value is different from the one that is currently\\nset.\\n5. In the init method, the descriptor already exists, and this assignment\\ninstruction triggers the actions from step 2 (create the empty list to start tracking\\nvalues for it), and step 3 (append the value to this list, and set it to the key in the\\nobject for retrieval later).\\n6. The setdefault method in a dictionary is used to avoid a KeyError. In this\\ncase an empty list will be returned for those attributes that aren\\'t still available\\n(see https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3.\\xe2\\x80\\x8b6/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bstdtypes.\\xe2\\x80\\x8bhtml#dict.\\xe2\\x80\\x8bsetdefault\\nfor reference).\\nIt is true that the code in the descriptor is rather complex. On the other hand, the code in\\nthe client class is considerably simpler. Of course, this balance only pays off if we are\\ngoing to use this descriptor multiple times, which is a concern we have already covered.\\n\\n[ 171 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nWhat might not be so clear at this point is that the descriptor is indeed completely\\nindependent from the client class. Nothing in it suggests anything about the business\\nlogic. This makes it perfectly suitable to apply it in any other class; even if it does\\nsomething completely different, the descriptor will take the same effect.\\nThis is the true Pythonic nature of descriptors. They are more appropriate for defining\\nlibraries, frameworks, or internal APIs, and not that much for business logic.\\n\\nDifferent forms of implementing descriptors\\nWe have to first understand a common issue that\\'s specific to the nature of descriptors\\nbefore thinking of ways of implementing them. First, we will discuss the problem of a\\nglobal shared state, and afterward we will move on and look at different ways descriptors\\ncan be implemented while taking this into consideration.\\n\\nThe issue of global shared state\\nAs we have already mentioned, descriptors need to be set as class attributes to work. This\\nshould not be a problem most of the time, but it does come with some warnings that need\\nto be taken into consideration.\\nThe problem with class attributes is that they are shared across all instances of that class.\\nDescriptors are not an exception here, so if we try to keep data in a descriptor object,\\nkeep in mind that all of them will have access to the same value.\\nLet\\'s see what happens when we incorrectly define a descriptor that keeps the data itself,\\ninstead of storing it in each object:\\nclass SharedDataDescriptor:\\ndef __init__(self, initial_value):\\nself.value = initial_value\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn self.value\\ndef __set__(self, instance, value):\\nself.value = value\\nclass ClientClass:\\ndescriptor = SharedDataDescriptor(\"first value\")\\n\\n[ 172 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nIn this example, the descriptor object stores the data itself. This carries with it the\\ninconvenience that when we modify the value for an instance all other instances of the\\nsame classes are also modified with this value as well. The following code listing puts that\\ntheory in action:\\n>>> client1 = ClientClass()\\n>>> client1.descriptor\\n\\'first value\\'\\n>>> client2 = ClientClass()\\n>>> client2.descriptor\\n\\'first value\\'\\n>>> client2.descriptor = \"value for client 2\"\\n>>> client2.descriptor\\n\\'value for client 2\\'\\n>>> client1.descriptor\\n\\'value for client 2\\'\\n\\nNotice how we change one object, and suddenly all of them are from the same class, and\\nwe can see that this value is reflected. This is because ClientClass.descriptor is\\nunique; it\\'s the same object for all of them.\\nIn some cases, this might be what we actually want (for instance, if we were to create a sort\\nof Borg pattern implementation, on which we want to share state across all objects from a\\nclass), but in general, that is not the case, and we need to differentiate between objects. Such\\npattern is discussed with more detail in Chapter 9, Common Design Patterns.\\nTo achieve this, the descriptor needs to know the value for each instance and return it\\naccordingly. That is the reason we have been operating with the dictionary (__dict__) of\\neach instance and setting and retrieving the values from there.\\nThis is the most common approach. We have already covered why we cannot use\\ngetattr() and setattr() on those methods, so modifying the __dict__ attribute is the\\nlast standing option, and, in this case, is acceptable.\\n\\nAccessing the dictionary of the object\\nThe way we implement descriptors throughout this book is making the descriptor object\\nstore the values in the dictionary of the object, __dict__, and retrieve the parameters from\\nthere as well.\\n\\n[ 173 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nAlways store and return the data from the __dict__ attribute of the\\ninstance.\\n\\nUsing weak references\\nAnother alternative (if we don\\'t want to use __dict__) is to make the descriptor object\\nkeep track of the values for each instance itself, in an internal mapping, and return values\\nfrom this mapping as well.\\nThere is a caveat, though. This mapping cannot just be any dictionary. Since the client\\nclass has a reference to the descriptor, and now the descriptor will keep references to the\\nobjects that use it, this will create circular dependencies, and, as a result, these objects will\\nnever be garbage-collected because they are pointing at each other.\\nIn order to address this, the dictionary has to be a weak key one, as defined in the\\nweakref (WEAKREF 01) module.\\nIn this case, the code for the descriptor might look like the following:\\nfrom weakref import WeakKeyDictionary\\n\\nclass DescriptorClass:\\ndef __init__(self, initial_value):\\nself.value = initial_value\\nself.mapping = WeakKeyDictionary()\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn self.mapping.get(instance, self.value)\\ndef __set__(self, instance, value):\\nself.mapping[instance] = value\\n\\nThis addresses the issues, but it does come with some considerations:\\nThe objects no longer hold their attributes\\xe2\\x80\\x94the descriptor does instead. This is\\nsomewhat controversial, and it might not be entirely accurate from a conceptual\\npoint of view. If we forget this detail, we might be asking the object by inspecting\\nits dictionary, trying to find things that just aren\\'t there (calling\\nvars(client) will not return the complete data, for example).\\n\\n[ 174 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nIt poses the requirement over the objects that they need to be hashable. If they\\naren\\'t, they can\\'t be part of the mapping. This might be too demanding a\\nrequirement for some applications.\\nFor these reasons, we prefer the implementation that has been shown so far in this book,\\nwhich uses the dictionary of each instance. However, for completeness, we have shown this\\nalternative as well.\\n\\nMore considerations about descriptors\\nHere, we will discuss general considerations about descriptors in terms of what we can do\\nwith them when it is a good idea to use them, and also how things that we might have\\ninitially conceived as having been resolved by means of another approach can be improved\\nthrough descriptors. We will then analyze the pros and cons of the original implementation\\nversus the one after descriptors have been used.\\n\\nReusing code\\nDescriptors are a generic tool and a powerful abstraction that we can use to avoid code\\nduplication. The best way to decide when to use descriptors is to identify cases where we\\nwould be using a property (whether for its get logic, set logic, or both), but repeating its\\nstructure many times.\\nProperties are just a particular case of descriptors (the @property decorator is a descriptor\\nthat implements the full descriptor protocol to define their get, set, and delete actions),\\nwhich means that we can use descriptors for far more complex tasks.\\nAnother powerful type we have seen for reusing code was decorators, as explained in\\nChapter 5, Using Decorators to Improve Our Code. Descriptors can help us create to better\\ndecorators by making sure that they will be able to work correctly for class methods as\\nwell.\\nWhen it comes to decorators, we could say that it is safe to always implement the\\n__get__() method on them, and also make it a descriptor. When trying to decide whether\\nthe decorator is worth creating, consider the three problems rule we stated in Chapter\\n5, Using Decorators to Improve Our Code, but note that there are no extra considerations\\ntoward descriptors.\\n\\n[ 175 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nAs for generic descriptors, besides the aforementioned three instances rule that applies to\\ndecorators (and, in general, any reusable component), it is advisable to also keep in mind\\nthat you should use descriptors for cases when we want to define an internal API, which is\\nsome code that will have clients consuming it. This is a feature-oriented more toward\\ndesigning libraries and frameworks, rather than one-time solutions.\\nUnless there is a very good reason to, or that the code will look significantly better, we\\nshould avoid putting business logic in a descriptor. Instead, the code of a descriptor will\\ncontain more implementational code rather than business code. It is more similar to\\ndefining a new data structure or object that another part of our business logic will use as a\\ntool.\\nIn general, descriptors will contain implementation logic, and not so much\\nbusiness logic.\\n\\nAvoiding class decorators\\nIf we recall the class decorator we used in Chapter 5, Using Decorators to Improve Our Code,\\nto determine how an event object is going to be serialized, we ended up with an\\nimplementation that (for Python 3.7+) relied on two class decorators:\\n@Serialization(\\nusername=show_original,\\npassword=hide_field,\\nip=show_original,\\ntimestamp=format_time,\\n)\\n@dataclass\\nclass LoginEvent:\\nusername: str\\npassword: str\\nip: str\\ntimestamp: datetime\\n\\nThe first one takes the attributes from the annotations to declare the variables, whereas the\\nsecond one defines how to treat each file. Let\\'s see whether we can change these two\\ndecorators for descriptors instead.\\n\\n[ 176 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThe idea is to create a descriptor that will apply the transformation over the values of each\\nattribute, returning the modified version according to our requirements (for example,\\nhiding sensitive information, and formatting dates correctly):\\nfrom functools import partial\\nfrom typing import Callable\\n\\nclass BaseFieldTransformation:\\ndef __init__(self, transformation: Callable[[], str]) -> None:\\nself._name = None\\nself.transformation = transformation\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nraw_value = instance.__dict__[self._name]\\nreturn self.transformation(raw_value)\\ndef __set_name__(self, owner, name):\\nself._name = name\\ndef __set__(self, instance, value):\\ninstance.__dict__[self._name] = value\\n\\nShowOriginal = partial(BaseFieldTransformation, transformation=lambda x: x)\\nHideField = partial(\\nBaseFieldTransformation, transformation=lambda x: \"**redacted**\"\\n)\\nFormatTime = partial(\\nBaseFieldTransformation,\\ntransformation=lambda ft: ft.strftime(\"%Y-%m-%d %H:%M\"),\\n)\\n\\nThis descriptor is interesting. It was created with a function that takes one argument and\\nreturns one value. This function will be the transformation we want to apply to the field.\\nFrom the base definition that defines generically how it is going to work, the rest of the\\ndescriptor classes are defined, simply by changing the particular function each one\\nneeds.\\nThe example uses functools.partial (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3.\\xe2\\x80\\x8b6/\\xe2\\x80\\x8blibrary/\\nfunctools.\\xe2\\x80\\x8bhtml#functools.\\xe2\\x80\\x8bpartial) as a way of simulating sub-classes, by applying a\\npartial application of the transformation function for that class, leaving a new callable that\\ncan be instantiated directly.\\n\\n[ 177 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nIn order to keep the example simple, we will implement the __init__() and\\nserialize() methods, although they could be abstracted away as well. Under these\\nconsiderations, the class for the event will now be defined as follows:\\nclass LoginEvent:\\nusername = ShowOriginal()\\npassword = HideField()\\nip = ShowOriginal()\\ntimestamp = FormatTime()\\ndef __init__(self, username, password, ip, timestamp):\\nself.username = username\\nself.password = password\\nself.ip = ip\\nself.timestamp = timestamp\\ndef serialize(self):\\nreturn {\\n\"username\": self.username,\\n\"password\": self.password,\\n\"ip\": self.ip,\\n\"timestamp\": self.timestamp,\\n}\\n\\nWe can see how the object behaves at runtime:\\n>>> le = LoginEvent(\"john\", \"secret password\", \"1.1.1.1\",\\ndatetime.utcnow())\\n>>> vars(le)\\n{\\'username\\': \\'john\\', \\'password\\': \\'secret password\\', \\'ip\\': \\'1.1.1.1\\',\\n\\'timestamp\\': ...}\\n>>> le.serialize()\\n{\\'username\\': \\'john\\', \\'password\\': \\'**redacted**\\', \\'ip\\': \\'1.1.1.1\\',\\n\\'timestamp\\': \\'...\\'}\\n>>> le.password\\n\\'**redacted**\\'\\n\\nThere are some differences with respect to the previous implementation that used a\\ndecorator. This example added the serialize() method and hid the fields before\\npresenting them to its resulting dictionary, but if we asked for any of these attributes to an\\ninstance of the event in memory at any point, it would still give us the original value,\\nwithout any transformation applied to it (we could have chosen to apply the\\ntransformation when setting the value, and return it directly on the __get__(), as well).\\n\\n[ 178 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nDepending on the sensitivity of the application, this may or may not be acceptable, but in\\nthis case, when we ask the object for its public attributes, the descriptor will apply the\\ntransformation before presenting the results. It is still possible to access the original values\\nby asking for the dictionary of the object (by accessing __dict__), but when we ask for the\\nvalue, by default, it will return it converted.\\nIn this example, all descriptors follow a common logic, which is defined in the base class.\\nThe descriptor should store the value in the object and then ask for it, applying the\\ntransformation it defines. We could create a hierarchy of classes, each one defining its own\\nconversion function, in a way that the template method design pattern works. In this case,\\nsince the changes in the derived classes are relatively small (just one function), we opted for\\ncreating the derived classes as partial applications of the base class. Creating any new\\ntransformation field should be as simple as defining a new class that will be the base class,\\nwhich is partially applied with the function we need. This can even be done ad hoc, so there\\nmight be no need to set a name for it.\\nRegardless of this implementation, the point is that since descriptors are objects, we can\\ncreate models, and apply all rules of object-oriented programming to them. Design patterns\\nalso apply to descriptors. We could define our hierarchy, set the custom behavior, and so\\non. This example follows the OCP, which we introduced in Chapter 4, The SOLID\\nPrinciples, because adding a new type of conversion method would just be about creating a\\nnew class, derived from the base one with the function it needs, without having to modify\\nthe base class itself (to be fair, the previous implementation with decorators was also OCPcompliant, but there were no classes involved for each transformation mechanism).\\nLet\\'s take an example where we create a base class that implements the __init__() and\\nserialize() methods so that we can define the LoginEvent class simply by deriving\\nfrom it, as follows:\\nclass LoginEvent(BaseEvent):\\nusername = ShowOriginal()\\npassword = HideField()\\nip = ShowOriginal()\\ntimestamp = FormatTime()\\n\\nOnce we achieve this code, the class looks cleaner. It only defines the attributes it needs,\\nand its logic can be quickly analyzed by looking at the class for each attribute. The base\\nclass will abstract only the common methods, and the class of each event will look simpler\\nand more compact.\\n\\n[ 179 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nNot only do the classes for each event look simple, but the descriptor itself is very compact\\nand a lot simpler than the class decorators. The original implementation with class\\ndecorators was good, but descriptors made it even better.\\n\\nAnalysis of descriptors\\nWe have seen how descriptors work so far and explored some interesting situations in\\nwhich they contribute to clean design by simplifying their logic and leveraging more\\ncompact classes.\\nUp to this point, we know that by using descriptors, we can achieve cleaner code,\\nabstracting away repeated logic and implementation details. But how do we know our\\nimplementation of the descriptors is clean and correct? What makes a good descriptor? Are\\nwe using this tool properly or over-engineering with it?\\nIn this section, we will analyze descriptors in order to answer these questions.\\n\\nHow Python uses descriptors internally\\nReferring to the question as to what makes a good descriptor?, a simple answer would be\\nthat a good descriptor is pretty much like any other good Python object. It is consistent with\\nPython itself. The idea that follows this premise is that analyzing how Python uses\\ndescriptors will give us a good idea of good implementations so that we know what to\\nexpect from the descriptors we write.\\nWe will see the most common scenarios where Python itself uses descriptors to solve parts\\nof its internal logic, and we will also discover elegant descriptors and that they have been\\nthere in plain sight all along.\\n\\nFunctions and methods\\nThe most resonating case of an object that is a descriptor is probably a function. Functions\\nimplement the __get__ method, so they can work as methods when defined inside a class.\\nMethods are just functions that take an extra argument. By convention, the first argument\\nof a method is named \"self\", and it represents an instance of the class that the method is\\nbeing defined in. Then, whatever the method does with \"self\", would be the same as any\\nother function receiving the object and applying modifications to it.\\n\\n[ 180 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nIn order words, when we define something like this:\\nclass MyClass:\\ndef method(self, ...):\\nself.x = 1\\n\\nIt is actually the same as if we define this:\\nclass MyClass: pass\\ndef method(myclass_instance, ...):\\nmyclass_instance.x = 1\\nmethod(MyClass())\\n\\nSo, it is just another function, modifying the object, only that it\\'s defined inside the class,\\nand it is said to be bound to the object.\\nWhen we call something in the form of this:\\ninstance = MyClass()\\ninstance.method(...)\\n\\nPython is, in fact, doing something equivalent to this:\\ninstance = MyClass()\\nMyClass.method(instance, ...)\\n\\nNotice that this is just a syntax conversion that is handled internally by Python. The way\\nthis works is by means of descriptors.\\nSince functions implement the descriptor protocol (see the following listing) before calling\\nthe method, the __get__() method is invoked first, and some transformations happen\\nbefore running the code on the internal callable:\\n>>> def function(): pass\\n...\\n>>> function.__get__\\n<method-wrapper \\'__get__\\' of function object at 0x...>\\n\\nIn the instance.method(...) statement, before processing all the arguments of the\\ncallable inside the parenthesis, the \"instance.method\" part is evaluated.\\nSince method is an object defined as a class attribute, and it has a __get__ method, this is\\ncalled. What this does is convert the function to a method, which means binding the\\ncallable to the instance of the object it is going to work with.\\n\\n[ 181 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nLet\\'s see this with an example so that we can get an idea of what Python might be doing\\ninternally.\\nWe will define a callable object inside a class that will act as a sort of function or method\\nthat we want to define to be invoked externally. An instance of the Method class is\\nsupposed to be a function or method to be used inside a different class. This function will\\njust print its three parameters\\xe2\\x80\\x94the instance that it received (which would be the\\nself parameter on the class it\\'s being defined in), and two more arguments. Notice that in\\nthe __call__() method, the self parameter does not represent the instance of\\nMyClass, but instead an instance of Method. The parameter named instance is meant to\\nbe a MyClass type of object:\\nclass Method:\\ndef __init__(self, name):\\nself.name = name\\ndef __call__(self, instance, arg1, arg2):\\nprint(f\"{self.name}: {instance} called with {arg1} and {arg2}\")\\nclass MyClass:\\nmethod = Method(\"Internal call\")\\n\\nUnder these considerations and, after creating the object, the following two calls should be\\nequivalent, based on the preceding definition:\\ninstance = MyClass()\\nMethod(\"External call\")(instance, \"first\", \"second\")\\ninstance.method(\"first\", \"second\")\\n\\nHowever, only the first one works as expected, as the second one gives an error:\\nTraceback (most recent call last):\\nFile \"file\", line , in <module>\\ninstance.method(\"first\", \"second\")\\nTypeError: __call__() missing 1 required positional argument: \\'arg2\\'\\n\\nWe are seeing the same error we faced with a decorator in Chapter 5, Using Decorators to\\nImprove Our Code. The arguments are being shifted to the left by one, instance is taking\\nthe place of self, arg1 is going to be instance, and there is nothing to provide for arg2.\\nIn order to fix this, we need to make Method a descriptor.\\n\\n[ 182 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThis way, when we call instance.method first, we are going to call its __get__(), on\\nwhich we bind this callable to the object accordingly (bypassing the object as the first\\nparameter), and then proceed:\\nfrom types import MethodType\\nclass Method:\\ndef __init__(self, name):\\nself.name = name\\ndef __call__(self, instance, arg1, arg2):\\nprint(f\"{self.name}: {instance} called with {arg1} and {arg2}\")\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nreturn MethodType(self, instance)\\n\\nNow, both calls work as expected:\\nExternal call: <MyClass object at 0x...> called with fist and second\\nInternal call: <MyClass object at 0x...> called with first and second\\n\\nWhat we did is convert the function (actually the callable object we defined instead) to a\\nmethod by using MethodType from the types module. The first parameter of this class\\nshould be a callable (self, in this case, is one by definition because it implements\\n__call__), and the second one is the object to bind this function to.\\nSomething similar to this is what function objects use in Python so they can work as\\nmethods when they are defined inside a class.\\nSince this is a very elegant solution, it\\'s worth exploring it to keep it in mind as a Pythonic\\napproach when defining our own objects. For instance, if we were to define our own\\ncallable, it would be a good idea to also make it a descriptor so that we can use it in classes\\nas class attributes as well.\\n\\nBuilt-in decorators for methods\\nAs you might have known from looking at the official documentation (PYDESCR-02), all\\n@property, @classmethod, and @staticmethod decorators are descriptors.\\n\\n[ 183 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nWe have mentioned several times that the idiom makes the descriptor return itself when it\\'s\\nbeing called from a class directly. Since properties are actually descriptors, that is the\\nreason why, when we ask it from the class, we don\\'t get the result of computing the\\nproperty, but the entire property object instead:\\n>>> class MyClass:\\n... @property\\n... def prop(self): pass\\n...\\n>>> MyClass.prop\\n<property object at 0x...>\\n\\nFor class methods, the __get__ function in the descriptor will make sure that the class is\\nthe first parameter to be passed to the function being decorated, regardless of whether it\\'s\\ncalled from the class directly or from an instance. For static methods, it will make sure that\\nno parameters are bound other than those defined by the function, namely undoing the\\nbinding done by __get__() on functions that make self the first parameter of that\\nfunction.\\nLet\\'s take an example; we create a @classproperty decorator that works as the regular\\n@property decorator, but for classes instead. With a decorator like this one, the following\\ncode should be able to work:\\nclass TableEvent:\\nschema = \"public\"\\ntable = \"user\"\\n@classproperty\\ndef topic(cls):\\nprefix = read_prefix_from_config()\\nreturn f\"{prefix}{cls.schema}.{cls.table}\"\\n\\n>>> TableEvent.topic\\n\\'public.user\\'\\n>>> TableEvent().topic\\n\\'public.user\\'\\n\\n[ 184 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nSlots\\nWhen a class defines the __slots__ attribute, it can contain all the attributes that the class\\nexpects and no more.\\nTrying to add extra attributes dynamically to a class that defines __slots __ will result in\\nan AttributeError. By defining this attribute, the class becomes static, so it will not have\\na __dict__ attribute where you can add more objects dynamically.\\nHow, then, are its attributes retrieved if not from the dictionary of the object? By using\\ndescriptors. Each name defined in a slot will have its own descriptor that will store the\\nvalue for retrieval later:\\nclass Coordinate2D:\\n__slots__ = (\"lat\", \"long\")\\ndef __init__(self, lat, long):\\nself.lat = lat\\nself.long = long\\ndef __repr__(self):\\nreturn f\"{self.__class__.__name__}({self.lat}, {self.long})\"\\n\\nWhile this is an interesting feature, it has to be used with caution because it is taking away\\nthe dynamic nature of Python. In general, this ought to be reserved only for objects that we\\nknow are static, and if we are absolutely sure we are not adding any attributes to them\\ndynamically in other parts of the code.\\nAs an upside of this, objects defined with slots use less memory, since they only need a\\nfixed set of fields to hold values and not an entire dictionary.\\n\\nImplementing descriptors in decorators\\nWe now understand how Python uses descriptors in functions to make them work as\\nmethods when they are defined inside a class. We have also seen examples of cases where\\nwe can make decorators work by making them comply with the descriptor protocol by\\nusing the __get__() method of the interface to adapt the decorator to the object it is being\\ncalled with. This solves the problem for our decorators in the same way that Python solves\\nthe issue of functions as methods in objects.\\n\\n[ 185 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nThe general recipe for adapting a decorator in such a way is to implement the __get__()\\nmethod on it and use types.MethodType to convert the callable (the decorator itself) to a\\nmethod bound to the object it is receiving (the instance parameter received by __get__).\\nFor this to work, we will have to implement the decorator as an object, because otherwise, if\\nwe are using a function, it will already have a __get__() method, which will be doing\\nsomething different that will not work unless we adapt it. The cleaner way to proceed is to\\ndefine a class for the decorator.\\nUse a decorator class when defining a decorator that we want to apply to\\nclass methods, and implement the __get__() method on it.\\n\\nSummary\\nDescriptors are a more advanced feature in Python that push the boundaries, closer to\\nmetaprogramming. One of their most interesting aspects is how they make crystal-clear\\nthat classes in Python are just regular objects, and, as such, they have properties and we can\\ninteract with them. Descriptors are, in this sense, the most interesting type of attribute a\\nclass can have because its protocol facilitates more advanced, object-oriented possibilities.\\nWe have seen the mechanics of descriptors, their methods, and how all of this fits together,\\nmaking a more interesting picture of object-oriented software design. By understanding\\ndescriptors, we were able to create powerful abstractions that yield clean and compact\\nclasses. We have seen how to fix decorators that we want to apply to functions and\\nmethods, we have understood a lot more about how Python works internally, and how\\ndescriptors play such a core and critical role in the implementation of the language.\\nThis study of how descriptors are used internally in Python should work as a reference to\\nidentify good uses of descriptors in our own code, with the goal of achieving idiomatic\\nsolutions.\\nDespite all of the powerful options that descriptors represent to our advantage, we have to\\nkeep in mind when to properly make use of them without over-engineering. In this line, we\\nhave suggested that we should reserve the functionality of descriptors for truly generic\\ncases, such as the design of internal development APIs, libraries, or frameworks. Another\\nimportant consideration along these lines is that, in general, we should not place business\\nlogic in descriptors, but rather logic that implements technical functionality to be used by\\nother components that do contain business logic.\\n\\n[ 186 ]\\n\\n\\x0cGetting More Out of Our Objects with Descriptors\\n\\nChapter 6\\n\\nSpeaking of advanced functionality, the next chapter also covers an interesting and indepth topic: generators. On the face of it generators are rather simple (and most readers are\\nprobably already familiar with them), but what they have in common with descriptors is\\nthat they can also be complex, yield a more advanced and elegant design, and make Python\\na unique language to work with.\\n\\nReferences\\nHere is a list of a few things you can reference for more information:\\nPython\\'s official documentation on descriptors (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\nreference/\\xe2\\x80\\x8bdatamodel.\\xe2\\x80\\x8bhtml#implementing-\\xe2\\x80\\x8bdescriptors)\\nWEAKREF 01: Python weakref module (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\nweakref.\\xe2\\x80\\x8bhtml)\\nPYDESCR-02: Built-in decorators as descriptors (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\nhowto/\\xe2\\x80\\x8bdescriptor.\\xe2\\x80\\x8bhtml#static-\\xe2\\x80\\x8bmethods-\\xe2\\x80\\x8band-\\xe2\\x80\\x8bclass-\\xe2\\x80\\x8bmethods)\\n\\n[ 187 ]\\n\\n\\x0c7\\nUsing Generators\\nGenerators are another of those features that makes Python a peculiar language over more\\ntraditional ones. In this chapter, we will explore their rationale, why they were introduced\\nin the language, and the problems they solve. We will also cover how to address problems\\nidiomatically by using generators, and how to make our generators (or any iterable, for that\\nmatter) Pythonic.\\nWe will understand why iteration (in the form of the iterator pattern) is automatically\\nsupported in the language. From there, we will take another journey and explore how\\ngenerators became such a fundamental feature of Python in order to support other\\nfunctionality, such as coroutines and asynchronous programming.\\nThe goals for this chapter are as follows:\\nTo create generators that improve the performance of our programs\\nTo study how iterators (and the iterator pattern, in particular) are deeply\\nembedded in Python\\nTo solve problems that involve iteration idiomatically\\nTo understand how generators work as the basis for coroutines and\\nasynchronous programming\\nTo explore the syntactic support for coroutines\\xe2\\x80\\x94yield from, await, and async\\ndef\\n\\nTechnical requirements\\nThe examples in this chapter will work with any version of Python 3.6 on any platform.\\nThe code used in this chapter can be found at https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bgithub.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bPacktPublishing/\\nClean-\\xe2\\x80\\x8bCode-\\xe2\\x80\\x8bin-\\xe2\\x80\\x8bPython.\\xe2\\x80\\x8b\\n\\nThe instructions are available in the README file.\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nCreating generators\\nGenerators were introduced in Python a long time ago (PEP-255), with the idea of\\nintroducing iteration in Python while improving the performance of the program (by using\\nless memory) at the same time.\\nThe idea of a generator is to create an object that is iterable, and, while it\\'s being iterated,\\nwill produce the elements it contains, one at a time. The main use of generators is to save\\nmemory\\xe2\\x80\\x94instead of having a very large list of elements in memory, holding everything at\\nonce, we have an object that knows how to produce each particular element, one at a time,\\nas they are required.\\nThis feature enables lazy computations or heavyweight objects in memory, in a similar\\nmanner to what other functional programming languages (Haskell, for instance) provide. It\\nwould even be possible to work with infinite sequences because the lazy nature of\\ngenerators allows for such an option.\\n\\nA first look at generators\\nLet\\'s start with an example. The problem at hand now is that we want to process a large list\\nof records and get some metrics and indicators over them. Given a large data set with\\ninformation about purchases, we want to process it in order to get the lowest sale, highest\\nsale, and the average price of a sale.\\nFor the simplicity of this example, we will assume a CSV with only two fields, in the\\nfollowing format:\\n<purchase_date>, <price>\\n...\\n\\nWe are going to create an object that receives all the purchases, and this will give us the\\nnecessary metrics. We could get some of these values out of the box by simply using the\\nmin() and max() built-in functions, but that would require iterating all of the purchases\\nmore than once, so instead, we are using our custom object, which will get these values in a\\nsingle iteration.\\n\\n[ 189 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe code that will get the numbers for us looks rather simple. It\\'s just an object with a\\nmethod that will process all prices in one go, and, at each step, will update the value of each\\nparticular metric we are interested in. First, we will show the first implementation in the\\nfollowing listing, and, later on in this chapter (once we have seen more about iteration), we\\nwill revisit this implementation and get a much better (and compact) version of it. For now,\\nwe are settling on the following:\\nclass PurchasesStats:\\ndef __init__(self, purchases):\\nself.purchases = iter(purchases)\\nself.min_price: float = None\\nself.max_price: float = None\\nself._total_purchases_price: float = 0.0\\nself._total_purchases = 0\\nself._initialize()\\ndef _initialize(self):\\ntry:\\nfirst_value = next(self.purchases)\\nexcept StopIteration:\\nraise ValueError(\"no values provided\")\\nself.min_price = self.max_price = first_value\\nself._update_avg(first_value)\\ndef process(self):\\nfor purchase_value in self.purchases:\\nself._update_min(purchase_value)\\nself._update_max(purchase_value)\\nself._update_avg(purchase_value)\\nreturn self\\ndef _update_min(self, new_value: float):\\nif new_value < self.min_price:\\nself.min_price = new_value\\ndef _update_max(self, new_value: float):\\nif new_value > self.max_price:\\nself.max_price = new_value\\n@property\\ndef avg_price(self):\\nreturn self._total_purchases_price / self._total_purchases\\ndef _update_avg(self, new_value: float):\\nself._total_purchases_price += new_value\\n\\n[ 190 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nself._total_purchases += 1\\ndef __str__(self):\\nreturn (\\nf\"{self.__class__.__name__}({self.min_price}, \"\\nf\"{self.max_price}, {self.avg_price})\"\\n)\\n\\nThis object will receive all the totals for the purchases and process the required values.\\nNow, we need a function that loads these numbers into something that this object can\\nprocess. Here is the first version:\\ndef _load_purchases(filename):\\npurchases = []\\nwith open(filename) as f:\\nfor line in f:\\n*_, price_raw = line.partition(\",\")\\npurchases.append(float(price_raw))\\nreturn purchases\\n\\nThis code works; it loads all the numbers of the file into a list that, when passed to our\\ncustom object, will produce the numbers we want. It has a performance issue, though. If\\nyou run it with a rather large dataset, it will take a while to complete, and it might even fail\\nif the dataset is large enough as to not fit into the main memory.\\nIf we take a look at our code that consumes this data, it is processing the purchases, one at\\na time, so we might be wondering why our producer fits everything in memory at once. It\\nis creating a list where it puts all of the content of the file, but we know we can do better.\\nThe solution is to create a generator. Instead of loading the entire content of the file in a list,\\nwe will produce the results one at a time. The code will now look like this:\\ndef load_purchases(filename):\\nwith open(filename) as f:\\nfor line in f:\\n*_, price_raw = line.partition(\",\")\\nyield float(price_raw)\\n\\nIf you measure the process this time, you will notice that the usage of memory has dropped\\nsignificantly. We can also see how the code looks simpler\\xe2\\x80\\x94there is no need to define the\\nlist (therefore, there is no need to append to it), and that the return statement also\\ndisappeared.\\nIn this case, the load_purchases function is a generator function, or simply a generator.\\n\\n[ 191 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIn Python, the mere presence of the keyword yield in any function makes it a generator,\\nand, as a result, when calling it, nothing other than creating an instance of the generator\\nwill happen:\\n>>> load_purchases(\"file\")\\n<generator object load_purchases at 0x...>\\n\\nA generator object is an iterable (we will revisit iterables in more detail later on), which\\nmeans that it can work with for loops. Notice how we did not have to change anything on\\nthe consumer code\\xe2\\x80\\x94our statistics processor remained the same, with the for loop\\nunmodified, after the new implementation.\\nWorking with iterables allows us to create these kinds of powerful abstractions that are\\npolymorphic with respect to for loops. As long as we keep the iterable interface, we can\\niterate over that object transparently.\\n\\nGenerator expressions\\nGenerators save a lot of memory, and since they are iterators, they are a convenient\\nalternative to other iterables or containers that require more space in memory such as lists,\\ntuples, or sets.\\nMuch like these data structures, they can also be defined by comprehension, only that it is\\ncalled a generator expression (there is an ongoing argument about whether they should be\\ncalled generator comprehensions. In this book, we will just refer to them by their canonical\\nname, but feel free to use whichever you prefer).\\nIn the same way, we would define a list comprehension. If we replace the square brackets\\nwith parenthesis, we get a generator that results from the expression. Generator\\nexpressions can also be passed directly to functions that work with iterables, such as sum(),\\nand, max():\\n>>> [x**2 for x in range(10)]\\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\\n>>> (x**2 for x in range(10))\\n<generator object <genexpr> at 0x...>\\n>>> sum(x**2 for x in range(10))\\n285\\n\\n[ 192 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nAlways pass a generator expression, instead of a list comprehension, to\\nfunctions that expect iterables, such as min(), max(), and sum(). This is\\nmore efficient and pythonic.\\n\\nIterating idiomatically\\nIn this section, we will first explore some idioms that come in handy when we have to deal\\nwith iteration in Python. These code recipes will help us get a better idea of the types of\\nthings we can do with generators (especially after we have already seen generator\\nexpressions), and how to solve typical problems in relation to them.\\nOnce we have seen some idioms, we will move on to exploring iteration in Python in more\\ndepth, analyzing the methods that make iteration possible, and how iterable objects work.\\n\\nIdioms for iteration\\nWe are already familiar with the built-in enumerate() function that, given an iterable, will\\nreturn another one on which the element is a tuple, whose first element is the enumeration\\nof the second one (corresponding to the element in the original iterable):\\n>>> list(enumerate(\"abcdef\"))\\n[(0, \\'a\\'), (1, \\'b\\'), (2, \\'c\\'), (3, \\'d\\'), (4, \\'e\\'), (5, \\'f\\')]\\n\\nWe wish to create a similar object, but in a more low-level fashion; one that can simply\\ncreate an infinite sequence. We want an object that can produce a sequence of numbers,\\nfrom a starting one, without any limits.\\nAn object as simple as the following one can do the trick. Every time we call this object, we\\nget the next number of the sequence ad infinitum:\\nclass NumberSequence:\\ndef __init__(self, start=0):\\nself.current = start\\ndef next(self):\\ncurrent = self.current\\nself.current += 1\\nreturn current\\n\\n[ 193 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nBased on this interface, we would have to use this object by explicitly invoking its next()\\nmethod:\\n>>> seq = NumberSequence()\\n>>> seq.next()\\n0\\n>>> seq.next()\\n1\\n>>> seq2 = NumberSequence(10)\\n>>> seq2.next()\\n10\\n>>> seq2.next()\\n11\\n\\nBut with this code, we cannot reconstruct the enumerate() function as we would like to,\\nbecause its interface does not support being iterated over a regular Python for loop, which\\nalso means that we cannot pass it as a parameter to functions that expect something to\\niterate over. Notice how the following code fails:\\n>>> list(zip(NumberSequence(), \"abcdef\"))\\nTraceback (most recent call last):\\nFile \"...\", line 1, in <module>\\nTypeError: zip argument #1 must support iteration\\n\\nThe problem lies in the fact that NumberSequence does not support iteration. To fix this,\\nwe have to make the object an iterable by implementing the magic\\nmethod __iter__(). We have also changed the previous next() method, by using the\\nmagic method __next__, which makes the object an iterator:\\nclass SequenceOfNumbers:\\ndef __init__(self, start=0):\\nself.current = start\\ndef __next__(self):\\ncurrent = self.current\\nself.current += 1\\nreturn current\\ndef __iter__(self):\\nreturn self\\n\\n[ 194 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThis has an advantage\\xe2\\x80\\x94not only can we iterate over the element, we also don\\'t even need\\nthe .next() method any more because having __next__() allows us to use the\\nnext() built-in function:\\n>>> list(zip(SequenceOfNumbers(), \"abcdef\"))\\n[(0, \\'a\\'), (1, \\'b\\'), (2, \\'c\\'), (3, \\'d\\'), (4, \\'e\\'), (5, \\'f\\')]\\n>>> seq = SequenceOfNumbers(100)\\n>>> next(seq)\\n100\\n>>> next(seq)\\n101\\n\\nThe next() function\\nThe next() built-in function will advance the iterable to its next element and return it:\\n>>> word = iter(\"hello\")\\n>>> next(word)\\n\\'h\\'\\n>>> next(word)\\n\\'e\\' # ...\\n\\nIf the iterator does not have more elements to produce, the StopIteration exception is\\nraised:\\n>>> ...\\n>>> next(word)\\n\\'o\\'\\n>>> next(word)\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nStopIteration\\n>>>\\n\\nThis exception signals that the iteration is over and that there are no more elements to\\nconsume.\\nIf we wish to handle this case, besides catching the StopIteration exception, we could\\nprovide this function with a default value in its second parameter. Should this be provided,\\nit will be the return value in lieu of throwing StopIteration:\\n>>> next(word, \"default value\")\\n\\'default value\\'\\n\\n[ 195 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nUsing a generator\\nThe previous code can be simplified significantly by simply using a generator. Generator\\nobjects are iterators. This way, instead of creating a class, we can define a function that\\nyield the values as needed:\\ndef sequence(start=0):\\nwhile True:\\nyield start\\nstart += 1\\n\\nRemember that from our first definition, the yield keyword in the body of the function\\nmakes it a generator. Because it is a generator, it\\'s perfectly fine to create an infinite loop\\nlike this, because, when this generator function is called, it will run all the code until the\\nnext yield statement is reached. It will produce its value and suspend there:\\n>>> seq = sequence(10)\\n>>> next(seq)\\n10\\n>>> next(seq)\\n11\\n>>> list(zip(sequence(), \"abcdef\"))\\n[(0, \\'a\\'), (1, \\'b\\'), (2, \\'c\\'), (3, \\'d\\'), (4, \\'e\\'), (5, \\'f\\')]\\n\\nItertools\\nWorking with iterables has the advantage that the code blends better with Python itself\\nbecause iteration is a key component of the language. Besides that, we can take full\\nadvantage of the itertools module (ITER-01). Actually, the sequence() generator we\\njust created is fairly similar to itertools.count(). However, there is more we can do.\\nOne of the nicest things about iterators, generators, and itertools, is that they are\\ncomposable objects that can be chained together.\\nFor instance, getting back to our first example that processed purchases in order to get\\nsome metrics, what if we want to do the same, but only for those values over a certain\\nthreshold? The naive approach of solving this problem would be to place the condition\\nwhile iterating:\\n# ...\\ndef process(self):\\nfor purchase in self.purchases:\\nif purchase > 1000.0:\\n...\\n\\n[ 196 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThis is not only non-Pythonic, but it\\'s also rigid (and rigidity is a trait that denotes bad\\ncode). It doesn\\'t handle changes very well. What if the number changes now? Do we pass it\\nby parameter? What if we need more than one? What if the condition is different (less than,\\nfor instance)? Do we pass a lambda?\\nThese questions should not be answered by this object, whose sole responsibility is to\\ncompute a set of well-defined metrics over a stream of purchases represented as numbers.\\nAnd, of course, the answer is no. It would be a huge mistake to make such a change (once\\nagain, clean code is flexible, and we don\\'t want to make it rigid by coupling this object to\\nexternal factors). These requirements will have to be addressed elsewhere.\\nIt\\'s better to keep this object independent of its clients. The less responsibility this class has,\\nthe more useful it will be for more clients, hence enhancing its chances of being reused.\\nInstead of changing this code, we\\'re going to keep it as it is and assume that the new data is\\nfiltered according to whatever requirements each customer of the class has.\\nFor instance, if we wanted to process only the first 10 purchases that amount to more than\\n1,000, we would do the following:\\n>>> from itertools import islice\\n>>> purchases = islice(filter(lambda p: p > 1000.0, purchases), 10)\\n>>> stats = PurchasesStats(purchases).process() # ...\\n\\nThere is no memory penalization for filtering this way because since they all are generators,\\nthe evaluation is always lazy. This gives us the power of thinking as if we had filtered the\\nentire set at once and then passed it to the object, but without actually fitting everything in\\nmemory.\\n\\nSimplifying code through iterators\\nNow, we will briefly discuss some situations that can be improved with the help of\\niterators, and occasionally the itertools module. After discussing each case, and its\\nproposed optimization, we will close each point with a corollary.\\n\\n[ 197 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nRepeated iterations\\nNow that we have seen more about iterators, and introduced the itertools module, we\\ncan show you how one of the first examples of this chapter (the one for computing statistics\\nabout some purchases), can be dramatically simplified:\\ndef process_purchases(purchases):\\nmin_, max_, avg = itertools.tee(purchases, 3)\\nreturn min(min_), max(max_), median(avg)\\n\\nIn this example, itertools.tee will split the original iterable into three new ones. We\\nwill use each of these for the different kinds of iterations that we require, without needing\\nto repeat three different loops over purchases.\\nThe reader can simply verify that if we pass an iterable object as the purchases parameter,\\nthis one is traversed only once (thanks to the itertools.tee function [see references]),\\nwhich was our main requirement. It is also possible to verify how this version is equivalent\\nto our original implementation. In this case, there is no need to manually raise ValueError\\nbecause passing an empty sequence to the min() function will do the same.\\nIf you are thinking about running a loop over the same object more than\\none time, stop and think if itertools.tee can be of any help.\\n\\nNested loops\\nIn some situations, we need to iterate over more than one dimension, looking for a value,\\nand nested loops come as the first idea. When the value is found, we need to stop iterating,\\nbut the break keyword doesn\\'t work entirely because we have to escape from two (or\\nmore) for loops, not just one.\\nWhat would be the solution for this? A flag signaling escape? No. Raising an exception?\\nNo, this would be the same as the flag, but even worse because we know that exceptions\\nare not to be used for control flow logic. Moving the code to a smaller function and return\\nit? Close, but not quite.\\nThe answer is, whenever possible, flat the iteration to a single for loop.\\nThis is the kind of code we would like to avoid:\\ndef search_nested_bad(array, desired_value):\\ncoords = None\\nfor i, row in enumerate(array):\\n\\n[ 198 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nfor j, cell in enumerate(row):\\nif cell == desired_value:\\ncoords = (i, j)\\nbreak\\nif coords is not None:\\nbreak\\nif coords is None:\\nraise ValueError(f\"{desired_value} not found\")\\nlogger.info(\"value %r found at [%i, %i]\", desired_value, *coords)\\nreturn coords\\n\\nAnd here is a simplified version of it that does not rely on flags to signal termination, and\\nhas a simpler, more compact structure of iteration:\\ndef _iterate_array2d(array2d):\\nfor i, row in enumerate(array2d):\\nfor j, cell in enumerate(row):\\nyield (i, j), cell\\ndef search_nested(array, desired_value):\\ntry:\\ncoord = next(\\ncoord\\nfor (coord, cell) in _iterate_array2d(array)\\nif cell == desired_value\\n)\\nexcept StopIteration:\\nraise ValueError(\"{desired_value} not found\")\\nlogger.info(\"value %r found at [%i, %i]\", desired_value, *coord)\\nreturn coord\\n\\nIt\\'s worth mentioning how the auxiliary generator that was created works as an abstraction\\nfor the iteration that\\'s required. In this case, we just need to iterate over two dimensions,\\nbut if we needed more, a different object could handle this without the client needing to\\nknow about it. This is the essence of the iterator design pattern, which, in Python, is\\ntransparent, since it supports iterator objects automatically, which is the topic covered in\\nthe next section.\\nTry to simplify the iteration as much as possible with as many\\nabstractions as are required, flatting the loops whenever possible.\\n\\n[ 199 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe iterator pattern in Python\\nHere, we will take a small detour from generators to understand iteration in Python more\\ndeeply. Generators are a particular case of iterable objects, but iteration in Python goes\\nbeyond generators, and being able to create good iterable objects will give us the chance to\\ncreate more efficient, compact, and readable code.\\nIn the previous code listings, we have been seeing examples of iterable objects that are\\nalso iterators, because they implement both the __iter__() and __next__() magic\\nmethods. While this is fine in general, it\\'s not strictly required that they always have to\\nimplement both methods, and here we\\'ll show the subtle differences between\\nan iterable object (one that implements __iter__) and an iterator (that\\nimplements __next__).\\nWe also explore other topics related to iterations, such as sequences and container objects.\\n\\nThe interface for iteration\\nAn iterable is an object that supports iteration, which, at a very high level, means that we\\ncan run a for .. in ... loop over it, and it will work without any issues.\\nHowever, iterable does not mean the same as iterator.\\nGenerally speaking, an iterable is just something we can iterate, and it uses an iterator to do\\nso. This means that in the __iter__ magic method, we would like to return an iterator,\\nnamely, an object with a __next__() method implemented.\\nAn iterator is an object that only knows how to produce a series of values, one at a time,\\nwhen it\\'s being called by the already explored built-in next() function. While the iterator\\nis not called, it\\'s simply frozen, sitting idly by until it\\'s called again for the next value to\\nproduce. In this sense, generators are iterators.\\nPython concept Magic method\\nIterable\\n\\nIterator\\n\\nConsiderations\\n\\n__iter__\\n\\nThey work with an iterator to construct the iteration logic.\\nThese objects can be iterated in a for ... in ...: loop\\n\\n__next__\\n\\nDefine the logic for producing values one at the time.\\nThe StopIteration exception signals that the iteration is\\nover.\\nThe values can be obtained one by one via the built-in next()\\nfunction.\\n\\n[ 200 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIn the following code, we will see an example of an iterator object that is not iterable\\xe2\\x80\\x94it\\nonly supports invoking its values, one at a time. Here, the name sequence refers just to a\\nseries of consecutive numbers, not to the sequence concept in Python, which will we\\nexplore later on:\\nclass SequenceIterator:\\ndef __init__(self, start=0, step=1):\\nself.current = start\\nself.step = step\\ndef __next__(self):\\nvalue = self.current\\nself.current += self.step\\nreturn value\\n\\nNotice that we can get the values of the sequence one at a time, but we can\\'t iterate over this\\nobject (this is fortunate because it would otherwise result in an endless loop):\\n>>> si = SequenceIterator(1, 2)\\n>>> next(si)\\n1\\n>>> next(si)\\n3\\n>>> next(si)\\n5\\n>>> for _ in SequenceIterator(): pass\\n...\\nTraceback (most recent call last):\\n...\\nTypeError: \\'SequenceIterator\\' object is not iterable\\n\\nThe error message is clear, as the object doesn\\'t implement __iter__().\\nJust for explanatory purposes, we can separate the iteration in another object (again, it\\nwould be enough to make the object implement both __iter__ and __next__, but doing\\nso separately will help clarify the distinctive point we\\'re trying to make in this explanation).\\n\\nSequence objects as iterables\\nAs we have just seen, if an object implements the __iter__() magic method, it means it\\ncan be used in a for loop. While this is a great feature, it\\'s not the only possible form of\\niteration we can achieve. When we write a for loop, Python will try to see if the object\\nwe\\'re using implements __iter__, and, if it does, it will use that to construct the iteration,\\nbut if it doesn\\'t, there are fallback options.\\n\\n[ 201 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIf the object happens to be a sequence (meaning that it implements __getitem__()\\nand __len__() magic methods), it can also be iterated. If that is the case, the interpreter\\nwill then provide values in sequence, until the IndexError exception is raised, which,\\nanalogous to the aforementioned StopIteration, also signals the stop for the iteration.\\nWith the sole purpose of illustrating such a behavior, we run the following experiment that\\nshows a sequence object that implements map() over a range of numbers:\\n# generators_iteration_2.py\\nclass MappedRange:\\n\"\"\"Apply a transformation to a range of numbers.\"\"\"\\ndef __init__(self, transformation, start, end):\\nself._transformation = transformation\\nself._wrapped = range(start, end)\\ndef __getitem__(self, index):\\nvalue = self._wrapped.__getitem__(index)\\nresult = self._transformation(value)\\nlogger.info(\"Index %d: %s\", index, result)\\nreturn result\\ndef __len__(self):\\nreturn len(self._wrapped)\\n\\nKeep in mind that this example is only designed to illustrate that an object such as this one\\ncan be iterated with a regular for loop. There is a logging line placed in the __getitem__\\nmethod to explore what values are passed while the object is being iterated, as we can see\\nfrom the following test:\\n>>> mr = MappedRange(abs, -10, 5)\\n>>> mr[0]\\nIndex 0: 10\\n10\\n>>> mr[-1]\\nIndex -1: 4\\n4\\n>>> list(mr)\\nIndex 0: 10\\nIndex 1: 9\\nIndex 2: 8\\nIndex 3: 7\\nIndex 4: 6\\nIndex 5: 5\\nIndex 6: 4\\nIndex 7: 3\\n\\n[ 202 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIndex 8: 2\\nIndex 9: 1\\nIndex 10: 0\\nIndex 11: 1\\nIndex 12: 2\\nIndex 13: 3\\nIndex 14: 4\\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4]\\n\\nAs a word of caution, it\\'s important to highlight that while it is useful to know this, it\\'s also\\na fallback mechanism for when the object doesn\\'t implement __iter__, so most of the time\\nwe\\'ll want to resort to these methods by thinking in creating proper sequences, and not just\\nobjects we want to iterate over.\\nWhen thinking about designing an object for iteration, favor a proper\\niterable object (with __iter__), rather than a sequence that can\\ncoincidentally also be iterated.\\n\\nCoroutines\\nAs we already know, generator objects are iterables. They implement __iter__() and\\n__next__(). This is provided by Python automatically so that when we create a generator\\nobject function, we get an object that can be iterated or advanced through the next()\\nfunction.\\nBesides this basic functionality, they have more methods so that they can work as\\ncoroutines (PEP-342). Here, we will explore how generators evolved into coroutines to\\nsupport the basis of asynchronous programming before we go into more detail in the next\\nsection, where we explore the new features of Python and the syntax that covers\\nprogramming asynchronously. The basic methods added in (PEP-342) to support\\ncoroutines are as follows:\\n.close()\\n.throw(ex_type[, ex_value[, ex_traceback]])\\n.send(value)\\n\\n[ 203 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe methods of the generator interface\\nIn this section, we will explore what each of the aforementioned methods does, how it\\nworks, and how it is expected to be used. By understanding how to use these methods, we\\nwill be able to make use of simple coroutines.\\nLater on, we will explore more advanced uses of coroutines, and how to delegate to subgenerators (coroutines) in order to refactor code, and how to orchestrate different\\ncoroutines.\\n\\nclose()\\nWhen calling this method, the generator will receive the GeneratorExit exception. If it\\'s\\nnot handled, then the generator will finish without producing any more values, and its\\niteration will stop.\\nThis exception can be used to handle a finishing status. In general, if our coroutine does\\nsome sort of resource management, we want to catch this exception and use that control\\nblock to release all resources being held by the coroutine. In general, it is similar to using a\\ncontext manager or placing the code in the finally block of an exception control, but\\nhandling this exception specifically makes it more explicit.\\nIn the following example, we have a coroutine that makes use of a database handler object\\nthat holds a connection to a database, and runs queries over it, streaming data by pages of a\\nfixed length (instead of reading everything that is available at once):\\ndef stream_db_records(db_handler):\\ntry:\\nwhile True:\\nyield db_handler.read_n_records(10)\\nexcept GeneratorExit:\\ndb_handler.close()\\n\\nAt each call to the generator, it will return 10 rows obtained from the database handler, but\\nwhen we decide to explicitly finish the iteration and call close(), we also want to close the\\nconnection to the database:\\n>>> streamer = stream_db_records(DBHandler(\"testdb\"))\\n>>> next(streamer)\\n[(0, \\'row 0\\'), (1, \\'row 1\\'), (2, \\'row 2\\'), (3, \\'row 3\\'), ...]\\n>>> next(streamer)\\n[(0, \\'row 0\\'), (1, \\'row 1\\'), (2, \\'row 2\\'), (3, \\'row 3\\'), ...]\\n>>> streamer.close()\\nINFO:...:closing connection to database \\'testdb\\'\\n\\n[ 204 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nUse the close() method on generators to perform finishing-up tasks\\nwhen needed.\\n\\nthrow(ex_type[, ex_value[, ex_traceback]])\\nThis method will throw the exception at the line where the generator is currently\\nsuspended. If the generator handles the exception that was sent, the code in that\\nparticular except clause will be called, otherwise, the exception will propagate to the\\ncaller.\\nHere, we are modifying the previous example slightly to show the difference when we use\\nthis method for an exception that is handled by the coroutine, and when it\\'s not:\\nclass CustomException(Exception):\\npass\\n\\ndef stream_data(db_handler):\\nwhile True:\\ntry:\\nyield db_handler.read_n_records(10)\\nexcept CustomException as e:\\nlogger.info(\"controlled error %r, continuing\", e)\\nexcept Exception as e:\\nlogger.info(\"unhandled error %r, stopping\", e)\\ndb_handler.close()\\nbreak\\n\\nNow, it is a part of the control flow to receive a CustomException, and, in such a case, the\\ngenerator will log an informative message (of course, we can adapt this according to our\\nbusiness logic on each case), and move on to the next yield statement, which is the line\\nwhere the coroutine reads from the database and returns that data.\\nThis particular example handles all exceptions, but if the last block (except Exception:)\\nwasn\\'t there, the result would be that the generator is raised at the line where the generator\\nis paused (again, the yield), and it will propagate from there to the caller:\\n>>> streamer = stream_data(DBHandler(\"testdb\"))\\n>>> next(streamer)\\n[(0, \\'row 0\\'), (1, \\'row 1\\'), (2, \\'row 2\\'), (3, \\'row 3\\'), (4, \\'row 4\\'), ...]\\n>>> next(streamer)\\n[(0, \\'row 0\\'), (1, \\'row 1\\'), (2, \\'row 2\\'), (3, \\'row 3\\'), (4, \\'row 4\\'), ...]\\n>>> streamer.throw(CustomException)\\n\\n[ 205 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nWARNING:controlled error CustomException(), continuing\\n[(0, \\'row 0\\'), (1, \\'row 1\\'), (2, \\'row 2\\'), (3, \\'row 3\\'), (4, \\'row 4\\'), ...]\\n>>> streamer.throw(RuntimeError)\\nERROR:unhandled error RuntimeError(), stopping\\nINFO:closing connection to database \\'testdb\\'\\nTraceback (most recent call last):\\n...\\nStopIteration\\n\\nWhen our exception from the domain was received, the generator continued. However,\\nwhen it received another exception that was not expected, the default block caught where\\nwe closed the connection to the database and finished the iteration, which resulted in the\\ngenerator being stopped. As we can see from the StopIteration that was raised, this\\ngenerator can\\'t be iterated further.\\n\\nsend(value)\\nIn the previous example, we created a simple generator that reads rows from a database,\\nand when we wished to finish its iteration, this generator released the resources linked to\\nthe database. This is a good example of using one of the methods that generators provide\\n(close), but there is more we can do.\\nAn obvious of such a generator is that it was reading a fixed number of rows from the\\ndatabase.\\nWe would like to parametrize that number (10) so that we can change it throughout\\ndifferent calls. Unfortunately, the next() function does not provide us with options for\\nthat. But luckily, we have send():\\ndef stream_db_records(db_handler):\\nretrieved_data = None\\nprevious_page_size = 10\\ntry:\\nwhile True:\\npage_size = yield retrieved_data\\nif page_size is None:\\npage_size = previous_page_size\\nprevious_page_size = page_size\\nretrieved_data = db_handler.read_n_records(page_size)\\nexcept GeneratorExit:\\ndb_handler.close()\\n\\n[ 206 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe idea is that we have now made the coroutine able to receive values from the caller by\\nmeans of the send() method. This method is the one that actually distinguishes a\\ngenerator from a coroutine because when it\\'s used, it means that the yield keyword will\\nappear on the right-hand side of the statement, and its return value will be assigned to\\nsomething else.\\nIn coroutines, we generally find the yield keyword to be used in the following form:\\nreceive = yield produced\\n\\nThe yield, in this case, will do two things. It will send produced back to the caller, which\\nwill pick it up on the next round of iteration (after calling next(), for example), and it will\\nsuspend there. At a later point, the caller will want to send a value back to the coroutine by\\nusing the send() method. This value will become the result of the yield statement,\\nassigned in this case to the variable named receive.\\nSending values to the coroutine only works when this one is suspended at a yield\\nstatement, waiting for something to produce. For this to happen, the coroutine will have to\\nbe advanced to that status. The only way to do this is by calling next() on it. This means\\nthat before sending anything to the coroutine, this has to be advanced at least once via the\\nnext() method. Failure to do so will result in an exception:\\n>>> c = coro()\\n>>> c.send(1)\\nTraceback (most recent call last):\\n...\\nTypeError: can\\'t send non-None value to a just-started generator\\n\\nAlways remember to advance a coroutine by calling next() before\\nsending any values to it.\\n\\nBack to our example. We are changing the way elements are produced or streamed to make\\nit able to receive the length of the records it expects to read from the database.\\nThe first time we call next(), the generator will advance up to the line containing yield; it\\nwill provide a value to the caller (None, as set in the variable), and it will suspend there).\\nFrom here, we have two options. If we choose to advance the generator by calling next(),\\nthe default value of 10 will be used, and it will go on with this as usual. This is because\\nnext() is technically the same as send(None), but this is covered in the if statement that\\nwill handle the value that we previously set.\\n\\n[ 207 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIf, on the other hand, we decide to provide an explicit value via send(<value>), this one\\nwill become the result of the yield statement, which will be assigned to the variable\\ncontaining the length of the page to use, which, in turn, will be used to read from the\\ndatabase.\\nSuccessive calls will have this logic, but the important point is that now we can\\ndynamically change the length of the data to read in the middle of the iteration, at any\\npoint.\\nNow that we understand how the previous code works, most Pythonistas would expect a\\nsimplified version of it (after all, Python is also about brevity and clean and compact code):\\ndef stream_db_records(db_handler):\\nretrieved_data = None\\npage_size = 10\\ntry:\\nwhile True:\\npage_size = (yield retrieved_data) or page_size\\nretrieved_data = db_handler.read_n_records(page_size)\\nexcept GeneratorExit:\\ndb_handler.close()\\n\\nThis version is not only more compact, but it also illustrates the idea better. The parenthesis\\naround the yield makes it clearer that it\\'s a statement (think of it as if it were a function\\ncall), and that we are using the result of it to compare it against the previous value.\\nThis works as we expect it does, but we always have to remember to advance the coroutine\\nbefore sending any data to it. If we forget to call the first next(), we\\'ll get a TypeError.\\nThis call could be ignored for our purposes because it doesn\\'t return anything we\\'ll use.\\nIt would be good if we could use the coroutine directly, right after it is created without\\nhaving to remember to call next() the first time, every time we are going to use it. Some\\nauthors (PYCOOK) devised an interesting decorator to achieve this. The idea of this\\ndecorator is to advance the coroutine, so the following definition works automatically:\\n@prepare_coroutine\\ndef stream_db_records(db_handler):\\nretrieved_data = None\\npage_size = 10\\ntry:\\nwhile True:\\npage_size = (yield retrieved_data) or page_size\\nretrieved_data = db_handler.read_n_records(page_size)\\nexcept GeneratorExit:\\ndb_handler.close()\\n\\n[ 208 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\n>>> streamer = stream_db_records(DBHandler(\"testdb\"))\\n>>> len(streamer.send(5))\\n5\\n\\nLet\\'s take an example we create the prepare_coroutine(), decorator.\\n\\nMore advanced coroutines\\nSo far, we have a better understanding of coroutines, and we are able to create simple ones\\nto handle small tasks. We can say that these coroutines are, in fact, just more advanced\\ngenerators (and that would be right, coroutines are just fancy generators), but, if we\\nactually want to start supporting more complex scenarios, we usually have to go for a\\ndesign that handles many coroutines concurrently, and that requires more features.\\nWhen handling many coroutines, we find new problems. As the control flow of our\\napplication becomes more complex, we want to pass values up and down the stack (as well\\nas exceptions), be able to capture values from sub-coroutines we might call at any level, and\\nfinally schedule multiple coroutines to run toward a common goal.\\nTo make things simpler, generators had to be extended once again. This is what PEP-380\\naddressed\\xe2\\x80\\x94by changing the semantic of generators so that they are able to return values,\\nand introducing the new yield from construction.\\n\\nReturning values in coroutines\\nAs introduced at the beginning of this chapter, the iteration is a mechanism that calls\\nnext() on an iterable object many times until a StopIteration exception is raised.\\nSo far, we have been exploring the iterative nature of generators\\xe2\\x80\\x94we produce values one at\\na time, and, in general, we only care about each value as it\\'s being produced at every step of\\nthe for loop. This is a very logical way of thinking about generators, but coroutines have a\\ndifferent idea; even though they are technically generators, they weren\\'t conceived with the\\nidea of iteration in mind, but with the goal of suspending the execution of a code until it\\'s\\nresumed later on.\\nThis is an interesting challenge; when we design a coroutine, we usually care more about\\nsuspending the state rather than iterating (and iterating a coroutine would be an odd case).\\nThe challenge lies in that it is easy to mix them both. This is because of a technical\\nimplementation detail; the support for coroutines in Python was built upon generators.\\n\\n[ 209 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIf we want to use coroutines to process some information and suspend its execution, it\\nwould make sense to think of them as lightweight threads (or green threads, as they are\\ncalled in other platforms). In such a case, it would make sense if they could return values,\\nmuch like calling any other regular function.\\nBut let\\'s remember that generators are not regular functions, so in a generator, the\\nconstruction value = generator() will do nothing other than create a generator object.\\nWhat would be the semantics for making a generator return a value? It will have to be after\\nthe iteration is done.\\nWhen a generator returns a value, it iteration is immediately stopped (it can\\'t be iterated\\nany further). To preserve the semantics, the StopIteration exception is still raised, and\\nthe value to be returned is stored inside the exception object. It\\'s the responsibility of the\\ncaller to catch it.\\nIn the following example, we are creating a simple generator that produces two values\\nand then returns a third. Notice how we have to catch the exception in order to get this\\nvalue, and how it\\'s stored precisely inside the exception under the attribute named value:\\n>>> def generator():\\n...\\nyield 1\\n...\\nyield 2\\n...\\nreturn 3\\n...\\n>>> value = generator()\\n>>> next(value)\\n1\\n>>> next(value)\\n2\\n>>> try:\\n...\\nnext(value)\\n... except StopIteration as e:\\n...\\nprint(\">>>>>> returned value \", e.value)\\n...\\n>>>>>> returned value 3\\n\\nDelegating into smaller coroutines \\xe2\\x80\\x93 the yield from\\nsyntax\\nThe previous feature is interesting in the sense that it opens up a lot of new possibilities\\nwith coroutines (generators), now that they can return values. But this feature, by itself,\\nwould not be so useful without proper syntax support, because catching the returned value\\nthis way is a bit cumbersome.\\n\\n[ 210 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThis is one of the main features of the yield from syntax. Among other things (that we\\'ll\\nreview in detail), it can collect the value returned by a sub-generator. Remember that we\\nsaid that returning data in a generator was nice, but that, unfortunately, writing statements\\nas value = generator() wouldn\\'t work? Well, writing it as value = yield from\\ngenerator() would.\\n\\nThe simplest use of yield from\\nIn its most basic form, the new yield from syntax can be used to chain generators from\\nnested for loops into a single one, which will end up with a single string of all the values in\\na continuous stream.\\nThe canonical example is about creating a function similar to itertools.chain() from\\nthe standard library. This is a very nice function because it allows you to pass any number\\nof iterables, and will return them all together in one stream.\\nThe naive implementation might look like this:\\ndef chain(*iterables):\\nfor it in iterables:\\nfor value in it:\\nyield value\\n\\nIt receives a variable number of iterables, traverses through all of them, and since each\\nvalue is iterable, it supports a for... in.. construction, so we have another for loop\\nto get every value inside each particular iterable, which is produced by the caller function.\\nThis might be helpful in multiple cases, such as chaining generators together or trying to\\niterate things that it wouldn\\'t normally be possible to compare in one go (such as lists with\\ntuples, and so on).\\nHowever, the yield from syntax allows us to go further and avoid the nested loop\\nbecause it\\'s able to produce the values from a sub-generator directly. In this case, we could\\nsimplify the code like this:\\ndef chain(*iterables):\\nfor it in iterables:\\nyield from it\\n\\nNotice that for both implementations, the behavior of the generator is exactly the same:\\n>>> list(chain(\"hello\", [\"world\"], (\"tuple\", \" of \", \"values.\")))\\n[\\'h\\', \\'e\\', \\'l\\', \\'l\\', \\'o\\', \\'world\\', \\'tuple\\', \\' of \\', \\'values.\\']\\n\\n[ 211 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThis means that we can use yield from over any other iterable, and it will work as if the\\ntop-level generator (the one the yield from is using) were generating those values itself.\\nThis works with any iterable, and even generator expressions aren\\'t the exception. Now\\nthat we\\'re familiar with its syntax, let\\'s see how we could write a simple generator function\\nthat will produce all the powers of a number (for instance, if provided with\\nall_powers(2, 3), it will have to produce 2^0, 2^1,... 2^3):\\ndef all_powers(n, pow):\\nyield from (n ** i for i in range(pow + 1))\\n\\nWhile this simplifies the syntax a bit, saving one line of a for statement isn\\'t a big\\nadvantage, and it wouldn\\'t justify adding such a change to the language.\\nIndeed, this is actually just a side effect and the real raison d\\'\\xc3\\xaatre of the yield from\\nconstruction is what we are going to explore in the following two sections.\\n\\nCapturing the value returned by a sub-generator\\nIn the following example, we have a generator that calls another two nested generators,\\nproducing values in a sequence. Each one of these nested generators returns a value, and\\nwe will see how the top-level generator is able to effectively capture the return value since\\nit\\'s calling the internal generators through yield from:\\ndef sequence(name, start, end):\\nlogger.info(\"%s started at %i\", name, start)\\nyield from range(start, end)\\nlogger.info(\"%s finished at %i\", name, end)\\nreturn end\\ndef main():\\nstep1 = yield from sequence(\"first\", 0, 5)\\nstep2 = yield from sequence(\"second\", step1, 10)\\nreturn step1 + step2\\n\\nThis is a possible execution of the code in main while it\\'s being iterated:\\n>>> g = main()\\n>>> next(g)\\nINFO:generators_yieldfrom_2:first started at 0\\n0\\n>>> next(g)\\n1\\n>>> next(g)\\n2\\n>>> next(g)\\n\\n[ 212 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\n3\\n>>> next(g)\\n4\\n>>> next(g)\\nINFO:generators_yieldfrom_2:first finished at 5\\nINFO:generators_yieldfrom_2:second started at 5\\n5\\n>>> next(g)\\n6\\n>>> next(g)\\n7\\n>>> next(g)\\n8\\n>>> next(g)\\n9\\n>>> next(g)\\nINFO:generators_yieldfrom_2:second finished at 10\\nTraceback (most recent call last):\\nFile \"<stdin>\", line 1, in <module>\\nStopIteration: 15\\n\\nThe first line of main delegates into the internal generator, and produces the values,\\nextracting them directly from it. This is nothing new, as we have already seen. Notice,\\nthough, how the sequence() generator function returns the end value, which is assigned\\nin the first line to the variable named step1, and how this value is correctly used at the\\nstart of the following instance of that generator.\\nIn the end, this other generator also returns the second end value (10), and the main\\ngenerator, in turn, returns the sum of them (5+10=15), which is the value we see once the\\niteration has stopped.\\nWe can use yield from to capture the last value of a coroutine after it\\nhas finished its processing.\\n\\nSending and receiving data to and from a sub-generator\\nNow, we will see the other nice feature of the yield from syntax, which is probably what\\ngives it its full power. As we have already introduced when we explored generators acting\\nas coroutines, we know that we can send values and throw exceptions at them, and, in such\\ncases, the coroutine will either receive the value for its internal processing, or it will have to\\nhandle the exception accordingly.\\n\\n[ 213 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nIf we now have a coroutine that delegates into other ones (such as in the previous example),\\nwe would also like to preserve this logic. Having to do so manually would be quite\\ncomplex (you can take a look at the code described in PEP-380 if we didn\\'t have this\\nhandled by yield from automatically.\\nIn order to illustrate this, let\\'s keep the same top-level generator (main) unmodified with\\nrespect to the previous example (calling other internal generators), but let\\'s modify the\\ninternal generators to make them able to receive values and handle exceptions. The code is\\nprobably not idiomatic, only for the purposes of showing how this mechanism works:\\ndef sequence(name, start, end):\\nvalue = start\\nlogger.info(\"%s started at %i\", name, value)\\nwhile value < end:\\ntry:\\nreceived = yield value\\nlogger.info(\"%s received %r\", name, received)\\nvalue += 1\\nexcept CustomException as e:\\nlogger.info(\"%s is handling %s\", name, e)\\nreceived = yield \"OK\"\\nreturn end\\n\\nNow, we will call the main coroutine, not only by iterating it, but also by passing values\\nand throwing exceptions at it to see how they are handled inside sequence:\\n>>> g = main()\\n>>> next(g)\\nINFO: first started at 0\\n0\\n>>> next(g)\\nINFO: first received None\\n1\\n>>> g.send(\"value for 1\")\\nINFO: first received \\'value for 1\\'\\n2\\n>>> g.throw(CustomException(\"controlled error\"))\\nINFO: first is handling controlled error\\n\\'OK\\'\\n... # advance more times\\nINFO:second started at 5\\n5\\n>>> g.throw(CustomException(\"exception at second generator\"))\\nINFO: second is handling exception at second generator\\n\\'OK\\'\\n\\n[ 214 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThis example is showing us a lot of different things. Notice how we never send values\\nto sequence, but only to main, and even so, the code that is receiving those values is the\\nnested generators. Even though we never explicitly send anything to sequence, it\\'s\\nreceiving the data as it\\'s being passed along by yield from.\\nThe main coroutine calls two other coroutines internally, producing their values, and it will\\nbe suspended at a particular point in time in any of those. When it\\'s stopped at the first one,\\nwe can see the logs telling us that it is that instance of the coroutine that received the value\\nwe sent. The same happens when we throw an exception to it. When the first coroutine\\nfinishes, it returns the value that was assigned in the variable named step1, and passed as\\ninput for the second coroutine, which will do the same (it will handle the send()\\nand throw() calls, accordingly).\\nThe same happens for the values that each coroutine produces. When we are at any given\\nstep, the return from calling send() corresponds to the value that the subcoroutine (the\\none that main is currently suspended at) has produced. When we throw an exception that is\\nbeing handled, the sequence coroutine produces the value OK, which is propagated to the\\ncalled (main), and which in turn will end up at main\\'s caller.\\n\\nAsynchronous programming\\nWith the constructions we have seen so far, we are able to create asynchronous programs in\\nPython. This means that we can create programs that have many coroutines, schedule them\\nto work in a particular order, and switch between them when they\\'re suspended after a\\nyield from has been called on each of them.\\nThe main advantage that we can take out of this is the possibility of parallelizing I/O\\noperations in a non-blocking way. What we would need is a low-level generator (usually\\nimplemented by a third-party library) that knows how to handle the actual I/O while the\\ncoroutine is suspended. The idea is for the coroutine to effect suspension so that our\\nprogram can handle another task in the meantime. The way the application would retrieve\\nthe control back is by means of the yield from statement, which will suspend and\\nproduce a value to the caller (as in the examples we saw previously when we used this\\nsyntax to alter the control flow of the program).\\nThis is roughly the way asynchronous programming had been working in Python for quite\\na few years, until it was decided that better syntactic support was needed.\\n\\n[ 215 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe fact that coroutines and generators are technically the same causes some confusion.\\nSyntactically (and technically), they are the same, but semantically, they are different. We\\ncreate generators when we want to achieve efficient iteration. We typically create\\ncoroutines with the goal of running non-blocking I/O operations.\\nWhile this difference is clear, the dynamic nature of Python would still allow developers to\\nmix these different type of objects, ending up with a runtime error at a very late stage of the\\nprogram. Remember that in the simplest and most basic form of the yield from syntax,\\nwe used this construction over iterables (we created a sort of chain function applied over\\nstrings, lists, and so on). None of these objects were coroutines, and it still worked. Then,\\nwe saw that we can have multiple coroutines, use yield from to send the value (or\\nexceptions), and get some results back. These are clearly two very different use cases,\\nhowever, if we write something along the lines of the following statement:\\nresult = yield from iterable_or_awaitable()\\n\\nIt\\'s not clear what iterable_or_awaitable returns. It can be a simple iterable such as a\\nstring, and it might still be syntactically correct. Or, it might be an actual coroutine. The cost\\nof this mistake will be paid much later.\\nFor this reason, the typing system in Python had to be extended. Before Python 3.5,\\ncoroutines were just generators with a @coroutine decorator applied, and they were to be\\ncalled with the yield from syntax. Now, there is a specific type of object, that is, a\\ncoroutine.\\nThis change heralded, syntax changes as well. The await and async def syntax were\\nintroduced. The former is intended to be used instead of yield from, and it only works\\nwith awaitable objects (which coroutines conveniently happen to be). Trying to\\ncall await with something that doesn\\'t respect the interface of an awaitable will raise an\\nexception. The async def is the new way of defining coroutines, replacing the\\naforementioned decorator, and this actually creates an object that, when called, will return\\nan instance of a coroutine.\\nWithout going into all the details and possibilities of asynchronous programming in\\nPython, we can say that despite the new syntax and the new types, this is not doing\\nanything fundamentally different from concepts we have covered in this chapter.\\n\\n[ 216 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nThe idea of programming asynchronously in Python is that there is an event loop\\n(typically asyncio because it\\'s the one that is included in the standard library, but there\\nare many others that will work just the same) that manages a series of coroutines. These\\ncoroutines belong to the event loop, which is going to call them according to its scheduling\\nmechanism. When each one of these runs, it will call our code (according to the logic we\\nhave defined inside the coroutine we programmed), and when we want to get control back\\nto the event loop, we call await <coroutine>, which will process a task asynchronously.\\nThe event loop will resume and another coroutine will take place while that operation is left\\nrunning.\\nIn practice, there are more particularities and edge cases that are beyond the scope of this\\nbook. It is, however, worth mentioning that these concepts are related to the ideas\\nintroduced in this chapter and that this arena is another place where generators\\ndemonstrate being a core concept of the language, as there are many things constructed on\\ntop of them.\\n\\nSummary\\nGenerators are everywhere in Python. Since their inception in Python a long time ago, they\\nproved to be a great addition that makes programs more efficient and iteration much\\nsimpler.\\nAs time moved on, and more complex tasks needed to be added to Python, generators\\nhelped again in supporting coroutines.\\nAnd, while in Python, coroutines are generators, we still don\\'t have to forget that they\\'re\\nsemantically different. Generators are created with the idea of iteration, while coroutines\\nhave the goal of asynchronous programming (suspending and resuming the execution of a\\npart of our program at any given time). This distinction became so important that it made\\nPython\\'s syntax (and type system) evolve.\\nIteration and asynchronous programming constitute the last of the main pillars of Python\\nprogramming. Now, it\\'s time to see how everything fits together and to put all of these\\nconcepts we have been exploring over the past few chapters into action.\\nThe following chapters will describe other fundamental aspects of Python projects, such as\\ntesting, design patterns, and architecture.\\n\\n[ 217 ]\\n\\n\\x0cUsing Generators\\n\\nChapter 7\\n\\nReferences\\nHere is a list of information you can refer to:\\nPEP-234: Iterators (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0234/\\xe2\\x80\\x8b)\\nPEP-255: Simple Generators (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0255/\\xe2\\x80\\x8b)\\nITER-01: Python\\'s itertools module (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\nitertools.\\xe2\\x80\\x8bhtml)\\nGoF: The book written by Erich Gamma, Richard Helm, Ralph Johnson, John\\nVlissides named Design Patterns: Elements of Reusable Object-Oriented Software\\nPEP-342: Coroutines via Enhanced Generators (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\npeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0342/\\xe2\\x80\\x8b)\\nPYCOOK: The book written by Brian Jones, David Beazley named Python\\nCookbook: Recipes for Mastering Python 3, Third Edition\\nPY99: Fake threads (generators, coroutines, and continuations) (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bmail.\\npython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bpipermail/\\xe2\\x80\\x8bpython-\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8b1999-\\xe2\\x80\\x8bJuly/\\xe2\\x80\\x8b000467.\\xe2\\x80\\x8bhtml)\\nCORO-01: Co Routine (http://wiki.c2.com/?CoRoutine)\\nCORO-02: Generators Are Not Coroutines (http:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwiki.\\xe2\\x80\\x8bc2.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8b?\\nGeneratorsAreNotCoroutines)\\nTEE: The itertools.tee function (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\nitertools.\\xe2\\x80\\x8bhtml#itertools.\\xe2\\x80\\x8btee)\\n\\n[ 218 ]\\n\\n\\x0c8\\nUnit Testing and Refactoring\\nThe ideas explored in this chapter are fundamental pillars in the global context of the book,\\nbecause of their importance towards our ultimate goal: to write better and more\\nmaintainable software.\\nUnit tests (and any form of automatic tests, for that matter) are critical to software\\nmaintainability, and therefore are something that cannot be missing from any quality\\nproject. It is for that reason that this chapter is dedicated exclusively to aspects of\\nautomated testing as a key strategy, to safely modify the code, and iterate over it, in\\nincrementally better versions.\\nAfter this chapter, we will have gained more insight into the following:\\nWhy automated tests are critical for projects that run under an agile software\\ndevelopment methodology\\nHow unit tests work as a heuristic of the quality of the code\\nWhat frameworks and tools are available to develop automated tests and set up\\nquality gates\\nTaking advantage of unit tests to understand the domain problem better and\\ndocument code\\nConcepts related to unit testing, such as test-driven development\\n\\nDesign principles and unit testing\\nIn this section, are first going to take a look at unit testing from a conceptual point of view.\\nWe will revisit some of the software engineering principles we discussed in the previous to\\nget an idea of how this is related to clean code.\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nAfter that, we will discuss in more detail how to put these concepts into practice (at the\\ncode level), and what frameworks and tools we can make use of.\\nFirst we quickly define what unit testing is about. Unit tests are parts of the code in charge\\nof validating other parts of the code. Normally, anyone would be tempted to say that unit\\ntests, validate the \"core\" of the application, but such definition regards unit tests to a\\nsecondary place, which is not the way they are thought of in this book. Unit tests are core,\\nand a critical component of the software and they should be treated with the same\\nconsiderations as the business logic.\\nA unit tests is a piece of code that imports parts of the code with the business logic, and\\nexercises its logic, asserting several scenarios with the idea to guarantee certain conditions.\\nThere are some traits that unit tests must have, such as:\\nIsolation: unit test should be completely independent from any other external\\nagent, and they have to focus only on the business logic. For this reason, they do\\nnot connect to a database, they don\\'t perform HTTP requests, etc. Isolation also\\nmeans that the tests are independent among themselves: they must be able to run\\nin any order, without depending on any previous state.\\nPerformance: unit tests must run quickly. They are intended to be run multiple\\ntimes, repeatedly.\\nSelf-validating: The execution of a unit tests determines its result. There should\\nbe no extra step required to interpret the unit test (much less manual).\\nMore concretely, in Python this means that we will have new *.py files where we are going\\nto place our unit tests, and they are going to be called by some tool. These files will have\\nimport statements, to take what we need from our business logic (what we intend to test),\\nand inside this file we program the tests themselves. Afterwards, a tool will collect our unit\\ntests and run them, giving a result.\\nThis last part is what self-validation actually means. When the tool calls our files, a Python\\nprocess will be launched, and our tests will be running on it. If the tests fail, the process will\\nhave exited with an error code (in a Unix environment, this can be any number different\\nthan 0). The standard is that the tool runs the test, and prints a dot (.) for every successful\\ntest, an F if the test failed (the condition of the test was not satisfied), and an E if there was\\nan exception.\\n\\n[ 220 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nA note about other forms of automated testing\\nUnit tests are intended to verify very small units, for example a function, or a method. We\\nwant from our unit tests to reach a very detailed level of granularity, testing as much code\\nas possible. To test a class we would not want to use a unit tests, but rather a test suite,\\nwhich is a collection of unit tests. Each one of them will be testing something more specific,\\nlike a method of that class.\\nThis is not the only form of unit tests, and it cannot catch every possible error. There are\\nalso acceptance and integration tests, both out of the scope of this book.\\nIn an integration test, we will want to test multiple components at once. In this case we\\nwant to validate if collectively, they work as expected. In this case is acceptable (more than\\nthat, desirable) to have side-effects, and to forget about isolation, meaning that we will\\nwant to issue HTTP requests, connect to databases, and so on.\\nAn acceptance test is an automated form of testing that tries to validate the system from the\\nperspective of an user, typically executing use cases.\\nThese two last forms of testing lose another nice trait with respect of unit tests: velocity. As\\nyou can imagine, they will take more time to run, therefore they will be run less frequently.\\nIn a good development environment, the programmer will have the entire test suite, and\\nwill run unit tests all the time, repeatedly, while he or she is making changes to the code,\\niterating, refactoring, and so on. Once the changes is ready, and the pull request is open, the\\ncontinuous integration service will run the build for that branch, where the unit tests will\\nrun, as long as the integration or acceptance tests that might exist. Needless to say, the\\nstatus of the build should be successful (green) before merging, but the important part is\\nthe difference between the kind of tests: we want to run unit tests all the time, and less\\nfrequently those test that take longer. For this reason, we want to have a lot of small unit\\ntests, and a few automated tests, strategically designed to cover as much as possible of\\nwhere the unit tests could not reach (the database, for instance).\\nFinally, a word to the wise. Remember that this book encourages pragmatism. Besides these\\ndefinitions give, and the points made about unit tests in the beginning of the section, the\\nreader has to keep in mind that the best solution according to your criteria and context,\\nshould predominate. Nobody knows your system better than you. Which means, if for\\nsome reason you have to write an unit tests that needs to launch a Docker container to test\\nagainst a database, go for it. As we have repeatedly remembered throughout the book,\\npracticality beats purity.\\n\\n[ 221 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nUnit testing and agile software development\\nIn modern software development, we want to deliver value constantly, and as quickly as\\npossible. The rationale behind these goals is that the earlier we get feedback, the less the\\nimpact, and the easier it will be to change. These are no new ideas at all; some of them\\nresemble manufacturing principles from decades ago, and others (such as the idea of\\ngetting feedback from stakeholders as soon as possible and iterating upon it) you can find\\nin essays such as The Cathedral and the Bazaar (abbreviated as CatB).\\nTherefore, we want to be able to respond effectively to changes, and for that, the software\\nwe write will have to change. Like we mentioned in the previous chapters, we want our\\nsoftware to be adaptable, flexible, and extensible.\\nThe code alone (regardless of how well written and designed it is) cannot guarantee us that\\nit\\'s flexible enough to be changed. Let\\'s say we design a piece of software following the\\nSOLID principles, and in one part we actually have a set of components that comply with\\nthe open/closed principle, meaning that we can easily extend them without affecting too\\nmuch existing code. Assume further that the code is written in a way that favors\\nrefactoring, so we could change it as required. What\\'s to say that when we make these\\nchanges, we aren\\'t introducing any bugs? How do we know that existing functionality is\\npreserved? Would you feel confident enough releasing that to your users? Will they believe\\nthat the new version works just as expected?\\nThe answer to all of these questions is that we can\\'t be sure unless we have a formal proof\\nof it. And unit tests are just that, formal proof that the program works according to the\\nspecification.\\nUnit (or automated) tests, therefore, work as a safety net that gives us the confidence to\\nwork on our code. Armed with these tools, we can efficiently work on our code, and\\ntherefore this is what ultimately determines the velocity (or capacity) of the team working\\non the software product. The better the tests, the more likely it is we can deliver value\\nquickly without being stopped by bugs every now and then.\\n\\nUnit testing and software design\\nThis is the other face of the coin when it comes to the relationship between the main code\\nand unit testing. Besides the pragmatic reasons explored in the previous section, it comes\\ndown to the fact that good software is testable software. Testability (the quality attribute\\nthat determines how easy to test software is) is not just a nice to have, but a driver for clean\\ncode.\\n\\n[ 222 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nUnit tests aren\\'t just something complementary to the main code base, but rather something\\nthat has a direct impact and real influence on how the code is written. There are many\\nlevels of this, from the very beginning when we realize that the moment we want to add\\nunit tests for some parts of our code, we have to change it (resulting in a better version of\\nit), to its ultimate expression (explored near the end of this chapter) when the entire code\\n(the design) is driven by the way it\\'s going to be tested via test-driven design.\\nStarting off with a simple example, we will show you a small use case in which tests (and\\nthe need to test our code) lead to improvements in the way our code ends up being written.\\nIn the following example, we will simulate a process that requires sending metrics to an\\nexternal system about the results obtained at each particular task (as always, details won\\'t\\nmake any difference as long as we focus on the code). We have a Process object that\\nrepresents some task on the domain problem, and it uses a metrics client (an external\\ndependency and therefore something we don\\'t control) to send the actual metrics to the\\nexternal entity (that this could be sending data to syslog, or statsd, for instance):\\nclass MetricsClient:\\n\"\"\"3rd-party metrics client\"\"\"\\ndef send(self, metric_name, metric_value):\\nif not isinstance(metric_name, str):\\nraise TypeError(\"expected type str for metric_name\")\\nif not isinstance(metric_value, str):\\nraise TypeError(\"expected type str for metric_value\")\\nlogger.info(\"sending %s = %s\", metric_name, metric_value)\\n\\nclass Process:\\ndef __init__(self):\\nself.client = MetricsClient() # A 3rd-party metrics client\\ndef process_iterations(self, n_iterations):\\nfor i in range(n_iterations):\\nresult = self.run_process()\\nself.client.send(\"iteration.{}\".format(i), result)\\n\\n[ 223 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nIn the simulated version of the third-party client, we put the requirement that the\\nparameters provided must be of type string. Therefore, if the result of the run_process\\nmethod is not a string, we might expect it to fail, and indeed it does:\\nTraceback (most recent call last):\\n...\\nraise TypeError(\"expected type str for metric_value\")\\nTypeError: expected type str for metric_value\\n\\nRemember that this validation is out of our hands and we cannot change the code, so we\\nmust provide the method with parameters of the correct type before proceeding. But since\\nthis is a bug we detected, we first want to write a unit test to make sure it will not happen\\nagain. We do this to actually prove that we fixed the issue, and to protect against this bug in\\nthe future, regardless of how many times the code is refactored.\\nIt would be possible to test the code as is by mocking the client of the Process object (we\\nwill see how to do so in the section about mock objects, when we explore the tools for unit\\ntesting), but doing so runs more code than is needed (notice how the part we want to test is\\nnested into the code). Moreover, it\\'s good that the method is relatively small, because if it\\nweren\\'t, the test would have to run even more undesired parts that we might also need to\\nmock. This is another example of good design (small, cohesive functions or methods), that\\nrelates to testability.\\nFinally, we decide not to go to much trouble and test just the part that we need to, so\\ninstead of interacting with the client directly on the main method, we delegate to a\\nwrapper method, and the new class looks like this:\\nclass WrappedClient:\\ndef __init__(self):\\nself.client = MetricsClient()\\ndef send(self, metric_name, metric_value):\\nreturn self.client.send(str(metric_name), str(metric_value))\\nclass Process:\\ndef __init__(self):\\nself.client = WrappedClient()\\n... # rest of the code remains unchanged\\n\\nIn this case, we opted for creating our own version of the client for metrics, that is, a\\nwrapper around the third-party library one we used to have. To do this, we place a class\\nthat (with the same interface) will make the conversion of the types accordingly.\\n\\n[ 224 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nThis way of using composition resembles the adapter design pattern (we\\'ll explore design\\npatterns in the next chapter, so, for now, it\\'s just an informative message), and since this is a\\nnew object in our domain, it can have its respective unit tests. Having this object will make\\nthings simpler to test, but more importantly, now that we look at it, we realize that this is\\nprobably the way the code should have been written in the first place. Trying to write a unit\\ntest for our code made us realize that we were missing an important abstraction entirely!\\nNow that we have separated the method as it should be, let\\'s write the actual unit test for it.\\nThe details about the unittest module used in this example will be explored in more\\ndetail in the part of the chapter where we explore testing tools and libraries, but for now\\nreading the code will give us a first impression on how to test it, and it will make the\\nprevious concepts a little less abstract:\\nimport unittest\\nfrom unittest.mock import Mock\\nclass TestWrappedClient(unittest.TestCase):\\ndef test_send_converts_types(self):\\nwrapped_client = WrappedClient()\\nwrapped_client.client = Mock()\\nwrapped_client.send(\"value\", 1)\\nwrapped_client.client.send.assert_called_with(\"value\", \"1\")\\n\\nMock is a type that\\'s available in the unittest.mock module, which is a quite convenient\\n\\nobject to ask about all sort of things. For example, in this case, we\\'re using it in place of the\\nthird-party library (mocked into the boundaries of the system, as commented on the next\\nsection) to check that it\\'s called as expected (and once again, we\\'re not testing the library\\nitself, only that it is called correctly). Notice how we run a call like the one our Process\\nobject, but we expect the parameters to be converted to strings.\\n\\nDefining the boundaries of what to test\\nTesting requires effort. And if we are not careful when deciding what to test, we will never\\nend testing, hence wasting a lot of effort without achieving much.\\nWe should scope the testing to the boundaries of our code. If we don\\'t, we would have to\\nalso test the dependencies (external/third-party libraries or modules) or our code, and then\\ntheir respective dependencies, and so on and so forth in a never-ending journey. It\\'s not our\\nresponsibility to test dependencies, so we can assume that these projects have tests of their\\nown. It would be enough just to test that the correct calls to external dependencies are done\\nwith the correct parameters (and that might even be an acceptable use of patching), but we\\nshouldn\\'t put more effort in than that.\\n\\n[ 225 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nThis is another instance where good software design pays off. If we have been careful in\\nour design, and clearly defined the boundaries of our system (that is, we designed towards\\ninterfaces, instead of concrete implementations that will change, hence inverting the\\ndependencies over external components to reduce temporal coupling), then it will be much\\nmore easier to mock these interfaces when writing unit tests.\\nIn good unit testing, we want to patch on the boundaries of our system and focus on the\\ncore functionality to be exercised. We don\\'t test external libraries (third-party tools installed\\nvia pip, for instance), but instead, we check that they are called correctly. When we explore\\nmock objects later on in this chapter, we will review techniques and tools for performing\\nthese types of assertion.\\n\\nFrameworks and tools for testing\\nThere are a lot of tools we can use for writing out unit tests, all of them with pros and cons\\nand serving different purposes. But among all of them, there are two that will most likely\\ncover almost every scenario, and therefore we limit this section to just them.\\nAlong with testing frameworks and test running libraries, it\\'s often common to find projects\\nthat configure code coverage, which they use as a quality metric. Since coverage (when\\nused as a metric) is misleading, after seeing how to create unit tests we\\'ll discuss why it\\'s\\nnot to be taken lightly.\\n\\nFrameworks and libraries for unit testing\\nIn this section, we will discuss two frameworks for writing and running unit tests. The first\\none, unittest, is available in the standard library of Python, while the second\\none, pytest, has to be installed externally via pip.\\nunittest: https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b3/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bunittest.\\xe2\\x80\\x8bhtml\\npytest: https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpytest.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8ben/\\xe2\\x80\\x8blatest/\\xe2\\x80\\x8b\\n\\nWhen it comes to covering testing scenarios for our code, unittest alone will most likely\\nsuffice, since it has plenty of helpers. However, for more complex systems on which we\\nhave multiple dependencies, connections to external systems, and probably the need to\\npatch objects, and define fixtures parameterize test cases, then pytest looks like a more\\ncomplete option.\\n\\n[ 226 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nWe will use a small program as an example to show you how could it be tested using both\\noptions which in the end will help us to get a better picture of how the two of them\\ncompare.\\nThe example demonstrating testing tools is a simplified version of a version control tool\\nthat supports code reviews in merge requests. We will start with the following criteria:\\nA merge request is rejected if at least one person disagrees with the changes\\nIf nobody has disagreed, and the merge request is good for at least two other\\ndevelopers, it\\'s approved\\nIn any other case, its status is pending\\nAnd here is what the code might look like:\\nfrom enum import Enum\\nclass MergeRequestStatus(Enum):\\nAPPROVED = \"approved\"\\nREJECTED = \"rejected\"\\nPENDING = \"pending\"\\nclass MergeRequest:\\ndef __init__(self):\\nself._context = {\\n\"upvotes\": set(),\\n\"downvotes\": set(),\\n}\\n@property\\ndef status(self):\\nif self._context[\"downvotes\"]:\\nreturn MergeRequestStatus.REJECTED\\nelif len(self._context[\"upvotes\"]) >= 2:\\nreturn MergeRequestStatus.APPROVED\\nreturn MergeRequestStatus.PENDING\\ndef upvote(self, by_user):\\nself._context[\"downvotes\"].discard(by_user)\\nself._context[\"upvotes\"].add(by_user)\\ndef downvote(self, by_user):\\nself._context[\"upvotes\"].discard(by_user)\\nself._context[\"downvotes\"].add(by_user)\\n\\n[ 227 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nunittest\\nThe unittest module is a great option with which to start writing unit tests because it\\nprovides a rich API to write all kinds of testing conditions, and since it\\'s available in the\\nstandard library, it\\'s quite versatile and convenient.\\nThe unittest module is based on the concepts of JUnit (from Java), which in turn is also\\nbased on the original ideas of unit testing that come from Smalltalk, so it\\'s object-oriented in\\nnature. For this reason, tests are written through objects, where the checks are verified by\\nmethods, and it\\'s common to group tests by scenarios in classes.\\nTo start writing unit tests, we have to create a test class that inherits from\\nunittest.TestCase, and define the conditions we want to stress on its methods. These\\nmethods should start with test_*, and can internally use any of the methods inherited\\nfrom unittest.TestCase to check conditions that must hold true.\\nSome examples of conditions we might want to verify for our case are as follows:\\nclass TestMergeRequestStatus(unittest.TestCase):\\ndef test_simple_rejected(self):\\nmerge_request = MergeRequest()\\nmerge_request.downvote(\"maintainer\")\\nself.assertEqual(merge_request.status, MergeRequestStatus.REJECTED)\\ndef test_just_created_is_pending(self):\\nself.assertEqual(MergeRequest().status, MergeRequestStatus.PENDING)\\ndef test_pending_awaiting_review(self):\\nmerge_request = MergeRequest()\\nmerge_request.upvote(\"core-dev\")\\nself.assertEqual(merge_request.status, MergeRequestStatus.PENDING)\\ndef test_approved(self):\\nmerge_request = MergeRequest()\\nmerge_request.upvote(\"dev1\")\\nmerge_request.upvote(\"dev2\")\\nself.assertEqual(merge_request.status, MergeRequestStatus.APPROVED)\\n\\nThe API for unit testing provides many useful methods for comparison, the most common\\none being assertEquals(<actual>, <expected>[, message]), which can be used to\\ncompare the result of the operation against the value we were expecting, optionally using a\\nmessage that will be shown in the case of an error.\\n\\n[ 228 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nAnother useful testing method allows us to check whether a certain exception was raised or\\nnot. When something exceptional happens, we raise an exception in our code to prevent\\ncontinuous processing under the wrong assumptions, and also to inform the caller that\\nsomething is wrong with the call as it was performed. This is the part of the logic that ought\\nto be tested, and that\\'s what this method is for.\\nImagine that we are now extending our logic a little bit further to allow users to close their\\nmerge requests, and once this happens, we don\\'t want any more votes to take place (it\\nwouldn\\'t make sense to evaluate a merge request once this was already closed). To prevent\\nthis from happening, we extend our code and we raise an exception on the unfortunate\\nevent when someone tries to cast a vote on a closed merge request.\\nAfter adding two new statuses (OPEN and CLOSED), and a new close() method, we\\nmodify the previous methods for the voting to handle this check first:\\nclass MergeRequest:\\ndef __init__(self):\\nself._context = {\\n\"upvotes\": set(),\\n\"downvotes\": set(),\\n}\\nself._status = MergeRequestStatus.OPEN\\ndef close(self):\\nself._status = MergeRequestStatus.CLOSED\\n...\\ndef _cannot_vote_if_closed(self):\\nif self._status == MergeRequestStatus.CLOSED:\\nraise MergeRequestException(\"can\\'t vote on a closed merge\\nrequest\")\\ndef upvote(self, by_user):\\nself._cannot_vote_if_closed()\\nself._context[\"downvotes\"].discard(by_user)\\nself._context[\"upvotes\"].add(by_user)\\ndef downvote(self, by_user):\\nself._cannot_vote_if_closed()\\nself._context[\"upvotes\"].discard(by_user)\\nself._context[\"downvotes\"].add(by_user)\\n\\n[ 229 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nNow, we want to check that this validation indeed works. For this, we\\'re going to use\\nthe asssertRaises and assertRaisesRegex methods:\\ndef test_cannot_upvote_on_closed_merge_request(self):\\nself.merge_request.close()\\nself.assertRaises(\\nMergeRequestException, self.merge_request.upvote, \"dev1\"\\n)\\ndef test_cannot_downvote_on_closed_merge_request(self):\\nself.merge_request.close()\\nself.assertRaisesRegex(\\nMergeRequestException,\\n\"can\\'t vote on a closed merge request\",\\nself.merge_request.downvote,\\n\"dev1\",\\n)\\n\\nThe former will expect that the provided exception is raised when calling the callable in the\\nsecond argument, with the arguments (*args and **kwargs) on the rest of the function,\\nand if that\\'s not the case it will fail, saying that the exception that was expected to be raised,\\nwasn\\'t. The latter does the same but it also checks that the exception that was raised,\\ncontains the message matching the regular expression that was provided as a parameter.\\nEven if the exception is raised, but with a different message (not matching the regular\\nexpression), the test will fail.\\nTry to check for the error message, as not only will the exception, as an\\nextra check, be more accurate and ensure that it is actually the exception\\nwe want that is being triggered, it will check whether another one of the\\nsame types got there by chance.\\n\\nParametrized tests\\nNow, we would like to test how the threshold acceptance for the merge request works, just\\nby providing data samples of what the context looks like without needing the entire\\nMergeRequest object. We want to test the part of the status property that is after the line\\nthat checks if it\\'s closed, but independently.\\nThe best way to achieve this is to separate that component into another class, use\\ncomposition, and then move on to test this new abstraction with its own test suite:\\nclass AcceptanceThreshold:\\ndef __init__(self, merge_request_context: dict) -> None:\\nself._context = merge_request_context\\n\\n[ 230 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\ndef status(self):\\nif self._context[\"downvotes\"]:\\nreturn MergeRequestStatus.REJECTED\\nelif len(self._context[\"upvotes\"]) >= 2:\\nreturn MergeRequestStatus.APPROVED\\nreturn MergeRequestStatus.PENDING\\n\\nclass MergeRequest:\\n...\\n@property\\ndef status(self):\\nif self._status == MergeRequestStatus.CLOSED:\\nreturn self._status\\nreturn AcceptanceThreshold(self._context).status()\\n\\nWith these changes, we can run the tests again and verify that they pass, meaning that this\\nsmall refactor didn\\'t break anything of the current functionality (unit tests ensure\\nregression). With this, we can proceed with our goal to write tests that are specific to the\\nnew class:\\nclass TestAcceptanceThreshold(unittest.TestCase):\\ndef setUp(self):\\nself.fixture_data = (\\n(\\n{\"downvotes\": set(), \"upvotes\": set()},\\nMergeRequestStatus.PENDING\\n),\\n(\\n{\"downvotes\": set(), \"upvotes\": {\"dev1\"}},\\nMergeRequestStatus.PENDING,\\n),\\n(\\n{\"downvotes\": \"dev1\", \"upvotes\": set()},\\nMergeRequestStatus.REJECTED\\n),\\n(\\n{\"downvotes\": set(), \"upvotes\": {\"dev1\", \"dev2\"}},\\nMergeRequestStatus.APPROVED\\n),\\n)\\ndef test_status_resolution(self):\\nfor context, expected in self.fixture_data:\\nwith self.subTest(context=context):\\nstatus = AcceptanceThreshold(context).status()\\nself.assertEqual(status, expected)\\n\\n[ 231 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nHere, in the setUp() method, we define the data fixture to be used throughout the tests. In\\nthis case, it\\'s not actually needed, because we could have put it directly on the method, but\\nif we expect to run some code before any test is executed, this is the place to write it,\\nbecause this method is called once before every test is run.\\nBy writing this new version of the code, the parameters under the code being tested are\\nclearer and more compact, and at each case, it will report the results.\\nTo simulate that we\\'re running all of the parameters, the test iterates over all the data, and\\nexercises the code with each instance. One interesting helper here is the use of subTest,\\nwhich in this case we use to mark the test condition being called. If one of these iterations\\nfailed, unittest would report it with the corresponding value of the variables that were\\npassed to the subTest (in this case, it was named context, but any series of keyword\\narguments would work just the same). For example, one error occurrence might look like\\nthis:\\nFAIL: (context={\\'downvotes\\': set(), \\'upvotes\\': {\\'dev1\\', \\'dev2\\'}})\\n---------------------------------------------------------------------Traceback (most recent call last):\\nFile \"\" test_status_resolution\\nself.assertEqual(status, expected)\\nAssertionError: <MergeRequestStatus.APPROVED: \\'approved\\'> !=\\n<MergeRequestStatus.REJECTED: \\'rejected\\'>\\n\\nIf you choose to parameterize tests, try to provide the context of each\\ninstance of the parameters with as much information as possible to make\\ndebugging easier.\\n\\npytest\\nPytest is a great testing framework, and can be installed via pip install pytest. A\\ndifference with respect to unittest is that, while it\\'s still possible to classify test scenarios\\nin classes and create object-oriented models of our tests, this is not actually mandatory, and\\nit\\'s possible to write unit tests with less boilerplate by just checking the conditions we want\\nto verify with the assert statement.\\nBy default, making comparisons with an assert statement will be enough for pytest to\\nidentify a unit test and report its result accordingly. More advanced uses such as those seen\\nin the previous section are also possible, but they require using specific functions from the\\npackage.\\n\\n[ 232 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nA nice feature is that the command pytests will run all the tests that it can discover, even\\nif they were written with unittest. This compatibility makes it easier to transition\\nfrom unittest toward pytest gradually.\\n\\nBasic test cases with pytest\\nThe conditions we tested in the previous section can be rewritten in simple functions with\\npytest.\\nSome examples with simple assertions are as follows:\\ndef test_simple_rejected():\\nmerge_request = MergeRequest()\\nmerge_request.downvote(\"maintainer\")\\nassert merge_request.status == MergeRequestStatus.REJECTED\\ndef test_just_created_is_pending():\\nassert MergeRequest().status == MergeRequestStatus.PENDING\\ndef test_pending_awaiting_review():\\nmerge_request = MergeRequest()\\nmerge_request.upvote(\"core-dev\")\\nassert merge_request.status == MergeRequestStatus.PENDING\\n\\nBoolean equality comparisons don\\'t require more than a simple assert statement, whereas\\nother kinds of checks like the ones for the exceptions do require that we use some functions:\\ndef test_invalid_types():\\nmerge_request = MergeRequest()\\npytest.raises(TypeError, merge_request.upvote, {\"invalid-object\"})\\ndef test_cannot_vote_on_closed_merge_request():\\nmerge_request = MergeRequest()\\nmerge_request.close()\\npytest.raises(MergeRequestException, merge_request.upvote, \"dev1\")\\nwith pytest.raises(\\nMergeRequestException,\\nmatch=\"can\\'t vote on a closed merge request\",\\n):\\nmerge_request.downvote(\"dev1\")\\n\\n[ 233 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nIn this case, pytest.raises is the equivalent of unittest.TestCase.assertRaises,\\nand it also accepts that it be called both as a method and as a context manager. If we want\\nto check the message of the exception, instead of a different method\\n(like assertRaisesRegex), the same function has to be used, but as a context manager,\\nand by providing the match parameter with the expression we would like to identify.\\npytest will also wrap the original exception into a custom one that can be expected (by\\nchecking some of its attributes such as .value, for instance) in case we want to check for\\n\\nmore conditions, but this use of the function covers the vast majority of cases.\\n\\nParametrized tests\\nRunning parametrized tests with pytest is better, not only because it provides a cleaner\\nAPI, but also because each combination of the test with its parameters generates a new test\\ncase.\\nTo work with this, we have to use the pytest.mark.parametrize decorator on our test.\\nThe first parameter of the decorator is a string indicating the names of the parameters to\\npass to the test function, and the second has to be iterable with the respective values for\\nthose parameters.\\nNotice how the body of the testing function is reduced to one line (after removing the\\ninternal for loop, and its nested context manager), and the data for each test case is\\ncorrectly isolated from the body of the function, making it easier to extend and maintain:\\n@pytest.mark.parametrize(\"context,expected_status\", (\\n(\\n{\"downvotes\": set(), \"upvotes\": set()},\\nMergeRequestStatus.PENDING\\n),\\n(\\n{\"downvotes\": set(), \"upvotes\": {\"dev1\"}},\\nMergeRequestStatus.PENDING,\\n),\\n(\\n{\"downvotes\": \"dev1\", \"upvotes\": set()},\\nMergeRequestStatus.REJECTED\\n),\\n(\\n{\"downvotes\": set(), \"upvotes\": {\"dev1\", \"dev2\"}},\\nMergeRequestStatus.APPROVED\\n),\\n))\\ndef test_acceptance_threshold_status_resolution(context, expected_status):\\nassert AcceptanceThreshold(context).status() == expected_status\\n\\n[ 234 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nUse @pytest.mark.parametrize to eliminate repetition, keep the body\\nof the test as cohesive as possible, and make the parameters (test inputs or\\nscenarios) that the code must support explicitly.\\n\\nFixtures\\nOne of the great things about pytest is how it facilitates creating reusable features so that\\nwe can feed our tests with data or objects in order to test more effectively and without\\nrepetition.\\nFor example, we might want to create a MergeRequest object in a particular state, and use\\nthat object in multiple tests. We define our object as a fixture by creating a function and\\napplying the @pytest.fixture decorator. The tests that want to use that fixture will have\\nto have a parameter with the same name as the function that\\'s defined, and pytest will\\nmake sure that it\\'s provided:\\n@pytest.fixture\\ndef rejected_mr():\\nmerge_request = MergeRequest()\\nmerge_request.downvote(\"dev1\")\\nmerge_request.upvote(\"dev2\")\\nmerge_request.upvote(\"dev3\")\\nmerge_request.downvote(\"dev4\")\\nreturn merge_request\\ndef test_simple_rejected(rejected_mr):\\nassert rejected_mr.status == MergeRequestStatus.REJECTED\\ndef test_rejected_with_approvals(rejected_mr):\\nrejected_mr.upvote(\"dev2\")\\nrejected_mr.upvote(\"dev3\")\\nassert rejected_mr.status == MergeRequestStatus.REJECTED\\ndef test_rejected_to_pending(rejected_mr):\\nrejected_mr.upvote(\"dev1\")\\nassert rejected_mr.status == MergeRequestStatus.PENDING\\ndef test_rejected_to_approved(rejected_mr):\\nrejected_mr.upvote(\"dev1\")\\nrejected_mr.upvote(\"dev2\")\\nassert rejected_mr.status == MergeRequestStatus.APPROVED\\n\\n[ 235 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nRemember that tests affect the main code as well, so the principles of clean code apply to\\nthem as well. In this case, the Don\\'t Repeat Yourself (DRY) principle that we explored in\\nprevious chapters appears once again, and we can achieve it with the help of pytest\\nfixtures.\\nBesides creating multiple objects or exposing data that will be used throughout the test\\nsuite, it\\'s also possible to use them to set up some conditions, for example, to globally patch\\nsome functions that we don\\'t want to be called, or when we want patch objects to be used\\ninstead.\\n\\nCode coverage\\nTests runners support coverage plugins (to be installed via pip) that will provide useful\\ninformation about what lines in the code have been executed while the tests were running.\\nThis information is of great help so that we know which parts of the code need to be\\ncovered by tests, as well identifying improvements to be made (both in the production code\\nand in the tests). One of the most widely used libraries for this is coverage (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpypi.\\norg/\\xe2\\x80\\x8bproject/\\xe2\\x80\\x8bcoverage/\\xe2\\x80\\x8b).\\nWhile they are of great help (and we highly recommend that you use them and configure\\nyour project to run coverage in the CI when tests are run), they can also be misleading;\\nparticularly in Python, we can get a false impression if we don\\'t pay close attention to the\\ncoverage report.\\n\\nSetting up rest coverage\\nIn the case of pytest, we have to install the pytest-cov package (at the time of this\\nwriting, version 2.5.1 is used in this book). Once installed, when the tests are run, we have\\nto tell the pytest runner that pytest-cov will also run, and which package (or packages)\\nshould be covered (among other parameters and configurations).\\nThis package supports multiple configurations, like different sorts of output formats, and\\nit\\'s easy to integrate it with any CI tool, but among all these features a highly recommended\\noption is to set the flag that will tell us which lines haven\\'t been covered by tests yet,\\nbecause this is what\\'s going to help us diagnose our code and allow us to start writing more\\ntests.\\n\\n[ 236 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nTo show you an example of what this would look like, use the following command:\\npytest \\\\\\n--cov-report term-missing \\\\\\n--cov=coverage_1 \\\\\\ntest_coverage_1.py\\n\\nThis will produce an output similar to the following:\\ntest_coverage_1.py ................ [100%]\\n----------- coverage: platform linux, python 3.6.5-final-0 ----------Name\\nStmts Miss Cover Missing\\n--------------------------------------------coverage_1.py 38\\n1 97%\\n53\\n\\nHere, it\\'s telling us that there is a line that doesn\\'t have unit tests so that we can take a look\\nand see how to write a unit test for it. This is a common scenario where we realize that to\\ncover those missing lines, we need to refactor the code by creating smaller methods. As a\\nresult, our code will look much better, as in the example we saw at the beginning of this\\nchapter.\\nThe problem lies in the inverse situation\\xe2\\x80\\x94can we trust the high coverage? Does it mean our\\ncode is correct? Unfortunately, having good test coverage is necessary but in sufficient\\ncondition for clean code. Not having tests for parts of the code is clearly something bad.\\nHaving tests is actually very good (and we can say this for the tests that do exist), and\\nactually asserts real conditions that they are a guarantee of quality for that part of the code.\\nHowever, we cannot say that is all that is required; despite having a high level of coverage,\\neven more tests are required.\\nThese are the caveats of test coverage, which we will mention in the next section.\\n\\nCaveats of test coverage\\nPython is interpreted and, at a very high-level, coverage tools take advantage of this to\\nidentify the lines that were interpreted (run) while the tests were running. It will then\\nreport this at the end. The fact that a line was interpreted does not mean that it was\\nproperly tested, and this is why we should be careful about reading the final coverage\\nreport and trusting what it says.\\n\\n[ 237 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nThis is actually true for any language. The fact that a line was exercised does not mean at all\\nthat it was stressed with all its possible combinations. The fact that all branches run\\nsuccessfully with the provided data only means that the code supported that combination,\\nbut it doesn\\'t tell us anything about any other possible combinations of parameters that\\nwould make the program crash.\\nUse coverage as a tool to find blind spots in the code, but not as a metric\\nor target goal.\\n\\nMock objects\\nThere are cases where our code is not the only thing that will be present in the context of\\nour tests. After all, the systems we design and build have to do something real, and that\\nusually means connecting to external services (databases, storage services, external APIs,\\ncloud services, and so on). Because they need to have those side-effects, they\\'re inevitable.\\nAs much as we abstract our code, program towards interfaces, and isolate code from\\nexternal factors in order to minimize side-effects, they will be present in our tests, and we\\nneed an effective way to handle that.\\nMock objects are one of the best tactics to defend against undesired side-effects. Our code\\n\\nmight need to perform an HTTP request or send a notification email, but we surely don\\'t\\nwant that to happen in our unit tests. Besides, unit tests should run quickly, as we want to\\nrun them quite often (all the time, actually), and this means we cannot afford latency.\\nTherefore, real unit tests don\\'t use any actual service\\xe2\\x80\\x94they don\\'t connect to any database,\\nthey don\\'t issue HTTP requests, and basically, they do nothing other than exercise the logic\\nof the production code.\\nWe need tests that do such things, but they aren\\'t units. Integration tests are supposed to\\ntest functionality with a broader perspective, almost mimicking the behavior of a user. But\\nthey aren\\'t fast. Because they connect to external systems and services, they take longer to\\nrun and are more expensive. In general, we would like to have lots of unit tests that run\\nreally quickly in order to run them all the time, and have integration tests run less often (for\\ninstance, on any new merge request).\\nWhile mock objects are useful, abusing their use ranges between a code smell or an antipattern is the first caveat we would like to mention before going into the details of it.\\n\\n[ 238 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nA fair warning about patching and mocks\\nWe said before that unit tests help us write better code, because the moment we want to\\nstart testing parts of the code, we usually have to write them to be testable, which often\\nmeans they are also cohesive, granular, and small. These are all good traits to have in a\\nsoftware component.\\nAnother interesting gain is that testing will help us notice code smells in parts where we\\nthought our code was correct. One of the main warnings that our code has code smells is\\nwhether we find ourselves trying to monkey-patch (or mock) a lot of different things just to\\ncover a simple test case.\\nThe unittest module provides a tool for patching our objects at unittest.mock.patch.\\nPatching means that the original code (given by a string denoting its location at import\\ntime), will be replaced by something else, other than its original code, being the default a\\nmock object. This replaces the code at run-time, and has the disadvantage that we are losing\\ncontact with the original code that was there in the first place, making our tests a little more\\nshallow. It also carries performance considerations, because of the overhead that imposes\\nmodifying objects in the interpreter at run-time, and it\\'s something that might end up\\nupdate if we refactor our code and move things around.\\nUsing monkey-patching or mocks in our tests might be acceptable, and by itself it doesn\\'t\\nrepresent an issue. On the other hand, abuse in monkey-patching is indeed a flag that\\nsomething has to be improved in our code.\\n\\nUsing mock objects\\nIn unit testing terminology, there are several types of object that fall into the category\\nnamed test doubles. A test double is a type of object that will take the place of a real one in\\nour test suite for different kinds of reasons (maybe we don\\'t need the actual production\\ncode, but just a dummy object would work, or maybe we can\\'t use it because it requires\\naccess to services or it has side-effects that we don\\'t want in our unit tests, and so on).\\nThere are different types of test double, such as dummy objects, stubs, spies, or mocks.\\nMocks are the most general type of object, and since they\\'re quite flexible and versatile, they\\nare appropriate for all cases without needing to go into much detail about the rest of them.\\nIt is for this reason that the standard library also includes an object of this kind, and it is\\ncommon in most Python programs. That\\'s the one we are going to be using\\nhere: unittest.mock.Mock.\\n\\n[ 239 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nA mock is a type of object created to a specification (usually resembling the object of a\\nproduction class) and some configured responses (that is, we can tell the mock what it\\nshould return upon certain calls, and what its behavior should be). The Mock object will\\nthen record, as part of its internal status, how it was called (with what parameters, how\\nmany times, and so on), and we can use that information to verify the behavior of our\\napplication at a later stage.\\nIn the case of Python, the Mock object that\\'s available from the standard library provides a\\nnice API to make all sorts of behavioral assertions, such as checking how many times the\\nmock was called, with what parameters, and so on.\\n\\nTypes of mocks\\nThe standard library provides Mock and MagicMock objects in the unittest.mock\\nmodule. The former is a test double that can be configured to return any value and will\\nkeep track of the calls that were made to it. The latter does the same, but it also supports\\nmagic methods. This means that, if we have written idiomatic code that uses magic\\nmethods (and parts of the code we are testing will rely on that), it\\'s likely that we will have\\nto use a MagicMock instance instead of just a Mock.\\nTrying to use Mock when our code needs to call magic methods will result in an error. See\\nthe following code for an example of this:\\nclass GitBranch:\\ndef __init__(self, commits: List[Dict]):\\nself._commits = {c[\"id\"]: c for c in commits}\\ndef __getitem__(self, commit_id):\\nreturn self._commits[commit_id]\\ndef __len__(self):\\nreturn len(self._commits)\\n\\ndef author_by_id(commit_id, branch):\\nreturn branch[commit_id][\"author\"]\\n\\n[ 240 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nWe want to test this function; however, another test needs to call\\nthe author_by_id function. For some reason, since we\\'re not testing that function, any\\nvalue provided to that function (and returned) will be good:\\ndef test_find_commit():\\nbranch = GitBranch([{\"id\": \"123\", \"author\": \"dev1\"}])\\nassert author_by_id(\"123\", branch) == \"dev1\"\\n\\ndef test_find_any():\\nauthor = author_by_id(\"123\", Mock()) is not None\\n# ... rest of the tests..\\n\\nAs anticipated, this will not work:\\ndef author_by_id(commit_id, branch):\\n> return branch[commit_id][\"author\"]\\nE TypeError: \\'Mock\\' object is not subscriptable\\n\\nUsing MagicMock instead will work. We can even configure the magic method of this type\\nof mock to return something we need in order to control the execution of our test:\\ndef test_find_any():\\nmbranch = MagicMock()\\nmbranch.__getitem__.return_value = {\"author\": \"test\"}\\nassert author_by_id(\"123\", mbranch) == \"test\"\\n\\nA use case for test doubles\\nTo see a possible use of mocks, we need to add a new component to our application that\\nwill be in charge of notifying the merge request of the status of the build. When a build\\nis finished, this object will be called with the ID of the merge request and the status of the\\nbuild, and it will update the status of the merge request with this information by\\nsending an HTTP POST request to a particular fixed endpoint:\\n# mock_2.py\\nfrom datetime import datetime\\nimport requests\\nfrom constants import STATUS_ENDPOINT\\n\\nclass BuildStatus:\\n\"\"\"The CI status of a pull request.\"\"\"\\n\\n[ 241 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\n@staticmethod\\ndef build_date() -> str:\\nreturn datetime.utcnow().isoformat()\\n@classmethod\\ndef notify(cls, merge_request_id, status):\\nbuild_status = {\\n\"id\": merge_request_id,\\n\"status\": status,\\n\"built_at\": cls.build_date(),\\n}\\nresponse = requests.post(STATUS_ENDPOINT, json=build_status)\\nresponse.raise_for_status()\\nreturn response\\n\\nThis class has many side-effects, but one of them is an important external dependency\\nwhich is hard to surmount. If we try to write a test over it without modifying anything, it\\nwill fail with a connection error as soon as it tries to perform the HTTP connection.\\nAs a testing goal, we just want to make sure that the information is composed correctly, and\\nthat library requests are being called with the appropriate parameters. Since this is an\\nexternal dependency, we don\\'t test requests; just checking that it\\'s called correctly will be\\nenough.\\nAnother problem we will face when trying to compare data being sent to the library is that\\nthe class is calculating the current timestamp, which is impossible to predict in a unit test.\\nPatching datetime directly is not possible, because the module is written in C. There are\\nsome external libraries that can do that (freezegun, for example), but they come with a\\nperformance penalty, and for this example would be overkill. Therefore, we opt to\\nwrapping the functionality we want in a static method that we will be able to patch.\\nNow that we have established the points that need to be replaced in the code, let\\'s write the\\nunit test:\\n# test_mock_2.py\\nfrom unittest import mock\\nfrom constants import STATUS_ENDPOINT\\nfrom mock_2 import BuildStatus\\n\\n@mock.patch(\"mock_2.requests\")\\ndef test_build_notification_sent(mock_requests):\\nbuild_date = \"2018-01-01T00:00:01\"\\nwith mock.patch(\"mock_2.BuildStatus.build_date\",\\n\\n[ 242 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nreturn_value=build_date):\\nBuildStatus.notify(123, \"OK\")\\nexpected_payload = {\"id\": 123, \"status\": \"OK\", \"built_at\":\\nbuild_date}\\nmock_requests.post.assert_called_with(\\nSTATUS_ENDPOINT, json=expected_payload\\n)\\n\\nFirst, we use mock.patch as a decorator to replace the requests module. The result of this\\nfunction will create a mock object that will be passed as a parameter to the test\\n(named mock_requests in this example). Then, we use this function again, but this time as\\na context manager to change the return value of the method of the class that computes the\\ndate of the build, replacing the value with one we control, that we will use in the assertion.\\nOnce we have all of this in place, we can call the class method with some parameters, and\\nthen we can use the mock object to check how it was called. In this case, we are using the\\nmethod to see if requests.post was indeed called with the parameters as we wanted\\nthem to be composed.\\nThis is a nice feature of mocks\\xe2\\x80\\x94not only do they put some boundaries around all external\\ncomponents (in this case to prevent actually sending some notifications or issuing HTTP\\nrequests), but they also provide a useful API to verify the calls and their parameters.\\nWhile, in this case, we were able to test the code by setting the respective mock objects in\\nplace, it\\'s also true that we had to patch quite a lot in proportion to the total lines of code for\\nthe main functionality. There is no rule about the ratio of pure productive code being tested\\nversus how many parts of that code we have to mock, but certainly, by using common\\nsense, we can see that, if we had to patch quite a lot of things in the same parts, something\\nis not clearly abstracted, and it looks like a code smell.\\nIn the next section, we will explore how to refactor code to overcome this issue.\\n\\nRefactoring\\nRefactoring is a critical activity in software maintenance, yet something that can\\'t be done\\n(at least correctly) without having unit tests. Every now and then, we need to support a\\nnew feature or use our software in unintended ways. We need to realize that the only way\\nto accommodate such requirements is by first refactoring our code, make it more generic.\\nOnly then can we move forward.\\n\\n[ 243 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nTypically, when refactoring our code, we want to improve its structure and make it better,\\nsometimes more generic, more readable, or more flexible. The challenge is to achieve these\\ngoals while at the same time preserving the exact same functionality it had prior to the\\nmodifications that were made. This means that, in the eyes of the clients of those\\ncomponents we\\'re refactoring, it might as well be the case that nothing had happened at all.\\nThis constraint of having to support the same functionalities as before but with a different\\nversion of the code implies that we need to run regression tests on code that was modified.\\nThe only cost-effective way of running regression tests is if those tests are automatic. The\\nmost cost-effective version of automatic tests is unit tests.\\n\\nEvolving our code\\nIn the previous example, we were able to separate out the side-effects from our code to\\nmake it testable by patching those parts of the code that depended on things we couldn\\'t\\ncontrol on the unit test. This is a good approach since, after all, the mock.patch function\\ncomes in handy for these sorts of task and replaces the objects we tell it to, giving us back a\\nMock object.\\nThe downside of that is that we have to provide the path of the object we are going to\\nmock, including the module, as a string. This is a bit fragile, because if we refactor our code\\n(let\\'s say we rename the file or move it to some other location), all the places with the patch\\nwill have to be updated, or the test will break.\\nIn the example, the fact that the notify() method directly depends on an implementation\\ndetail (the requests module) is a design issue, that is, it is taking its toll on the unit tests as\\nwell with the aforementioned fragility that is implied.\\nWe still need to replace those methods with doubles (mocks), but if we refactor the code,\\nwe can do it in a better way. Let\\'s separate these methods into smaller ones, and most\\nimportantly inject the dependency rather than keep it fixed. The code now applies\\nthe dependency inversion principle, and it expects to work with something that supports\\nan interface (in this example, implicit one) such as the one the requests module provides:\\nfrom datetime import datetime\\nfrom constants import STATUS_ENDPOINT\\n\\nclass BuildStatus:\\nendpoint = STATUS_ENDPOINT\\n\\n[ 244 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\ndef __init__(self, transport):\\nself.transport = transport\\n@staticmethod\\ndef build_date() -> str:\\nreturn datetime.utcnow().isoformat()\\ndef compose_payload(self, merge_request_id, status) -> dict:\\nreturn {\\n\"id\": merge_request_id,\\n\"status\": status,\\n\"built_at\": self.build_date(),\\n}\\ndef deliver(self, payload):\\nresponse = self.transport.post(self.endpoint, json=payload)\\nresponse.raise_for_status()\\nreturn response\\ndef notify(self, merge_request_id, status):\\nreturn self.deliver(self.compose_payload(merge_request_id, status))\\n\\nWe separate the methods (not notify is now compose + deliver),\\nmake compose_payload() a new method (so that we can replace, without the need to\\npatch the class), and require the transport dependency to be injected. Now that\\ntransport is a dependency, it is much easier to change that object for any double we want.\\nIt is even possible to expose a fixture of this object with the doubles replaced as required:\\n@pytest.fixture\\ndef build_status():\\nbstatus = BuildStatus(Mock())\\nbstatus.build_date = Mock(return_value=\"2018-01-01T00:00:01\")\\nreturn bstatus\\n\\ndef test_build_notification_sent(build_status):\\nbuild_status.notify(1234, \"OK\")\\nexpected_payload = {\\n\"id\": 1234,\\n\"status\": \"OK\",\\n\"built_at\": build_status.build_date(),\\n}\\n\\n[ 245 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nbuild_status.transport.post.assert_called_with(\\nbuild_status.endpoint, json=expected_payload\\n)\\n\\nProduction code isn\\'t the only thing that evolves\\nWe keep saying that unit tests are as important as production code. And if we are careful\\nenough with production code as to create the best possible abstraction, why wouldn\\'t we\\ndo the same for unit tests?\\nIf the code for unit tests is as important as the main code, then it\\'s definitely wise to design\\nit with extensibility in mind and make it as maintainable as possible. After all, this is the\\ncode that will have to be maintained by an engineer other than its original author, so it has\\nto be readable.\\nThe reason why we pay so much attention to make the code\\'s flexibility is that we know\\nrequirements change and evolve over time, and eventually as domain business rules\\nchange, our code will have to change as well to support these new requirements. Since the\\nproduction code changed to support new requirements, in turn, the testing code will have\\nto change as well to support the newer version of the production code.\\nIn one of the first examples we used, we created a series of tests for the merge request\\nobject, trying different combinations and checking the status at which the merge request\\nwas left. This is a good first approach, but we can do better than that.\\nOnce we understand the problem better, we can start creating better abstractions. With this,\\nthe first idea that comes to mind is that we can create a higher-level abstraction that checks\\nfor particular conditions. For example, if we have an object that is a test suite that\\nspecifically targets the MergeRequest class, we know its functionality will be limited to the\\nbehavior of this class (because it should comply to the SRP), and therefore we could create\\nspecific testing methods on this testing class. These will only make sense for this class, but\\nthat will be helpful in reducing a lot of boilerplate code.\\n\\n[ 246 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nInstead of repeating assertions that follow the exact same structure, we can create a method\\nthat encapsulates this and reuse it across all of the tests:\\nclass TestMergeRequestStatus(unittest.TestCase):\\ndef setUp(self):\\nself.merge_request = MergeRequest()\\ndef assert_rejected(self):\\nself.assertEqual(\\nself.merge_request.status, MergeRequestStatus.REJECTED\\n)\\ndef assert_pending(self):\\nself.assertEqual(\\nself.merge_request.status, MergeRequestStatus.PENDING\\n)\\ndef assert_approved(self):\\nself.assertEqual(\\nself.merge_request.status, MergeRequestStatus.APPROVED\\n)\\ndef test_simple_rejected(self):\\nself.merge_request.downvote(\"maintainer\")\\nself.assert_rejected()\\ndef test_just_created_is_pending(self):\\nself.assert_pending()\\n\\nIf something changes with how we check the status of a merge request (or let\\'s say we want\\nto add extra checks), there is only one place (the assert_approved() method) that will\\nhave to be modified. More importantly, by creating these higher-level abstractions, the code\\nthat started as merely unit tests starts to evolve into what could end up being a testing\\nframework with its own API or domain language, making testing more declarative.\\n\\nMore about unit testing\\nWith the concepts we have revisited so far, we know how to test our code, think about our\\ndesign in terms of how it is going to be tested, and configure the tools in our project to run\\nthe automated tests that will give us some degree of confidence over the quality of the\\nsoftware we have written.\\n\\n[ 247 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nIf our confidence in the code is determined by the unit tests written on it, how do we know\\nthat they are enough? How could we be sure that we have been through enough on the test\\nscenarios and that we are not missing some tests? Who says that these tests are correct?\\nMeaning, who tests the tests?\\nThe first part of the question, about being thorough on the tests we wrote, is answered by\\ngoing beyond in our testing efforts through property-based testing.\\nThe second part of the question might have multiple answers from different points of view,\\nbut we are going to briefly mention mutation testing as a means of determining that our\\ntests are indeed correct. In this sense, we are thinking that the unit tests check our main\\nproductive code, and this works as a control for the unit tests as well.\\n\\nProperty-based testing\\nProperty-based testing consists of generating data for tests cases with the goal of finding\\nscenarios that will make the code fail, which weren\\'t covered by our previous unit tests.\\nThe main library for this is hypothesis which, configured along with our unit tests, will\\nhelp us find problematic data that will make our code fail.\\nWe can imagine that what this library does is find counter examples for our code. We write\\nour production code (and unit tests for it!), and we claim it\\'s correct. Now, with this library,\\nwe define some hypothesis that must hold for our code, and if there are some cases where\\nour assertions don\\'t hold, the hypothesis will provide a set of data that causes the error.\\nThe best thing about unit tests is that they make us think harder about our production code.\\nThe best thing about the hypothesis is that it makes us think harder about our unit tests.\\n\\nMutation testing\\nWe know that tests are the formal verification method we have to ensure that our code is\\ncorrect. And what makes sure that the test is correct? The production code, you might\\nthink, and yes, in a way this is correct, we can think of the main code as a counter balance\\nfor our tests.\\n\\n[ 248 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nThe point in writing unit tests is that we are protecting ourselves against bugs, and testing\\nfor failure scenarios we really don\\'t want to happen in production. It\\'s good that the tests\\npass, but it would be bad if they pass for the wrong reasons. That is, we can use unit tests as\\nan automatic regression tool\\xe2\\x80\\x94if someone introduces a bug in the code, later on, we expect\\nat least one of our tests to catch it and fail. If this doesn\\'t happen, either there is a test\\nmissing, or the ones we had are not doing the right checks.\\nThis is the idea behind mutation testing. With a mutation testing tool, the code will be\\nmodified to new versions (called mutants), that are variations of the original code but with\\nsome of its logic altered (for example, operators are swapped, conditions are inverted, and\\nso on). A good test suite should catch these mutants and kill them, in which case it means\\nwe can rely on the tests. If some mutants survive the experiment, it\\'s usually a bad sign. Of\\ncourse, this is not entirely precise, so there are intermediate states we might want to ignore.\\nTo quickly show you how this works and to allow you to get a practical idea of this, we are\\ngoing to use a different version of the code that computes the status of a merge request\\nbased on the number of approvals and rejections. This time, we have changed the code for a\\nsimple version that, based on these numbers, returns the result. We have moved the\\nenumeration with the constants for the statuses to a separate module so that it now looks\\nmore compact:\\n# File mutation_testing_1.py\\nfrom mrstatus import MergeRequestStatus as Status\\ndef evaluate_merge_request(upvote_count, downvotes_count):\\nif downvotes_count > 0:\\nreturn Status.REJECTED\\nif upvote_count >= 2:\\nreturn Status.APPROVED\\nreturn Status.PENDING\\n\\nAnd now will we add a simple unit test, checking one of the conditions and its expected\\nresult:\\n# file: test_mutation_testing_1.py\\nclass TestMergeRequestEvaluation(unittest.TestCase):\\ndef test_approved(self):\\nresult = evaluate_merge_request(3, 0)\\nself.assertEqual(result, Status.APPROVED)\\n\\nNow, we will install mutpy, a mutation testing tool for Python, with pip install mutpy,\\nand tell it to run the mutation testing for this module with these tests:\\n$ mut.py \\\\\\n--target mutation_testing_$N \\\\\\n\\n[ 249 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\n--unit-test test_mutation_testing_$N \\\\\\n--operator AOD `# delete arithmetic operator` \\\\\\n--operator AOR `# replace arithmetic operator` \\\\\\n--operator COD `# delete conditional operator` \\\\\\n--operator COI `# insert conditional operator` \\\\\\n--operator CRP `# replace constant` \\\\\\n--operator ROR `# replace relational operator` \\\\\\n--show-mutants\\n\\nThe result is going to look something similar to this:\\n[*] Mutation score [0.04649 s]: 100.0%\\n- all: 4\\n- killed: 4 (100.0%)\\n- survived: 0 (0.0%)\\n- incompetent: 0 (0.0%)\\n- timeout: 0 (0.0%)\\n\\nThis is a good sign. Let\\'s take a particular instance to analyze what happened. One of the\\nlines on the output shows the following mutant:\\n- [# 1] ROR mutation_testing_1:11 :\\n-----------------------------------------------------7: from mrstatus import MergeRequestStatus as Status\\n8:\\n9:\\n10: def evaluate_merge_request(upvote_count, downvotes_count):\\n~11:\\nif downvotes_count < 0:\\n12:\\nreturn Status.REJECTED\\n13:\\nif upvote_count >= 2:\\n14:\\nreturn Status.APPROVED\\n15:\\nreturn Status.PENDING\\n-----------------------------------------------------[0.00401 s] killed by test_approved\\n(test_mutation_testing_1.TestMergeRequestEvaluation)\\n\\nNotice that this mutant consists of the original version with the operator changed in line 11\\n(> for <), and the result is telling us that this mutant was killed by the tests. This means that\\nwith this version of the code (let\\'s imagine that someone by mistakes makes this change),\\nthen the result of the function would have been APPROVED, and since the test expects it to\\nbe REJECTED, it fails, which is a good sign (the test caught the bug that was introduced).\\n\\n[ 250 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nMutation testing is a good way to assure the quality of the unit tests, but it requires some\\neffort and careful analysis. By using this tool in complex environments, we will have to take\\nsome time analyzing each scenario. It is also true that it is expensive to run these tests\\nbecause it requires multiples runs of different versions of the code, which might take up too\\nmany resources and may take longer to complete. However, it would be even more\\nexpensive to have to make these checks manually and will require much more effort. Not\\ndoing these checks at all might be even riskier, because we would be jeopardizing the\\nquality of the tests.\\n\\nA brief introduction to test-driven\\ndevelopment\\nThere are entire books dedicated only to TDD, so it would not be realistic to try and cover\\nthis topic comprehensively in this book. However, it\\'s such an important topic that it has to\\nbe mentioned.\\nThe idea behind TDD is that tests should be written before production code in a way that\\nthe production code is only written to respond to tests that are failing due to that missing\\nimplementation of the functionality.\\nThere are multiple reasons why we would like to write the tests first and then the code.\\nFrom a pragmatic point of view, we would be covering our production code quite\\naccurately. Since all of the production code was written to respond to a unit test, it would\\nbe highly unlikely that there are tests missing for functionality (that doesn\\'t mean that there\\nis 100% of coverage of course, but at least all main functions, methods, or components will\\nhave their respective tests, even if they aren\\'t completely covered).\\nThe workflow is simple and at a high-level consist of three steps. First, we write a unit test\\nthat describes something we need to be implemented. When we run this test, it will fail,\\nbecause that functionality has not been implemented yet. Then, we move onto\\nimplementing the minimal required code that satisfies that condition, and we run the test\\nagain. This time, the test should pass. Now, we can improve (refactor) the code.\\nThis cycle has been popularized as the famous red-green-refactor, meaning that in the\\nbeginning, the tests fail (red), then we make them pass (green), and then we proceed to\\nrefactor the code and iterate it.\\n\\n[ 251 ]\\n\\n\\x0cUnit Testing and Refactoring\\n\\nChapter 8\\n\\nSummary\\nUnit testing is a really interesting and deep topic, but more importantly, it is a critical part\\nof the clean code. Ultimately, unit tests are what determine the quality of the code. Unit\\ntests often act as a mirror for the code\\xe2\\x80\\x94when the code is easy to test, it\\'s clear and correctly\\ndesigned, and this will be reflected in the unit tests.\\nThe code for the unit tests is as important as production code. All principles that apply to\\nproduction code also apply to unit tests. This means that they should be designed and\\nmaintained with the same effort and thoughtfulness. If we don\\'t care about our unit tests,\\nthey will start to have problems and become defective (or problematic), and as a result of\\nthat, useless. If this happens, and they are hard to maintain, they become a liability which\\nmakes things even worse, because people will tend to ignore them or disable them entirely.\\nThis is the worst scenario because once this happens, the entire production code is in\\njeopardy. Moving forward blindly (without unit tests) is a recipe for disaster.\\nLuckily, Python provides many tools for unit testing, both in the standard library and\\navailable through pip. They are of great help, and investing a time in configuring them\\nreally pays off in the long run.\\nWe have seen how unit tests work as the formal specification of the program, and the proof\\nthat a piece of software works according to the specification, and we also learned that when\\nit comes to discovering new testing scenarios, there is always room for improvement, and\\nthat we can always create more tests. In this sense, expanding our unit tests with different\\napproaches (like property-based testing or mutation testing) is a good investment.\\n\\nReferences\\nHere is a list of information you can refer to:\\nThe unittest module of the Python standard library contains comprehensive\\ndocumentation on how to start building a test suite (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpython.\\xe2\\x80\\x8borg/\\n3/\\xe2\\x80\\x8blibrary/\\xe2\\x80\\x8bunittest.\\xe2\\x80\\x8bhtml)\\nHypothesis official documentation (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bhypothesis.\\xe2\\x80\\x8breadthedocs.\\xe2\\x80\\x8bio/\\xe2\\x80\\x8ben/\\nlatest/\\xe2\\x80\\x8b)\\npytest official documentation (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bdocs.\\xe2\\x80\\x8bpytest.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8ben/\\xe2\\x80\\x8blatest/\\xe2\\x80\\x8b)\\nThe Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental\\nRevolutionary (CatB), written by Eric S. Raymond (publisher O\\'Reilly Media, 1999)\\n\\n[ 252 ]\\n\\n\\x0c9\\nCommon Design Patterns\\nDesign patterns have been a widespread topic in software engineering since their original\\ninception in the famous Gang of Four (GoF) book, Design Patterns: Elements of Reusable\\nObject-Oriented Software. Design patterns help to solve common problems with abstractions\\nthat work for certain scenarios. When they are implemented properly, the general design of\\nthe solution can benefit from them.\\nIn this chapter we take a look at some of the most common design patterns, but not from\\nthe perspective of tools to apply under certain conditions (once the patterns have been\\ndevised), but rather we analyze how design patterns contribute to clean code. After\\npresenting a solution that implements a design pattern, we analyze how the final\\nimplementation is comparatively better as if we had chosen a different path.\\nAs part of this analysis, we will see how to concretely implement design patterns in Python.\\nAs a result of that, we will see that the dynamic nature of Python implies some differences\\nof implementation, with respect to other static typed languages, for which many of the\\ndesign patterns were originally thought of. This means that there are some particularities\\nabout design patterns that you should bear in mind when it comes to Python, and, in some\\ncases, trying to apply a design pattern where it doesn\\'t really fit is non-Pythonic.\\nIn this chapter, we will cover the following topics:\\nCommon design patterns.\\nDesign patterns that don\\'t apply in Python, and the idiomatic alternative that\\nshould be followed.\\nThe Pythonic way of implementing the most common design patterns.\\nUnderstanding how good abstractions evolve naturally into patterns.\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nConsiderations for design patterns in\\nPython\\nObject-oriented design patterns are ideas of software construction that appear in different\\nscenarios when we deal with models of the problem we\\'re solving. Because they\\'re highlevel ideas, it\\'s hard to think of them as being tied to particular programming languages.\\nThey are instead more general concepts about how objects will interact in the application.\\nOf course, they will have their implementation details, varying from language to language,\\nbut that doesn\\'t form the essence of a design pattern.\\nThat\\'s the theoretical aspect of a design pattern, the fact that it is an abstract idea that\\nexpresses concepts about the layout of the objects in the solution. There are plenty of other\\nbooks and several other resources about object-oriented design, and design patterns in\\nparticular, so in this book, we are going to focus on those implementation details for\\nPython.\\nGiven the nature of Python, some of the classical design patterns aren\\'t actually needed.\\nThat means that Python already supports features that render those patterns invisible.\\nSome argue that they don\\'t exist in Python, but keep in mind that invisible doesn\\'t mean\\nnon-existing. They are there, just embedded in Python itself, so it\\'s likely that we won\\'t\\neven notice them.\\nOthers have a much simpler implementation, again thanks to the dynamic nature of the\\nlanguage, and the rest of them are practically the same as they are in other platforms, with\\nsmall differences.\\nIn any case, the important goal for achieving clean code in Python is knowing what\\npatterns to implement and how. That means recognizing some of the patterns that Python\\nalready abstracts and how we can leverage them. For instance, it would be completely nonPythonic to try to implement the standard definition of the iterator pattern (as we would do\\nin different languages), because (as we have already covered) iteration is deeply embedded\\nin Python, and the fact that we can create objects that will directly work in a for loop\\nmakes this the right way to proceed.\\n\\n[ 254 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nSomething similar happens with some of the creational patterns. Classes are regular objects\\nin Python, and so are functions. As we have seen in several examples so far, they can be\\npassed around, decorated, reassigned, and so on. That means that whatever kind of\\ncustomization we would like to make to our objects, we can most likely do it without\\nneeding any particular setup of factory classes. In addition, there is no special syntax for\\ncreating objects in Python (no new keyword, for example). This is another reason why,\\nmost of the time, a simple function call will just work as a factory.\\nOther patterns are still needed, and we will see how, with some small adaptations, we can\\nmake them more Pythonic, taking full advantage of the features that the language provides\\n(magic methods or the standard library).\\nOut of all the patterns available, not all of them are equally frequent, nor useful, so we will\\nfocus on the main ones, those that we would expect to see the most in our applications, and\\nwe will do so by following a pragmatic approach.\\n\\nDesign patterns in action\\nThe canonical reference in this subject, as written by the GoF, introduces 23 design patterns,\\neach falling under one of the creational, structural, and behavioral categories. There are\\neven more patterns or variations of existing ones, but rather than learning all of these\\npatterns off by heart, we should focus on keeping two things in mind. Some of the patterns\\nare invisible in Python, and we use them probably without even noticing. Secondly, not all\\npatterns are equally common; some of them are tremendously useful, and so they are found\\nvery frequently, while others are for more specific cases.\\nIn this section, we will revisit the most common patterns, those that are most likely to\\nemerge from our design. Note the use of the word emerge here. It is important. We should\\nnot force the application of a design pattern to the solution we are building, but rather\\nevolve, refactor, and improve our solution until a pattern emerges.\\nDesign patterns are therefore not invented but discovered. When a situation that occurs\\nrepeatedly in our code reveals itself, the general and more abstract layout of classes, objects,\\nand related components appears under a name by which we identify a pattern.\\n\\n[ 255 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThinking the same thing, but now backward, we realize that the name of a design pattern\\nwraps up a lot of concepts. This is probably the best thing about design patterns; they\\nprovide a language. Through design patterns, it\\'s easier to communicate design ideas\\neffectively. When two or more software engineers share the same vocabulary, and one of\\nthem mentions builder, the rest of them can immediately think about all the classes, and\\nhow they would be related, what their mechanics would be, and so on, without having to\\nrepeat this explanation all over again.\\nThe reader will notice that the code shown in this chapter is different from the canonical or\\noriginal envisioning of the design pattern in question. There is more than one reason for\\nthis. The first reason is that the examples take a more pragmatic approach, aimed at\\nsolutions for particular scenarios rather than exploring general design theory. The second\\nreason is that the patterns are implemented with the particularities of Python, which in\\nsome cases are very subtle, but in other cases, the differences are noticeable, generally\\nsimplifying the code.\\n\\nCreational patterns\\nIn software engineering, creational patterns are those that deal with object instantiation,\\ntrying to abstract away much of the complexity (like determining the parameters to\\ninitialize an object, all the related objects that might be needed, etc.), in order to leave the\\nuser with a simpler interface, that should be safer to use. The basic form of object creation\\ncould result in design problems or added complexity to the design. Creational design\\npatterns solve this problem by somehow controlling this object creation.\\nOut of the five patterns for creating objects, we will discuss mainly the variants that are\\nused to avoid the singleton pattern, and replace it with the Borg pattern (most commonly\\nused in Python applications), discussing their differences and advantages.\\n\\nFactories\\nAs was mentioned in the introduction, one of the core features of Python is that everything\\nis an object, and as such, they can all be treated equally. This means that there are no special\\ndistinctions of things that we can or cannot do with classes, functions, or custom objects.\\nThey can all be passed by parameter, assigned, and so on.\\nIt is for this reason that many of the factory patterns are not really needed. We could just\\nsimply define a function that will construct a set of objects, and we can even pass the class\\nthat we want to create by a parameter.\\n\\n[ 256 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nSingleton and shared state (monostate)\\nThe singleton pattern, on the other hand, is something not entirely abstracted away by\\nPython. The truth is that most of the time, this pattern is either not really needed or is a bad\\nchoice. There are a lot of problems with singletons (after all, they are, in fact, a form of\\nglobal variables for object-oriented software, and as such, are a bad practice). They are hard\\nto unit test, the fact that they might be modified at any time by any object makes them hard\\nto predict, and their side-effects can be really problematic.\\nAs a general principle, we should avoid using singletons as much as possible. If in some\\nextreme case, they are required, the easiest way of achieving this in Python is by using a\\nmodule. We can create an object in a module, and once it\\'s there, it will be available from\\nevery part of the module that is imported. Python itself makes sure that modules are\\nalready singletons, in the sense that no matter how many times they\\'re imported, and from\\nhow many places, the same module is always the one that is going to be loaded\\ninto sys.modules.\\n\\nShared state\\nRather than forcing our design to have a singleton in which only one instance is created, no\\nmatter how the object is invoked, constructed, or initialized, it is better to replicate the data\\nacross multiple instances.\\nThe idea of the monostate pattern (SNGMONO) is that we can have many instances that are\\njust regular objects, without having to care whether they\\'re singletons or not (seeing as\\nthey\\'re just objects). The good thing about this pattern is that these objects will have their\\ninformation synchronized, in a completely transparent way, without us having to worry\\nabout how this works internally.\\nThis makes this pattern a much better choice, not only for its convenience, but also because\\nit is less error-prone, and suffers from fewer of the disadvantages of singletons (regarding\\ntheir testability, creating derived classes, and so on).\\nWe can use this pattern on many levels, depending on how much information we need to\\nsynchronize.\\nIn its simplest form, we can assume that we only need to have one attribute to be reflected\\nacross all instances. If that is the case, the implementation is as trivial as using a class\\nvariable, and we just need to take care in providing a correct interface to update and\\nretrieve the value of the attribute.\\n\\n[ 257 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nLet\\'s say we have an object that has to pull a version of a code in a Git repository by the\\nlatest tag. There might be multiple instances of this object, and when every client calls the\\nmethod for fetching the code, this object will use the tag version from its attribute. At any\\npoint, this tag can be updated for a newer version, and we want any other instance (new or\\nalready created) to use this new branch when the fetch operation is being called, as shown\\nin the following code:\\nclass GitFetcher:\\n_current_tag = None\\ndef __init__(self, tag):\\nself.current_tag = tag\\n@property\\ndef current_tag(self):\\nif self._current_tag is None:\\nraise AttributeError(\"tag was never set\")\\nreturn self._current_tag\\n@current_tag.setter\\ndef current_tag(self, new_tag):\\nself.__class__._current_tag = new_tag\\ndef pull(self):\\nlogger.info(\"pulling from %s\", self.current_tag)\\nreturn self.current_tag\\n\\nThe reader can simply verify that creating multiple objects of the GitFetcher type with\\ndifferent versions will result in all objects being set with the latest version at any time, as\\nshown in the following code:\\n>>>\\n>>>\\n>>>\\n>>>\\n0.3\\n>>>\\n0.3\\n\\nf1 = GitFetcher(0.1)\\nf2 = GitFetcher(0.2)\\nf1.current_tag = 0.3\\nf2.pull()\\nf1.pull()\\n\\nIn the case that we need more attributes, or that we wish to encapsulate the shared attribute\\na bit more, to make the design cleaner, we can use a descriptor.\\n\\n[ 258 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nA descriptor, like the one shown in the following code, solves the problem, and while it\\'s\\ntrue that it requires more code, it also encapsulates a more concrete responsibility, and part\\nof the code is actually moved away from our original class, making either one of them more\\ncohesive and compliant with the single responsibility principle:\\nclass SharedAttribute:\\ndef __init__(self, initial_value=None):\\nself.value = initial_value\\nself._name = None\\ndef __get__(self, instance, owner):\\nif instance is None:\\nreturn self\\nif self.value is None:\\nraise AttributeError(f\"{self._name} was never set\")\\nreturn self.value\\ndef __set__(self, instance, new_value):\\nself.value = new_value\\ndef __set_name__(self, owner, name):\\nself._name = name\\n\\nApart from these considerations, it\\'s also true that the pattern is now more reusable. If we\\nwant to repeat this logic, we just have to create a new descriptor object that would work\\n(complying with the DRY principle).\\nIf we now want to do the same, but for the current branch, we create this new class\\nattribute, and the rest of the class is kept intact, while still having the desired logic in place,\\nas shown in the following code:\\nclass GitFetcher:\\ncurrent_tag = SharedAttribute()\\ncurrent_branch = SharedAttribute()\\ndef __init__(self, tag, branch=None):\\nself.current_tag = tag\\nself.current_branch = branch\\ndef pull(self):\\nlogger.info(\"pulling from %s\", self.current_tag)\\nreturn self.current_tag\\n\\n[ 259 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThe balance and trade-off of this new approach should be clear by now. This new\\nimplementation uses a bit more code, but it\\'s reusable, so it saves lines of code (and\\nduplicated logic) in the long run. Once again, refer to the three or more instances rule to\\ndecide if you should create such an abstraction.\\nAnother important benefit of this solution is that it also reduces the repetition of unit tests.\\nReusing code here will give us more confidence on the overall quality of the solution,\\nbecause now we just have to write unit tests for the descriptor object, not for all the classes\\nthat use it (we can safely assume that they\\'re correct as long as the unit tests prove the\\ndescriptor to be correct).\\n\\nThe borg pattern\\nThe previous solutions should work for most cases, but if we really have to go for a\\nsingleton (and this has to be a really good exception), then there is one last better\\nalternative to it, only this is a riskier one.\\nThis is the actual monostate pattern, referred to as the borg pattern in Python. The idea is to\\ncreate an object that is capable of replicating all of its attributes among all instances of the\\nsame class. The fact that absolutely every attribute is being replicated has to be a warning to\\nkeep in mind undesired side-effects. Still, this pattern has many advantages over the\\nsingleton.\\nIn this case, we are going to split the previous object into two\\xe2\\x80\\x94one that works over Git\\ntags, and the other over branches. And we are using the code that will make the borg\\npattern work:\\nclass BaseFetcher:\\ndef __init__(self, source):\\nself.source = source\\n\\nclass TagFetcher(BaseFetcher):\\n_attributes = {}\\ndef __init__(self, source):\\nself.__dict__ = self.__class__._attributes\\nsuper().__init__(source)\\ndef pull(self):\\nlogger.info(\"pulling from tag %s\", self.source)\\nreturn f\"Tag = {self.source}\"\\n\\nclass BranchFetcher(BaseFetcher):\\n\\n[ 260 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\n_attributes = {}\\ndef __init__(self, source):\\nself.__dict__ = self.__class__._attributes\\nsuper().__init__(source)\\ndef pull(self):\\nlogger.info(\"pulling from branch %s\", self.source)\\nreturn f\"Branch = {self.source}\"\\n\\nBoth objects have a base class, sharing their initialization method. But then they have to\\nimplement it again in order to make the borg logic work. The idea is that we use a class\\nattribute that is a dictionary to store the attributes, and then we make the dictionary of each\\nobject (at the time it\\'s being initialized) to use this very same dictionary. This means that\\nany update on the dictionary of an object will be reflected in the class, which will be the\\nsame for the rest of the objects because their class is the same, and dictionaries are mutable\\nobjects that are passed as a reference. In other words, when we create new objects of this\\ntype, they will all use the same dictionary, and this dictionary is constantly being updated.\\nNote that we cannot put the logic of the dictionary on the base class, because this will mix\\nthe values among the objects of different classes, which is not what we want. This\\nboilerplate solution is what would make many think it\\'s actually an idiom rather than a\\npattern.\\nA possible way of abstracting this in a way that achieves the DRY principle would be to\\ncreate a mixin class, as shown in the following code:\\nclass SharedAllMixin:\\ndef __init__(self, *args, **kwargs):\\ntry:\\nself.__class__._attributes\\nexcept AttributeError:\\nself.__class__._attributes = {}\\nself.__dict__ = self.__class__._attributes\\nsuper().__init__(*args, **kwargs)\\n\\nclass BaseFetcher:\\ndef __init__(self, source):\\nself.source = source\\n\\nclass TagFetcher(SharedAllMixin, BaseFetcher):\\ndef pull(self):\\nlogger.info(\"pulling from tag %s\", self.source)\\n\\n[ 261 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nreturn f\"Tag = {self.source}\"\\n\\nclass BranchFetcher(SharedAllMixin, BaseFetcher):\\ndef pull(self):\\nlogger.info(\"pulling from branch %s\", self.source)\\nreturn f\"Branch = {self.source}\"\\n\\nThis time, we are using the mixin class to create the dictionary with the attributes in each\\nclass in case it doesn\\'t already exist, and then continuing with the same logic.\\nThis implementation should not have any major problems with inheritance, so it\\'s a more\\nviable alternative.\\n\\nBuilder\\nThe builder pattern is an interesting pattern that abstracts away all the complex\\ninitialization of an object. This pattern does not rely on any particularity of the language, so\\nit\\'s as equally applicable in Python as it would be in any other language.\\nWhile it solves a valid case, it\\'s usually also a complicated case that is more likely to appear\\nin the design of a framework, library, or an API. Similar to the recommendations given for\\ndescriptors, we should reserve this implementation for cases where we expect to expose an\\nAPI that is going to be consumed by multiple users.\\nThe high level idea of this patter is that we need to create a complex object, that is an object\\nthat also requires many others to work with. Rather than letting the user create all those\\nauxiliary objects, and then assign them to the main one, we would like to create an\\nabstraction that allows all of that to be done in a single step. In order to achieve this, we will\\nhave a builder object that knows how to create all the parts and link them together, giving\\nthe user an interface (which could be a class method), to parametrize all the information\\nabout what the resulting object should look like.\\n\\nStructural patterns\\nStructural patterns are useful for situations where we need to create simpler interfaces or\\nobjects that are more powerful by extending their functionality without adding complexity\\nto their interfaces.\\n\\n[ 262 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThe best thing about these patterns is that we can create more interesting objects, with\\nenhanced functionality, and we can achieve this in a clean way; that is, by composing\\nmultiple single objects (the clearest example of this being the composite pattern), or by\\ngathering many simple and cohesive interfaces.\\n\\nAdapter\\nThe adapter pattern is probably one of the simplest design patterns there are, and one of the\\nmost useful ones at the same time. Also known as a wrapper, this pattern solves the\\nproblem of adapting interfaces of two or more objects that are not compatible.\\nWe typically encounter the situation where part of our code works with a model or set of\\nclasses that were polymorphic with respect to a method. For example, if there were\\nmultiple objects for retrieving data with a fetch() method, then we want to maintain this\\ninterface so we don\\'t have to make major changes to our code.\\nBut then we come to a point where the need to add a new data source, and alas, this one\\nwon\\'t have a fetch() method. To make things worse, not only is this type of object not\\ncompatible, but it is also not something we control (perhaps a different team decided on the\\nAPI, and we cannot modify the code).\\nInstead of using this object directly, we adopt its interface to the one we need. There are\\ntwo ways of doing this.\\nThe first way would be to create a class that inherits from the one we want to use, and that\\ncreates an alias for the method (if required, it will also have to adapt the parameters and the\\nsignature).\\nBy means of inheritance, we import the external class and create a new one that will define\\nthe new method, calling the one that has a different name. In this example, let\\'s say the\\nexternal dependency has a method named search(), which takes only one parameter for\\nthe search because it queries in a different fashion, so our adapter method not only calls\\nthe external one, but it also translates the parameters accordingly, as shown in the\\nfollowing code:\\nfrom _adapter_base import UsernameLookup\\n\\nclass UserSource(UsernameLookup):\\ndef fetch(self, user_id, username):\\nuser_namespace = self._adapt_arguments(user_id, username)\\nreturn self.search(user_namespace)\\n\\n[ 263 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\n@staticmethod\\ndef _adapt_arguments(user_id, username):\\nreturn f\"{user_id}:{username}\"\\n\\nIt might be the case that our class already derives from another one, in which case, this will\\nend up as a case of multiple inheritances, which Python supports, so it shouldn\\'t be a\\nproblem. However, as we have seen many times before, inheritance comes with more\\ncoupling (who knows how many other methods are being carried from the external\\nlibrary?), and it\\'s inflexible. Conceptually, it also wouldn\\'t be the right choice because we\\nreserve inheritance for situations of specification (an is a kind of relationship), and in this\\ncase, it\\'s not clear at all that our object has to be one of the kinds that are provided by a\\nthird-party library (especially since we don\\'t fully comprehend that object).\\nTherefore, a better approach would be to use composition instead. Assuming that we can\\nprovide our object with an instance of UsernameLookup, the code would be as simple as\\njust redirecting the petition prior to adopting the parameters, as shown in the following\\ncode:\\nclass UserSource:\\n...\\ndef fetch(self, user_id, username):\\nuser_namespace = self._adapt_arguments(user_id, username)\\nreturn self.username_lookup.search(user_namespace)\\n\\nIf we need to adopt multiple methods, and we can devise a generic way of adapting their\\nsignature as well, it might be worth using the __getattr__() magic method to redirect\\nrequests towards the wrapped object, but as always with generic implementations, we\\nshould be careful of not adding more complexity to the solution.\\n\\nComposite\\nThere will be parts of our programs that require us to work with objects that are made out\\nof other objects. We have base objects that have a well-defined logic, and then we will have\\nother container objects that will group a bunch of base objects, and the challenge is that we\\nwant to treat both of them (the base and the container objects) without noticing any\\ndifferences.\\nThe objects are structured in a tree hierarchy, where the basic objects would be the leaves of\\nthe tree, and the composed objects intermediate nodes. A client might want to call any of\\nthem to get the result of a method that is called. The composite object, however, will act as a\\nclient; this also will pass this request along with all the objects it contains whether they are\\nleaves or other intermediate notes until they all are processed.\\n\\n[ 264 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nImagine a simplified version of an online store in which we have products. Say that we\\noffer the possibility of grouping those products, and we give customers a discount per\\ngroup of products. A product has a price, and this value will be asked for when the\\ncustomers come to pay. But a set of grouped products also has a price that has to be\\ncomputed. We will have an object that represents this group that contains the products, and\\nthat delegates the responsibility of asking the price to each particular product (which might\\nbe another group of products as well), and so on, until there is nothing else to compute. The\\nimplementation of this is shown in the following code:\\nclass Product:\\ndef __init__(self, name, price):\\nself._name = name\\nself._price = price\\n@property\\ndef price(self):\\nreturn self._price\\n\\nclass ProductBundle:\\ndef __init__(\\nself,\\nname,\\nperc_discount,\\n*products: Iterable[Union[Product, \"ProductBundle\"]]\\n) -> None:\\nself._name = name\\nself._perc_discount = perc_discount\\nself._products = products\\n@property\\ndef price(self):\\ntotal = sum(p.price for p in self._products)\\nreturn total * (1 - self._perc_discount)\\n\\nWe expose the public interface through a property, and leave the price as a private\\nattribute. The ProductBundle class uses this property to compute the value with the\\ndiscount applied by first adding all the prices of all the products it contains.\\nThe only discrepancy between these objects is that they are created with different\\nparameters. To be fully compatible, we should have tried to mimic the same interface and\\nthen added extra methods for adding products to the bundle but using an interface that\\nallows the creation of complete objects. Not needing these extra steps is an advantage that\\njustifies this small difference.\\n\\n[ 265 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nDecorator\\nDon\\'t confuse the decorator pattern with the concept of a Python decorator which we have\\ngone through in Chapter 5, Using Decorators to Improve Our Code. There is some\\nresemblance, but the idea of the design pattern is quite different.\\nThis pattern allows us to dynamically extend the functionality of some objects, without\\nneeding inheritance. It\\'s a good alternative to multiple inheritance in creating more flexible\\nobjects.\\nWe are going to create a structure that let\\'s a user define a set of operations (decorations) to\\nbe applied over an object, and we\\'ll see how each step takes place in the specified order.\\nThe following code example is a simplified version of an object that constructs a query in\\nthe form of a dictionary from parameters that are passed to it (it might be an object that we\\nwould use for running queries to elasticsearch, for instance, but the code leaves out\\ndistracting implementation details to focus on the concepts of the pattern).\\nIn its most basic form, the query just returns the dictionary with the data it was provided\\nwhen it was created. Clients expect to use the render() method of this object:\\nclass DictQuery:\\ndef __init__(self, **kwargs):\\nself._raw_query = kwargs\\ndef render(self) -> dict:\\nreturn self._raw_query\\n\\nNow we want to render the query in different ways by applying transformations to the\\ndata (filtering values, normalizing them, and so on). We could create decorators and apply\\nthem to the render method, but that wouldn\\'t be flexible enough what if we want to\\nchange them at runtime? Or if we want to select some of them, but not others?\\nThe design is to create another object, with the same interface and the capability of\\nenhancing (decorating) the original result through many steps, but which can be combined.\\nThese objects are chained, and each one of them does what it was originally supposed to\\ndo, plus something else. This something else is the particular decoration step.\\nSince Python has duck typing, we don\\'t need to create a new base class and make these new\\nobjects part of that hierarchy, along with DictQuery. Simply creating a new class that has\\na render() method will be enough (again, polymorphism should not require inheritance).\\nThis process is shown in the following code:\\nclass QueryEnhancer:\\ndef __init__(self, query: DictQuery):\\n\\n[ 266 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nself.decorated = query\\ndef render(self):\\nreturn self.decorated.render()\\n\\nclass RemoveEmpty(QueryEnhancer):\\ndef render(self):\\noriginal = super().render()\\nreturn {k: v for k, v in original.items() if v}\\n\\nclass CaseInsensitive(QueryEnhancer):\\ndef render(self):\\noriginal = super().render()\\nreturn {k: v.lower() for k, v in original.items()}\\n\\nThe QueryEnhancer phrase has an interface that is compatible with what the clients\\nof DictQuery are expecting, so they are interchangeable. This object is designed to receive\\na decorated one. It\\'s going to take the values from this and convert them, returning the\\nmodified version of the code.\\nIf we want to remove all values that evaluate to False and normalize them to form our\\noriginal query, we would have to use the following schema:\\n>>> original = DictQuery(key=\"value\", empty=\"\", none=None,\\nupper=\"UPPERCASE\", title=\"Title\")\\n>>> new_query = CaseInsensitive(RemoveEmpty(original))\\n>>> original.render()\\n{\\'key\\': \\'value\\', \\'empty\\': \\'\\', \\'none\\': None, \\'upper\\': \\'UPPERCASE\\', \\'title\\':\\n\\'Title\\'}\\n>>> new_query.render()\\n{\\'key\\': \\'value\\', \\'upper\\': \\'uppercase\\', \\'title\\': \\'title\\'}\\n\\nThis is a pattern that we can also implement in different ways, taking advantage of the\\ndynamic nature of Python, and the fact that functions are objects. We could implement this\\npattern with functions that are provided to the base decorator object (QueryEnhancer),\\nand define each decoration step as a function, as shown in the following code:\\nclass QueryEnhancer:\\ndef __init__(\\nself,\\nquery: DictQuery,\\n*decorators: Iterable[Callable[[Dict[str, str]], Dict[str, str]]]\\n) -> None:\\nself._decorated = query\\nself._decorators = decorators\\n\\n[ 267 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\ndef render(self):\\ncurrent_result = self._decorated.render()\\nfor deco in self._decorators:\\ncurrent_result = deco(current_result)\\nreturn current_result\\n\\nWith respect to the client, nothing has changed because this class maintains the\\ncompatibility through its render() method. Internally, however, this object is used in a\\nslightly different fashion, as shown in the following code:\\n>>> query = DictQuery(foo=\"bar\", empty=\"\", none=None, upper=\"UPPERCASE\",\\ntitle=\"Title\")\\n>>> QueryEnhancer(query, remove_empty, case_insensitive).render()\\n{\\'foo\\': \\'bar\\', \\'upper\\': \\'uppercase\\', \\'title\\': \\'title\\'}\\n\\nIn the preceding code, remove_empty and case_insensitive are just regular functions\\nthat transform a dictionary.\\nIn this example, the function-based approach seems easier to understand. There might be\\ncases with more complex rules that rely on data from the object being decorated (not only\\nits result), and in those cases, it might be worth going for the object-oriented approach,\\nespecially if we really want to create a hierarchy of objects where each class actually\\nrepresents some knowledge we want to make explicit in our design.\\n\\nFacade\\nFacade is an excellent pattern. It\\'s useful in many situations where we want to simplify the\\ninteraction between objects. The pattern is applied where there is a relation of many-tomany among several objects, and we want them to interact. Instead of creating all of these\\nconnections, we place an intermediate object in front of many of them that act as a facade.\\nThe facade works as a hub or a single point of reference in this layout. Every time a new\\nobject wants to connect to another one, instead of having to have N interfaces for all N\\npossible objects it needs to connect to, it will instead just talk to the facade, and this will\\nredirect the request accordingly. Everything that\\'s behind the facade is completely opaque\\nto the rest of the external objects.\\nApart from the main and obvious benefit (the decoupling of objects), this pattern also\\nencourages a simpler design with fewer interfaces and better encapsulation.\\n\\n[ 268 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThis is a pattern that we can use not only for improving the code of our domain problem\\nbut also to create better APIs. If we use this pattern and provide a single interface, acting as\\na single point of truth or entry point for our code, it will be much easier for our users to\\ninteract with the functionality exposed. Not only that, but by exposing a functionality and\\nhiding everything behind an interface, we are free of changing or refactoring that\\nunderlying code as many times as we want, because as long as it is behind the facade, it\\nwill not break backward compatibility, and our users will not be affected.\\nNote how this idea of using facades is not even limited to objects and classes, but also\\napplies to packages (technically, packages are objects in Python, but still). We can use this\\nidea of the facade to decide the layout of a package; that is, what is visible to the user and\\nimportable, and what is internal and should not be imported directly.\\nWhen we create a directory to build a package, we place the __init__.py file along with\\nthe rest of the files. This is the root of the module, a sort of facade. The rest of the files\\ndefine the objects to export, but they shouldn\\'t be directly imported by clients. The init file\\nshould import them and then clients should get them from there. This creates a better\\ninterface because users only need to know a single entry point from which to get the\\nobjects, and more importantly, the package (the rest of the files) can be refactored or\\nrearranged as many times as needed, and this will not affect clients as long as the main API\\non the init file is maintained. It is of utmost importance to keep principles like this one in\\nmind in order to build maintainable software.\\nThere is an example of this in Python itself, with the os module. This module groups an\\noperating system\\'s functionality, but underneath it, uses the posix module for Portable\\nOperating System Interface (POSIX) operating systems (this is called nt in Windows\\nplatforms). The idea is that, for portability reasons, we shouldn\\'t ever really import\\nthe posix module directly, but always the os module. It is up to this module to determine\\nfrom which platform it is being called, and expose the corresponding functionality.\\n\\nBehavioral patterns\\nBehavioral patterns aim to solve the problem of how objects should cooperate, how they\\nshould communicate, and what their interfaces should be at run-time.\\nWe discuss mainly the following behavioral patterns:\\nChain of responsibility\\nTemplate method\\nCommand\\nState\\n\\n[ 269 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThis can be accomplished statically by means of inheritance or dynamically by using\\ncomposition. Regardless of what the pattern uses, what we will see throughout the\\nfollowing examples is that what these patterns have in common is the fact that the resulting\\ncode is better in some significant way, whether this is because it avoids duplication or\\ncreates good abstractions that encapsulate behavior accordingly and decouple our models.\\n\\nChain of responsibility\\nNow we are going to take another look at our event systems. We want to parse information\\nabout the events that happened on the system from the log lines (text files, dumped from\\nour HTTP application server, for example), and we want to extract this information in a\\nconvenient way.\\nIn our previous implementation, we achieved an interesting solution that was compliant\\nwith the open/closed principle and relied on the use of the __subclasses__() magic\\nmethod to discover all possible event types and process the data with the right event,\\nresolving the responsibility through a method encapsulated on each class.\\nThis solution worked for our purposes, and it was quite extensible, but as we\\'ll see, this\\ndesign pattern will bring additional benefits.\\nThe idea here is that we are going to create the events in a slightly different way. Each event\\nstill has the logic to determine whether or not it can process a particular log line, but it will\\nalso have a successor. This successor is a new event, the next one in the line, that will\\ncontinue processing the text line in case the first one was not able to do so. The logic is\\nsimple\\xe2\\x80\\x94we chain the events, and each one of them tries to process the data. If it can, then it\\njust returns the result. If it can\\'t, it will pass it to its successor and repeat, as shown in the\\nfollowing code:\\nimport re\\nclass Event:\\npattern = None\\ndef __init__(self, next_event=None):\\nself.successor = next_event\\ndef process(self, logline: str):\\nif self.can_process(logline):\\nreturn self._process(logline)\\nif self.successor is not None:\\nreturn self.successor.process(logline)\\n\\n[ 270 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\ndef _process(self, logline: str) -> dict:\\nparsed_data = self._parse_data(logline)\\nreturn {\\n\"type\": self.__class__.__name__,\\n\"id\": parsed_data[\"id\"],\\n\"value\": parsed_data[\"value\"],\\n}\\n@classmethod\\ndef can_process(cls, logline: str) -> bool:\\nreturn cls.pattern.match(logline) is not None\\n@classmethod\\ndef _parse_data(cls, logline: str) -> dict:\\nreturn cls.pattern.match(logline).groupdict()\\n\\nclass LoginEvent(Event):\\npattern = re.compile(r\"(?P<id>\\\\d+):\\\\s+login\\\\s+(?P<value>\\\\S+)\")\\nclass LogoutEvent(Event):\\npattern = re.compile(r\"(?P<id>\\\\d+):\\\\s+logout\\\\s+(?P<value>\\\\S+)\")\\n\\nWith this implementation, we create the event objects, and arrange them in the particular\\norder in which they are going to be processed. Since they all have a process() method,\\nthey are polymorphic for this message, so the order in which they are aligned is completely\\ntransparent to the client, and either one of them would be transparent too. Not only that,\\nbut the process() method has the same logic; it tries to extract the information if the data\\nprovided is correct for the type of object handling it, and if not, it moves on to the next one\\nin the line.\\nThis way, we could process a login event in the following way:\\n>>> chain = LogoutEvent(LoginEvent())\\n>>> chain.process(\"567: login User\")\\n{\\'type\\': \\'LoginEvent\\', \\'id\\': \\'567\\', \\'value\\': \\'User\\'}\\n\\nNote how LogoutEvent received LoginEvent as its successor, and when it was asked to\\nprocess something that it couldn\\'t handle, it redirected to the correct object. As we can see\\nfrom the type key on the dictionary, LoginEvent was the one that actually created that\\ndictionary.\\nThis solution is flexible enough, and shares an interesting trait with our previous one\\xe2\\x80\\x94all\\nconditions are mutually exclusive. As long as there are no collisions, and no piece of data\\nhas more than one handler, processing the events in any order will not be an issue.\\n\\n[ 271 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nBut what if we cannot make such an assumption? With the previous implementation, we\\ncould still change the __subclasses__() call for a list that we made according to our\\ncriteria, and that would have worked just fine. And what if we wanted that order of\\nprecedence to be determined at runtime (by the user or client, for example)? That would be\\na shortcoming.\\nWith the new solution, it\\'s possible to accomplish such requirements, because we assemble\\nthe chain at runtime, so we can manipulate it dynamically as we need to.\\nFor example, now we add a generic type that groups both the login and logout a session\\nevent, as shown in the following code:\\nclass SessionEvent(Event):\\npattern = re.compile(r\"(?P<id>\\\\d+):\\\\s+log(in|out)\\\\s+(?P<value>\\\\S+)\")\\n\\nIf for some reason, and in some part of the application, we want to capture this before the\\nlogin event, this can be done by the following chain:\\nchain = SessionEvent(LoginEvent(LogoutEvent()))\\n\\nBy changing the order, we can, for instance, say that a generic session event has a higher\\npriority than the login, but not the logout, and so on.\\nThe fact that this pattern works with objects makes it more flexible with respect to our\\nprevious implementation, which relied on classes (and while they are still objects in Python,\\nthey aren\\'t excluded from some degree of rigidity).\\n\\nThe template method\\nThe template method is a pattern that yields important benefits when implemented\\nproperly. Mainly, it allows us to reuse code, and it also makes our objects more flexible and\\neasy to change while preserving polymorphism.\\nThe idea is that there is a class hierarchy that defines some behavior, let\\'s say an important\\nmethod of its public interface. All of the classes of the hierarchy share a common template\\nand might need to change only certain elements of it. The idea, then, is to place this generic\\nlogic in the public method of the parent class that will internally call all other (private)\\nmethods, and these methods are the ones that the derived classes are going to modify;\\ntherefore, all the logic in the template is reused.\\n\\n[ 272 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nAvid readers might have noticed that we already implemented this pattern in the previous\\nsection (as part of the chain of responsibility example). Note that the classes derived\\nfrom Event implement only one thing their particular pattern. For the rest of the logic, the\\ntemplate is in the Event class. The process event is generic, and relies on two auxiliary\\nmethods can_process() and process() (which in turn calls _parse_data()).\\nThese extra methods rely on a class attribute pattern. Therefore, in order to extend this with\\na new type of object, we just have to create a new derived class and place the regular\\nexpression. After that, the rest of the logic will be inherited with this new attribute changed.\\nThis reuses a lot of code because the logic for processing the log lines is defined once and\\nonly once in the parent class.\\nThis makes the design flexible because preserving the polymorphism is also easily\\nachievable. If we need a new event type that for some reason needs a different way of\\nparsing data, we only override this private method in that subclass, and the compatibility\\nwill be kept, as long as it returns something of the same type as the original one (complying\\nwith Liskov\\'s substitution and open/closed principles). This is because it is the parent class\\nthat is calling the method from the derived classes.\\nThis pattern is also useful if we are designing our own library or framework. By arranging\\nthe logic this way, we give users the ability to change the behavior of one of the classes\\nquite easily. They would have to create a subclass and override the particular private\\nmethod, and the result will be a new object with the new behavior that is guaranteed to be\\ncompatible with previous callers of the original object.\\n\\nCommand\\nThe command pattern provides us with the ability to separate an action that needs to be\\ndone from the moment that it is requested to its actual execution. More than that, it can also\\nseparate the original request issued by a client from its recipient, which might be a different\\nobject. In this section, we are going to focus mainly on the first aspect of the patterns; the\\nfact that we can separate how an order has to be run from when it actually executes.\\nWe know we can create callable objects by implementing the __call__() magic method,\\nso we could just initialize the object and then call it later on. In fact, if this is the only\\nrequirement, we might even achieve this through a nested function that, by means of a\\nclosure, creates another function to achieve the effect of a delayed execution. But this\\npattern can be extended to ends that aren\\'t so easily achievable.\\n\\n[ 273 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThe idea is that the command might also be modified after its definition. This means that\\nthe client specifies a command to run, and then some of its parameters might be changed,\\nmore options added, and so on, until someone finally decides to perform the action.\\nExamples of this can be found in libraries that interact with databases. For instance,\\nin psycopg2 (a PostgreSQL client library), we establish a connection. From this, we get a\\ncursor, and to that cursor we can pass an SQL statement to run. When we call the execute\\nmethod, the internal representation of the object changes, but nothing is actually run in the\\ndatabase. It is when we call fetchall() (or a similar method) that the data is actually\\nqueried and is available in the cursor.\\nThe same happens in the popular Object Relational Mapper SQLAlchemy\\n(ORM SQLAlchemy). A query is defined through several steps, and once we have the\\nquery object, we can still interact with it (add or remove filters, change the conditions,\\napply for an order, and so on), until we decide we want the results of the query. After\\ncalling each method, the query object changes its internal properties and returns self\\n(itself).\\nThese are examples that resemble the behavior that we would like to achieve. A very\\nsimple way of creating this structure would be to have an object that stores the parameters\\nof the commands that are to be run. After that, it has to also provide methods for\\ninteracting with those parameters (adding or removing filters, and so on). Optionally, we\\ncan add tracing or logging capabilities to that object to audit the operations that have been\\ntaking place. Finally, we need to provide a method that will actually perform the action.\\nThis one can be just __call__() or a custom one. Let\\'s call it do().\\n\\nState\\nThe state pattern is a clear example of reification in software design, making the concept of\\nour domain problem an explicit object rather than just a side value.\\nIn Chapter 8, Unit Testing and Refactoring, we had an object that represented a merge\\nrequest, and it had a state associated with it (open, closed, and so on). We used an enum to\\nrepresent those states because, at that point, they were just data holding a value the string\\nrepresentation of that particular state. If they had to have some behavior, or the entire\\nmerge request had to perform some actions depending on its state and transitions, this\\nwould not have been enough.\\n\\n[ 274 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThe fact that we are adding behavior, a runtime structure, to a part of the code has to make\\nus think in terms of objects, because that\\'s what objects are supposed to do, after all. And\\nhere comes the reification\\xe2\\x80\\x94now the state cannot just simply be an enumeration with a\\nstring; it needs to be an object.\\nImagine that we have to add some rules to the merge request say, that when it moves from\\nopen to closed, all approvals are removed (they will have to review the code again)\\xe2\\x80\\x94and\\nthat when a merge request is just opened, the number of approvals is set to zero (regardless\\nof whether it\\'s a reopened or a brand new merge request). Another rule could be that when\\na merge request is merged, we want to delete the source branch, and of course, we want to\\nforbid users from performing invalid transitions (for example, a closed merge request\\ncannot be merged, and so on).\\nIf we were to put all that logic into a single place, namely in the MergeRequest class, we\\nwill end up with a class that has lots of responsibilities (a poor design), probably many\\nmethods, and a very large number of if statements. It would be hard to follow the code\\nand to understand which part is supposed to represent which business rule.\\nIt\\'s better to distribute this into smaller objects, each one with fewer responsibilities, and the\\nstate objects are a good place for this. We create an object for each kind of state we want to\\nrepresent, and, in their methods, we place the logic for the transitions with the\\naforementioned rules. The MergeRequest object will then have a state collaborator, and\\nthis, in turn, will also know about MergeRequest (the double-dispatching mechanism is\\nneeded to run the appropriate actions on MergeRequest and handle the transitions).\\nWe define a base abstract class with the set of methods to be implemented, and then a\\nsubclass for each particular state we want to represent. Then the MergeRequest object\\ndelegates all the actions to state, as shown in the following code:\\nclass InvalidTransitionError(Exception):\\n\"\"\"Raised when trying to move to a target state from an unreachable\\nsource\\nstate.\\n\"\"\"\\n\\nclass MergeRequestState(abc.ABC):\\ndef __init__(self, merge_request):\\nself._merge_request = merge_request\\n@abc.abstractmethod\\ndef open(self):\\n...\\n\\n[ 275 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\n@abc.abstractmethod\\ndef close(self):\\n...\\n@abc.abstractmethod\\ndef merge(self):\\n...\\ndef __str__(self):\\nreturn self.__class__.__name__\\n\\nclass Open(MergeRequestState):\\ndef open(self):\\nself._merge_request.approvals = 0\\ndef close(self):\\nself._merge_request.approvals = 0\\nself._merge_request.state = Closed\\ndef merge(self):\\nlogger.info(\"merging %s\", self._merge_request)\\nlogger.info(\"deleting branch %s\",\\nself._merge_request.source_branch)\\nself._merge_request.state = Merged\\n\\nclass Closed(MergeRequestState):\\ndef open(self):\\nlogger.info(\"reopening closed merge request %s\",\\nself._merge_request)\\nself._merge_request.state = Open\\ndef close(self):\\npass\\ndef merge(self):\\nraise InvalidTransitionError(\"can\\'t merge a closed request\")\\n\\nclass Merged(MergeRequestState):\\ndef open(self):\\nraise InvalidTransitionError(\"already merged request\")\\ndef close(self):\\nraise InvalidTransitionError(\"already merged request\")\\ndef merge(self):\\n\\n[ 276 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\npass\\n\\nclass MergeRequest:\\ndef __init__(self, source_branch: str, target_branch: str) -> None:\\nself.source_branch = source_branch\\nself.target_branch = target_branch\\nself._state = None\\nself.approvals = 0\\nself.state = Open\\n@property\\ndef state(self):\\nreturn self._state\\n@state.setter\\ndef state(self, new_state_cls):\\nself._state = new_state_cls(self)\\ndef open(self):\\nreturn self.state.open()\\ndef close(self):\\nreturn self.state.close()\\ndef merge(self):\\nreturn self.state.merge()\\ndef __str__(self):\\nreturn f\"{self.target_branch}:{self.source_branch}\"\\n\\nThe following list outlines some clarifications about implementation details and the design\\ndecisions that should be made:\\nThe state is a property, so not only is it public, but there is a single place with the\\ndefinitions of how states are created for a merge request, passing self as a\\nparameter.\\nThe abstract base class is not strictly needed, but there are benefits to having it.\\nFirst, it makes the kind of object we are dealing with more explicit. Second, it\\nforces every substate to implement all the methods of the interface. There are two\\nalternatives to this:\\nWe could have not put the methods, and let AttributeError raise when\\ntrying to perform an invalid action, but this is not correct, and it doesn\\'t\\nexpress what happened.\\n\\n[ 277 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nRelated to this point is the fact that we could have just used a simple base\\nclass and left those methods empty, but then the default behavior of not\\ndoing anything doesn\\'t make it any clearer what should happen. If one of\\nthe methods in the subclass should do nothing (as in the case of merge),\\nthen it\\'s better to let the empty method just sit there and make it explicit that\\nfor that particular case, nothing should be done, as opposed to force that\\nlogic to all objects.\\nMergeRequest and MergeRequestState have links to each other. The moment\\n\\na transition is made, the former object will not have extra references and should\\nbe garbage-collected, so this relationship should be always 1:1. With some small\\nand more detailed considerations, a weak reference might be used.\\nThe following code shows some examples of how the object is used:\\n>>> mr = MergeRequest(\"develop\", \"master\")\\n>>> mr.open()\\n>>> mr.approvals\\n0\\n>>> mr.approvals = 3\\n>>> mr.close()\\n>>> mr.approvals\\n0\\n>>> mr.open()\\nINFO:log:reopening closed merge request master:develop\\n>>> mr.merge()\\nINFO:log:merging master:develop\\nINFO:log:deleting branch develop\\n>>> mr.close()\\nTraceback (most recent call last):\\n...\\nInvalidTransitionError: already merged request\\n\\nThe actions for transitioning states are delegated to the state object, which MergeRequest\\nholds at all times (this can be any of the subclasses of ABC). They all know how to respond\\nto the same messages (in different ways), so these objects will take the appropriate actions\\ncorresponding to each transition (deleting branches, raising exceptions, and so on), and\\nwill then move MergeRequest to the next state.\\nSince MergeRequest delegates all actions to its state object, we will find that this\\ntypically happens every time the actions that it needs to do are in the form\\nself.state.open(), and so on. Can we remove some of that boilerplate?\\n\\n[ 278 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nWe could, by means of __getattr__(), as it is portrayed in the following code:\\nclass MergeRequest:\\ndef __init__(self, source_branch: str, target_branch: str) -> None:\\nself.source_branch = source_branch\\nself.target_branch = target_branch\\nself._state: MergeRequestState\\nself.approvals = 0\\nself.state = Open\\n@property\\ndef state(self):\\nreturn self._state\\n@state.setter\\ndef state(self, new_state_cls):\\nself._state = new_state_cls(self)\\n@property\\ndef status(self):\\nreturn str(self.state)\\ndef __getattr__(self, method):\\nreturn getattr(self.state, method)\\ndef __str__(self):\\nreturn f\"{self.target_branch}:{self.source_branch}\"\\n\\nOn the one hand, it is good that we reuse some code and remove repetitive lines. This gives\\nthe abstract base class even more sense. Somewhere, we want to have all possible actions\\ndocumented, listed in a single place. That place used to be the MergeRequest class, but\\nnow those methods are gone, so the only remaining source of that truth is\\nin MergeRequestState. Luckily, the type annotation on the state attribute is really\\nhelpful for users to know where to look for the interface definition.\\nA user can simply take a look and see that everything that MergeRequest doesn\\'t have will\\nbe asked of its state attribute. From the init definition, the annotation will tell us that\\nthis is an object of the MergeRequestState type, and by looking at this interface, we will\\nsee that we can safely ask for the open(), close(), and merge() methods on it.\\n\\n[ 279 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nThe null object pattern\\nThe null object pattern is an idea that relates to the good practices that were mentioned in\\nprevious chapters of this book. Here, we are formalizing them, and giving more context\\nand analysis to this idea.\\nThe principle is rather simple\\xe2\\x80\\x94functions or methods must return objects of a consistent\\ntype. If this is guaranteed, then clients of our code can use the objects that are returned with\\npolymorphism, without having to run extra checks on them.\\nIn the previous examples, we explored how the dynamic nature of Python made things\\neasier for most design patterns. In some cases, they disappear entirely, and in others, they\\nare much easier to implement. The main goal of design patterns as they were originally\\nthought of is that methods or functions should not explicitly name the class of the object\\nthat they need in order to work. For this reason, they propose the creation of interfaces and\\na way of rearranging the objects to make them fit these interfaces in order to modify the\\ndesign. But most of the time, this is not needed in Python, and we can just pass different\\nobjects, and as long as they respect the methods they must have, then the solution will\\nwork.\\nOn the other hand, the fact that objects don\\'t necessarily have to comply with an interface\\nrequires us to be more careful as to the things that are returning from such methods and\\nfunctions. In the same way that our functions didn\\'t make any assumptions about what\\nthey were receiving, it\\'s fair to assume that clients of our code will not make any\\nassumptions either (it is our responsibility to provide objects that are compatible). This can\\nbe enforced or validated with design by contract. Here, we will explore a simple pattern\\nthat will help us avoid these kinds of problems.\\nConsider the chain or responsibility design pattern explored in the previous section. We\\nsaw how flexible it is and its many advantages, such as decoupling responsibilities into\\nsmaller objects. One of the problems it has is that we never actually know what object will\\nend up processing the message, if any. In particular, in our example, if there was no\\nsuitable object to process the log line, then the method would simply return None.\\nWe don\\'t know how users will use the data we passed, but we do know that they are\\nexpecting a dictionary. Therefore, the following error might occur:\\nAttributeError: \\'NoneType\\' object has no attribute \\'keys\\'\\n\\nIn this case, the fix is rather simple\\xe2\\x80\\x94the default value of the process() method should be\\nan empty dictionary rather than None.\\nEnsure that you return objects of a consistent type.\\n\\n[ 280 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nBut what if the method didn\\'t return a dictionary, but a custom object of our domain?\\nTo solve this problem, we should have a class that represents the empty state for that object\\nand return it. If we have a class that represents users in our system, and a function that\\nqueries users by their ID, then in the case that a user is not found, it should do one of the\\nfollowing two things:\\nRaise an exception\\nReturn an object of the UserUnknown type\\nBut in no case should it return None. The phrase None doesn\\'t represent what just\\nhappened, and the caller might legitimately try to ask methods to it, and it will fail with\\nAttributeError.\\nWe have discussed exceptions and their pros and cons earlier on, so we should mention\\nthat this null object should just have the same methods as the original user and do nothing\\nfor each one of them.\\nThe advantage of using this structure is that not only are we avoiding an error at runtime\\nbut also that this object might be useful. It could make the code easier to test, and it can\\neven, for instance, help in debugging (maybe we could put logging into the methods to\\nunderstand why that state was reached, what data was provided to it, and so on).\\nBy exploiting almost all of the magic methods of Python, it would be possible to create a\\ngeneric null object that does absolutely nothing, no matter how it is called, but which can\\nbe called from almost any client. Such an object would slightly resemble a Mock object. It is\\nnot advisable to go down that path because of the following reasons:\\nIt loses meaning with the domain problem. Back in our example, having an object\\nof the UnknownUser type makes sense, and gives the caller a clear idea that\\nsomething went wrong with the query.\\nIt doesn\\'t respect the original interface. This is problematic. Remember that the\\npoint is that an UnknownUser is a user, and therefore it must have the same\\nmethods. If the caller accidentally asks for a method that is not there, then, in that\\ncase, it should raise an AttributeError exception, and that would be good.\\nWith the generic null object that can do anything and respond to anything, we\\nwould be losing this information, and bugs might creep in. If we opt for creating\\na Mock object with spec=User, then this anomaly would be caught, but again,\\nusing a Mock object to represent what is actually an empty state harms\\nthe intention revealing the degree of the code.\\nThis pattern is a good practice that allows us to maintain polymorphism in our objects.\\n\\n[ 281 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nFinal thoughts about design patterns\\nWe have seen the world of design patterns in Python, and in doing so, we have found\\nsolutions to common problems, as well as more techniques that will help us achieve a clean\\ndesign.\\nAll of this sounds good, but it begs the question, how good are design patterns? Some\\npeople argue that they do more harm than good, that they were created for languages\\nwhose limited type system (and lack of first-class functions) makes it impossible to\\naccomplish things we would normally do in Python. Others claim that design patterns\\nforce a design solution, creating some bias that limits a design that would have otherwise\\nemerged, and which would have been better. Let\\'s look at each of these points in turn.\\n\\nThe influence of patterns over the design\\nA design patterns, as with any other topic in software engineering, cannot be good or bad\\nin and of itself, but rather in how it\\'s implemented. In some cases, there is actually no need\\nfor a design pattern, and a simpler solution would do. Trying to force a pattern where it\\ndoesn\\'t fit is a case of over-engineering, and that\\'s clearly bad, but it doesn\\'t mean that there\\nis a problem with the design patterns, and most likely in these scenarios, the problem is not\\neven related to patterns at all. Some people try to over-engineer everything because they\\ndon\\'t understand what flexible and adaptable software really means. As we mentioned\\nbefore in this book, making good software is not about anticipating future requirements\\n(there is no point in doing futurology), but just solving the problem that we have at\\nhand right now, in a way that doesn\\'t prevent us from making changes to it in the future. It\\ndoesn\\'t have to handle those changes now; it just needs to be flexible enough so that it can\\nbe modified in the future. And when that future comes, we will still have to remember the\\nrule of three or more instances of the same problem before coming up with a generic\\nsolution or a proper abstraction.\\nThis is typically the point where the design patterns should emerge, once we have\\nidentified the problem correctly and are able to recognize the pattern and abstract\\naccordingly.\\n\\n[ 282 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nLet\\'s come back to the topic of the suitability of the patterns to the language. As we said in\\nthe introduction of the chapter, design patterns are high-level ideas. They typically refer to\\nthe relation of objects and their interactions. It\\'s hard to think that such things might\\ndisappear from one language to another. It\\'s true that some patterns are actually\\nimplemented manually in Python, as is the case of the iterator pattern (which, as it was\\nheavily discussed earlier in the book, is built in Python), or a strategy (because, instead, we\\nwould just pass functions as any other regular object; we don\\'t need to encapsulate the\\nstrategy method into an object the function itself would be an object).\\nBut other patterns are actually needed, and they indeed solve problems, as in the case of the\\ndecorator and composite patterns. In other cases, there are design patterns that Python\\nitself implements, and we just don\\'t always see them, as in the case of the facade pattern\\nthat we discussed in the section on os.\\nAs to our design patterns leading our solution in a wrong direction, we have to be careful\\nhere. Once again, it\\'s better if we start designing our solution by thinking in terms of the\\ndomain problem and creating the right abstractions, and then later see whether there is a\\ndesign pattern that emerges from that design. Let\\'s say that it does. Is that a bad thing? The\\nfact that there is already a solution to the problem we\\'re trying to solve cannot be a bad\\nthing. It would be bad to reinvent the wheel, as happens many times in our field. Moreover,\\nthe fact that we are applying a pattern, something already proven and validated, should\\ngive us greater confidence in the quality of what we are building.\\n\\nNames in our models\\nShould we mention that we are using a design pattern in our code?\\nIf the design is good and the code is clean, it should speak for itself. It is not recommended\\nthat you name things after the design patterns you are using for a couple of reasons:\\nUsers of our code and other developers don\\'t need to know the design pattern\\nbehind the code, as long as it works as intended.\\nStating the design pattern ruins the intention revealing principle. Adding the\\nname of the design pattern to a class makes it lose part of its original meaning. If\\na class represents a query, it should be named Query or EnhancedQuery,\\nsomething that reveals the intention of what that object is supposed to\\ndo. EnhancedQueryDecorator doesn\\'t mean anything meaningful, and the\\nDecorator suffix creates more confusion than clarity.\\n\\n[ 283 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nMentioning the design patterns in docstrings might be acceptable because they work as\\ndocumentation, and expressing the design ideas (again, communicating) in our design is a\\ngood thing. However, this should not be needed. Most of the time, though, we do not need\\nto know that a design pattern is there.\\nThe best designs are those in which design patterns are completely transparent to the users.\\nAn example of this is how the facade pattern appears in the standard library, making it\\ncompletely transparent to users as to how to access the os module. An even more elegant\\nexample is how the iterator design pattern is so completely abstracted by the language that\\nwe don\\'t even have to think about it.\\n\\nSummary\\nDesign patterns have always been seen as proven solutions to common problems. This is a\\ncorrect assessment, but in this chapter, we explored them from the point of view of good\\ndesign techniques, patterns that leverage clean code. In most of the cases, we looked at how\\nthey provide a good solution to preserve polymorphism, reduce coupling, and create the\\nright abstractions that encapsulate details as needed. All traits that relate to the concepts\\nexplored in Chapter 8, Unit Testing and Refactoring.\\nStill, the best thing about design patterns is not the clean design we can obtain from\\napplying them, but the extended vocabulary. Used as a communication tool, we can use\\ntheir names to express the intention of our design. And sometimes, it\\'s not the entire\\npattern that we need to apply, but we might need to take a particular idea (a substructure,\\nfor example) of a pattern from our solution, and here, too, they prove to be a way of\\ncommunicating more effectively.\\nWhen we create solutions by thinking in terms of patterns, we are solving problems at a\\nmore general level. Thinking in terms of design patterns, brings us closer to higher-level\\ndesign. We can slowly \"zoom-out\" and think more in terms of an architecture. And now\\nthat we are solving more general problems, it\\'s time to start thinking about how the system\\nis going to evolve and be maintained in the long run (how it\\'s going to scale, change, adapt,\\nand so on).\\nFor a software project to be successful in these goals, it requires clean code at its core, but\\nthe architecture also has to be clean as well, which is what we are going to look at in the\\nnext chapter.\\n\\n[ 284 ]\\n\\n\\x0cCommon Design Patterns\\n\\nChapter 9\\n\\nReferences\\nHere is a list of information you can refer to:\\nGoF: The book written by Erich Gamma, Richard Helm, Ralph Johnson, and John\\nVlissides named Design Patterns: Elements of Reusable Object-Oriented Software\\nSNGMONO: An article written by Robert C. Martin, 2002 named SINGLETON\\nand MONOSTATE\\nThe Null Object Pattern, written by Bobby Woolf\\n\\n[ 285 ]\\n\\n\\x0c10\\nClean Architecture\\nIn this final chapter, we focus on how everything fits together in the design of a whole\\nsystem. This is a more theoretical chapter. Given the nature of the topic, it would be too\\ncomplex to delve down into the more low-level details. Besides, the point is precisely to\\nescape from those details, assume that all the principles explored in previous chapters are\\nassimilated, and focus on the design of a system at scale.\\nThe main concerns and goals for this chapter are as follows:\\nDesigning software systems that can be maintained in the long run\\nWorking effectively on a software project by maintaining quality attributes\\nStudying how all concepts applied to code relate to systems in general\\n\\nFrom clean code to clean architecture\\nThis section is a discussion of how concepts that were emphasized in previous chapters\\nreappear in a slightly different shape when we consider aspects of large systems. There is\\nan interesting resemblance to how concepts that apply to more detailed design, as well as\\ncode, also apply to large systems and architectures.\\nThe concepts explored in previous chapters were related to single applications, generally, a\\nproject, that might be a single repository (or a few), for a source control version system (git).\\nThis is not to say that those design ideas are only applicable to code, or that they are of no\\nuse when thinking of an architecture, for two reasons: the code is the foundation of the\\narchitecture, and, if it\\'s not written carefully, the system will fail regardless of how well\\nthought-out the architecture is.\\nSecond, some principles that were revisited in previous chapters do not apply to code but\\nare instead design ideas. The clearest example comes from design patterns. They are highlevel ideas. With this, we can get a quick picture of how a component in our architecture\\nmight appear, without going into the details of the code.\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nBut large enterprise systems typically consist of many of these applications, and now it\\'s\\ntime to start thinking in terms of a larger design, in the form of a distributed system.\\nIn the following sections, we discuss the main topics that have been discussed throughout\\nthe book, but now from the perspective of a system.\\n\\nSeparation of concerns\\nInside an application, there are multiple components. Their code is divided into other\\nsubcomponents, such as modules or packages, and the modules into classes or functions,\\nand the classes into methods. Throughout the book, the emphasis has been on keeping\\nthese components as small as possible, particularly in the case of functions\\xe2\\x80\\x94functions\\nshould do one thing, and be small.\\nSeveral reasons were presented to justify this rationale. Small functions are easier to\\nunderstand, follow, and debug. They are also easier to test. The smaller the pieces in our\\ncode, the easier it will be to write unit tests for it.\\nFor the components of each application, we wanted different traits, mainly high cohesion,\\nand low coupling. By dividing components into smaller units, each one with a single and\\nwell-defined responsibility, we achieve a better structure where changes are easier to\\nmanage. In the face of new requirements, there will be a single rightful place to make the\\nchanges, and the rest of the code should probably be unaffected.\\nWhen we talk about code, we say component to refer to one of these cohesive units (it might\\nbe a class, for example). When speaking in terms of an architecture, a component means\\nanything in the system that can be treated as a working unit. The term component itself is\\nquite vague, so there is no universally accepted definition in software architecture of what\\nthis means more concretely. The concept of a working unit is something that can vary from\\nproject to project. A component should be able to be released or deployed with its own\\ncycles, independently from the rest of the parts of the system. And it is precisely that, one of\\nthe parts of a system, is namely the entire application.\\nFor Python projects, a component could be a package, but a service can also be a\\ncomponent. Notice how two different concepts, with different levels of granularity, can be\\nconsidered under the same category. To give an example, the event systems we used in\\nprevious chapters could be considered a component. It\\'s a working unit with a clearly\\ndefined purpose (to enrich events identified from logs), it can be deployed independently\\nfrom the rest (whether as a Python package, or, if we expose its functionality, as a service),\\nand it\\'s a part of the entire system, but not the whole application itself.\\n\\n[ 287 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nOn the examples of previous chapters we have seen an idiomatic code, and we have also\\nhighlighted the importance of good design for our code, with objects that have single welldefined responsibilities, being isolated, orthogonal, and easier to maintain. This very same\\ncriteria, which applies to detailed design (functions, classes, methods), also applies to the\\ncomponents of a software architecture.\\nIt\\'s probably undesirable for a large system to be just one component. A monolithic\\napplication will act as the single source of truth, responsible for everything in the system,\\nand that will carry a lot of undesired consequences (harder to isolate and identify changes,\\nto test effectively, and so on). In the same way, our code will be harder to maintain, if we\\nare not careful and place everything in one place, the application will suffer from similar\\nproblems if its components aren\\'t treated with the same level of attention.\\nThe idea of creating cohesive components in a system can have more than one\\nimplementation, depending on the level of abstraction we require.\\nOne option would be to identify common logic that is likely to be reused multiple times\\nand place it in a Python package (we will discuss the details later in the chapter).\\nAnother alternative would be to break the application into multiple smaller services, in\\na microservice architecture. The idea is to have components with a single and well-defined\\nresponsibility, and achieve the same functionality as a monolithic application by making\\nthose services cooperate, and exchange information.\\n\\nAbstractions\\nThis is where encapsulation appears again. From our systems (as we do in relation to the\\ncode), we want to speak in terms of the domain problem, and leave the implementation\\ndetails as hidden as possible.\\nIn the same way that the code has to be expressive (almost to the point of being selfdocumenting), and have the right abstractions that reveal the solution to the essential\\nproblem (minimizing accidental complexity), the architecture should tell us what the\\nsystem is about. Details such as the solution used to persist data on disk, the web\\nframework of choice, the libraries used to connect to external agents, and interaction\\nbetween systems, are not relevant. What is relevant is what the system does. A concept\\nsuch as a scream architecture (SCREAM) reflects this idea.\\n\\n[ 288 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nThe dependency inversion principle (DIP), explained in Chapter 4, The SOLID\\nPrinciples, is of great help in this regard; we don\\'t want to depend upon concrete\\nimplementations but rather abstractions. In the code, we place abstractions (or interfaces)\\non the boundaries, the dependencies, those parts of the application that we don\\'t control\\nand might change in the future. We do this because we want to invert the dependencies.\\nLet them have to adapt to our code (by having to comply with an interface), and not the\\nother way round.\\nCreating abstractions and inverting dependencies are good practices, but they\\'re not\\nenough. We want our entire application to be independent and isolated from things that are\\nout of our control. And this is even more than just abstracting with objects\\xe2\\x80\\x94we need layers\\nof abstraction.\\nThis is a subtle, but important difference with respect to the detailed design. In the DIP, it\\nwas recommended to create an interface, that could be implemented with the abc module\\nfrom the standard library, for instance. Because Python works with duck typing, while\\nusing an abstract class might be helpful, it\\'s not mandatory, as we can easily achieve the\\nsame effect with regular objects as long as they comply with the required interface. The\\ndynamic typing nature of Python allowed us to have these alternatives. When thinking in\\nterms of architecture, there is no such a thing. As it will become clearer with the example,\\nwe need to abstract dependencies entirely, and there is no feature of Python that can do\\nthat for us.\\nSome might argue \"Well, the ORM is a good abstraction for a database, isn\\'t it?\" It\\'s not\\nenough. The ORM itself is a dependency and, as such, out of our control. It would be even\\nbetter to create an intermediate layer, an adapter, between the API of the ORM and our\\napplication.\\nThis means that we don\\'t abstract the database just with an ORM; we use the abstraction\\nlayer we create on top of it, to define objects of our own that belong to our domain.\\nThe application then imports this component, and uses the entities provided by this layer,\\nbut not the other way round. The abstraction layer should not know about the logic of our\\napplication; it\\'s even truer that the database should know nothing about the application\\nitself. If that were the case, the database would be coupled to our application. The goal is to\\ninvert the dependency\\xe2\\x80\\x94this layer provides an API, and every storage component that\\nwants to connect has to conform to this API. This is the concept of a hexagonal architecture\\n(HEX).\\n\\n[ 289 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nSoftware components\\nWe have a large system now, and we need to scale it. It also has to be maintainable. At this\\npoint, the concerns aren\\'t only technical but also organizational. This means it\\'s not just\\nabout managing software repositories; each repository will most likely belong to an\\napplication, and it will be maintained by a team who owns that part of the system.\\nThis demands we keep in mind how a large system is divided into different components.\\nThis can have many phases, from a very simple approach about, say, creating Python\\npackages, to more complex scenarios in a microservice architecture.\\nThe situation could be even more complex when different languages are involved, but in\\nthis chapter, we will assume they are all Python projects.\\nThese components need to interact, as do the teams. The only way this can work at scale is\\nif all the parts agree on an interface, a contract.\\n\\nPackages\\nA Python package is a convenient way to distribute software and reuse code in a more\\ngeneral way. Packages that have been built can be published to an artifact repository (such\\nas an internal PyPi server for the company), from where it will be downloaded by the rest\\nof the applications that require it.\\nThe motivation behind this approach has many elements to it\\xe2\\x80\\x94it\\'s about reusing code at\\nlarge, and also achieving conceptual integrity.\\nHere, we discuss the basics of packaging a Python project that can be published in a\\nrepository. The default repository might be PyPi (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpypi.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8b), but also internal; or\\ncustom setups will work with the same basics.\\nWe are going to simulate that we have created a small library, and we will use that as an\\nexample to review the main points to take into consideration.\\nAside from all the open source libraries available, sometimes we might need some extra\\nfunctionality\\xe2\\x80\\x94perhaps our application uses a particular idiom repeatedly or relies on a\\nfunction or mechanism quite heavily and the team has devised a better function for these\\nparticular needs. In order to work more effectively, we can place this abstraction into a\\nlibrary, and encourage all team members to use the idioms as provided by it, because doing\\nso will help avoid mistakes and reduce bugs.\\n\\n[ 290 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nPotentially, there are infinite examples that could suit this scenario. Maybe the application\\nneeds to extract a lot of .tag.gz files (in a particular format) and has faced security\\nproblems in the past with malicious files that ended up with path traversal attacks. As a\\nmitigation measure, the functionality for abstracting custom file formats securely was put\\nin a library that wraps the default one and adds some extra checks. This sounds like a good\\nidea.\\nOr maybe there is a configuration file that has to be written, or parsed in a particular\\nformat, and this requires many steps to be followed in order; again, creating a helper\\nfunction to wrap this, and using it in all the projects that need it, constitutes a good\\ninvestment, not only because it saves a lot of code repetition, but also because it makes it\\nharder to make mistakes.\\nThe gain is not only complying with the DRY principle (avoiding code duplication,\\nencouraging reuse) but also that the abstracted functionality represents a single point of\\nreference of how things should be done, hence contributing to the attainment of conceptual\\nintegrity.\\nIn general, the minimum layout for a library would look like this:\\n.\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 Makefile\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 README.rst\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 setup.py\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 src\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 apptool\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 common.py\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 __init__.py\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 parse.py\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 tests\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 integration\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 unit\\n\\nThe important part is the setup.py file, which contains the definition for the package. In\\nthis file, all the important definitions of the project (its requirements, dependencies, name,\\ndescription, and so on) are specified.\\nThe apptool directory under src is the name of the library we\\'re working on. This is a\\ntypical Python project, so we place here all the files we need.\\n\\n[ 291 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nAn example of the setup.py file could be:\\nfrom setuptools import find_packages, setup\\nwith open(\"README.rst\", \"r\") as longdesc:\\nlong_description = longdesc.read()\\n\\nsetup(\\nname=\"apptool\",\\ndescription=\"Description of the intention of the package\",\\nlong_description=long_description,\\nauthor=\"Dev team\",\\nversion=\"0.1.0\",\\npackages=find_packages(where=\"src/\"),\\npackage_dir={\"\": \"src\"},\\n)\\n\\nThis minimal example contains the key elements of the project. The name argument in the\\nsetup function is used to give the name that the package will have in the repository (under\\nthis name, we run the command to install it, in this case its pip install apptool). It\\'s\\nnot strictly required that it matches the name of the project directory (src/apptool), but\\nit\\'s highly recommended, so its easier for users.\\nIn this case, since both names match, it\\'s easier to see the relationship between what pip\\ninstall apptool and then, in our code, run from apptool import myutil. But the\\nlatter corresponds to the name under the src/ directory and the former to the one specified\\nin the setup.py file.\\nThe version is important to keep different releases going on, and then the packages are\\nspecified. By using the find_packages() function, we automatically discover everything\\nthat\\'s a package, in this case under the src/ directory. Searching under this directory helps\\nto avoid mixing up files beyond the scope of the project and, for instance, accidentally\\nreleasing tests or a broken structure of the project.\\nA package is built by running the following commands, assuming its run inside a virtual\\nenvironment with the dependencies installed:\\n$VIRTUAL_ENV/bin/pip install -U setuptools wheel\\n$VIRTUAL_ENV/bin/python setup.py sdist bdist_wheel\\n\\nThis will place the artifacts in the dist/ directory, from where they can be later published\\neither to PyPi or to the internal package repository of the company.\\n\\n[ 292 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nThe key points in packaging a Python project are:\\nTest and verify that the installation is platform-independent and that it doesn\\'t\\nrely on any local setup (this can be achieved by placing the source files under an\\nsrc/ directory)\\nMake sure that unit tests aren\\'t shipped as part of the package being built\\nSeparate dependencies\\xe2\\x80\\x94what the project strictly needs to run is not the same as\\nwhat developers require\\nIt\\'s a good idea to create entry points for the commands that are going to be\\nrequired the most\\nThe setup.py file supports multiple other parameters and configurations and can be\\neffected in a much more complicated manner. If our package requires several operating\\nsystem libraries to be installed, it\\'s a good idea to write some logic in the setup.py file to\\ncompile and build the extensions that are required. This way, if something is amiss, it will\\nfail early on in the installation process, and if the package provides a helpful error message,\\nthe user will be able to fix the dependencies more quickly and continue.\\nInstalling such dependencies represents another difficult step in making the application\\nubiquitous, and easy to run by any developer regardless of their platform of choice. The\\nbest way to surmount this obstacle is to abstract the platform by creating a Docker image,\\nas we will discuss in the next section.\\n\\nContainers\\nThis chapter is dedicated to architecture, so the term container refers to something\\ncompletely different from a Python container (an object with a __contains__ method),\\nexplored in Chapter 2, Pythonic Code. A container is a process that runs in the operating\\nsystem under a group with certain restrictions and isolation considerations. Concretely we\\nrefer to Docker containers, which allow managing applications (services or processes) as\\nindependent components.\\nContainers represent another way of delivering software. Creating Python packages taking\\ninto account the considerations in the previous section is more suitable for libraries, or\\nframeworks, where the goal is to reuse code and take advantage of using a single place\\nwhere specific logic is gathered.\\n\\n[ 293 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nIn the case of containers, the objective will not be creating libraries but applications (most of\\nthe time). However, an application or platform does not necessarily mean an entire service.\\nThe idea of building containers is to create small components that represent a service with a\\nsmall and clear purpose.\\nIn this section, we will mention Docker when we talk about containers, and we will explore\\nthe basics of how to create Docker images and containers for Python projects. Keep in mind\\nthat this is not the only technology for launching applications into containers, and also that\\nit\\'s completely independent of Python.\\nA Docker container needs an image to run on, and this image is created from other base\\nimages. But the images we create can themselves serve as base images for other containers.\\nWe will want to do that in cases where there is a common base in our application that can\\nbe shared across many containers. A potential use would be creating a base image that\\ninstalls a package (or many) in the way we described in the previous section, and also all of\\nits dependencies, including those at the operating system level. As discussed in Chapter\\n9, Common Design Patterns, a package we create can depend not only on other Python\\nlibraries, but also on a particular platform (a specific operating system), and particular\\nlibraries preinstalled in that operating system, without which the package will simply not\\ninstall and will fail.\\nContainers are a great portability tool for this. They can help us ensure that our application\\nwill have a canonical way of running, and it will also ease the development process a lot\\n(reproducing scenarios across environments, replicating tests, on-boarding new team\\nmembers, and so on).\\nAs packages are the way we reuse code and unify criteria, containers represent the way we\\ncreate the different services of the application. They meet the criteria behind the principle of\\nseparation of concerns (SoC) of the architecture. Each service is another kind of component\\nthat will encapsulate a set of functionalities independently of the rest of the application.\\nThese containers ought to be designed in such a way that they favor maintainability\\xe2\\x80\\x94if the\\nresponsibilities are clearly divided, a change in a service should not impact any other part\\nof the application whatsoever.\\nWe cover the basics of how to create a Docker container from a Python project in the next\\nsection.\\n\\n[ 294 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nUse case\\nAs an example of how we might organize the components of our application, and how the\\nprevious concepts might work in practice, we present the following simple example.\\nThe use case is that there is an application for delivering food, and this application has a\\nspecific service for tracking the status of each delivery at its different stages. We are going\\nto focus only on this particular service, regardless of how the rest of the application might\\nappear. The service has to be really simple\\xe2\\x80\\x94a REST API that, when asked about the status\\nof a particular order, will return a JSON response with a descriptive message.\\nWe are going to assume that the information about each particular order is stored in a\\ndatabase, but this detail should not matter at all.\\nOur service has two main concerns for now: getting the information about a particular\\norder (from wherever this might be stored), and presenting this information in a useful way\\nto the clients (in this case, delivering the results in JSON format, exposed as a web service).\\nAs the application has to be maintainable and extensible, we want to keep these two\\nconcerns as hidden as possible and focus on the main logic. Therefore, these two details are\\nabstracted and encapsulated into Python packages that the main application with the core\\nlogic will use, as shown in the following diagram:\\n\\nIn the following sections, we briefly demonstrate how the code might appear, in terms of\\nthe packages mainly, and how to create services from these, in order to finally see what\\nconclusions we can infer.\\n\\n[ 295 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nThe code\\nThe idea of creating Python packages in this example is to illustrate how abstracted and\\nisolated components can be made, in order to work effectively. In reality, there is no actual\\nneed for these to be Python packages; we could just create the right abstractions as part of\\nthe \"delivery service\" project, and, while the correct isolation is preserved, it will work\\nwithout any issues.\\nCreating packages makes more sense when there is logic that is going to be repeated and is\\nexpected to be used across many other applications (that will import from those packages)\\nbecause we want to favor code reuse. In this particular case, there are no such\\nrequirements, so it might be beyond the scope of the design, but such distinction still makes\\nmore clear the idea of a \"pluggable architecture\" or component, something that is really a\\nwrapper abstracting technical details we don\\'t really want to deal with, much less depend\\nupon.\\nThe storage package is in charge of retrieving the data that is required, and presenting\\nthis to the next layer (the delivery service) in a convenient format, something that is suitable\\nfor the business rules. The main application should now know where this data came from,\\nwhat its format is, and so on. This is the entire reason why we have such an abstraction in\\nbetween so the application doesn\\'t use a row or an ORM entity directly, but rather\\nsomething workable.\\n\\nDomain models\\nThe following definitions apply to classes for business rules. Notice that they are meant to\\nbe pure business objects, not bound to anything in particular. They aren\\'t models of an\\nORM, or objects of an external framework, and so on. The application should work with\\nthese objects (or objects with the same criteria).\\nIn each case, the dosctring documents the purpose of each class, according to the business\\nrule:\\nfrom typing import Union\\nclass DispatchedOrder:\\n\"\"\"An order that was just created and notified to start its\\ndelivery.\"\"\"\\nstatus = \"dispatched\"\\ndef __init__(self, when):\\nself._when = when\\n\\n[ 296 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\ndef message(self) -> dict:\\nreturn {\\n\"status\": self.status,\\n\"msg\": \"Order was dispatched on {0}\".format(\\nself._when.isoformat()\\n),\\n}\\n\\nclass OrderInTransit:\\n\"\"\"An order that is currently being sent to the customer.\"\"\"\\nstatus = \"in transit\"\\ndef __init__(self, current_location):\\nself._current_location = current_location\\ndef message(self) -> dict:\\nreturn {\\n\"status\": self.status,\\n\"msg\": \"The order is in progress (current location:\\n{})\".format(\\nself._current_location\\n),\\n}\\n\\nclass OrderDelivered:\\n\"\"\"An order that was already delivered to the customer.\"\"\"\\nstatus = \"delivered\"\\ndef __init__(self, delivered_at):\\nself._delivered_at = delivered_at\\ndef message(self) -> dict:\\nreturn {\\n\"status\": self.status,\\n\"msg\": \"Order delivered on {0}\".format(\\nself._delivered_at.isoformat()\\n),\\n}\\n\\nclass DeliveryOrder:\\ndef __init__(\\nself,\\ndelivery_id: str,\\n\\n[ 297 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nstatus: Union[DispatchedOrder, OrderInTransit, OrderDelivered],\\n) -> None:\\nself._delivery_id = delivery_id\\nself._status = status\\ndef message(self) -> dict:\\nreturn {\"id\": self._delivery_id, **self._status.message()}\\n\\nFrom this code, we can already get an idea of what the application will look like\\xe2\\x80\\x94we want\\nto have a DeliveryOrder object, which will have its own status (as an internal\\ncollaborator), and once we have that, we will call its message() method to return this\\ninformation to the user.\\n\\nCalling from the application\\nHere is how these objects are going to be used in the application. Notice how this depends\\non the previous packages (web and storage), but not the other way round:\\nfrom storage import DBClient, DeliveryStatusQuery, OrderNotFoundError\\nfrom web import NotFound, View, app, register_route\\nclass DeliveryView(View):\\nasync def _get(self, request, delivery_id: int):\\ndsq = DeliveryStatusQuery(int(delivery_id), await DBClient())\\ntry\\nresult = await dsq.get()\\nexcept OrderNotFoundError as e:\\nraise NotFound(str(e)) from e\\nreturn result.message()\\nregister_route(DeliveryView, \"/status/<delivery_id:int>\")\\n\\nIn the previous section, the domain objects were shown and here the code for the\\napplication is displayed. Aren\\'t we missing something? Sure, but is it something we really\\nneed to know now? Not necessarily.\\nThe code inside the storage and web packages was deliberately left out (although the\\nreader is more than encouraged to look at it\\xe2\\x80\\x94the repository for the book contains the full\\nexample). Also, and this was done on purpose, the names of such packages were chosen so\\nas not to reveal any technical detail\\xe2\\x80\\x94storage and web.\\n\\n[ 298 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nLook again at the code in the previous listing. Can you tell which frameworks are being\\nused? Does it say whether the data comes from a text file, a database (if so, of what type?\\nSQL? NoSQL?), or another service (the web, for instance)? Assume that it comes from a\\nrelational database. Is there any clue to how this information is retrieved (manual SQL\\nqueries? Through an ORM?)?\\nWhat about the web? Can we guess what frameworks are used?\\nThe fact that we cannot answer any of those questions is probably a good sign. Those are\\ndetails, and details ought to be encapsulated. We can\\'t answer those questions unless we\\ntake a look at what\\'s inside those packages.\\nThere is another way of answering the previous questions, and it comes in the form of a\\nquestion itself: why do we need to know that? Looking at the code, we can see that there is\\na DeliveryOrder, created with an identifier of a delivery, and it that has a get() method,\\nwhich returns an object representing the status of the delivery. If all of this information is\\ncorrect, that\\'s all we should care about. What difference does it make how is it done?\\nThe abstractions we created make our code declarative. In declarative programming, we\\ndeclare the problem we want to solve, not how we want to solve it. It\\'s the opposite\\nof imperative, in which we have to make all the steps required explicit in order to get\\nsomething (for instance connect to the database, run this query, parse the result, load it into\\nthis object, and so on). In this case, we are declaring that we just want to know the status of\\nthe delivery given by some identifier.\\nThese packages are in charge of dealing with the details and presenting what the\\napplication needs in a convenient format, namely objects of the kind presented in the\\nprevious section. We just have to know that the storage package contains an object that,\\ngiven an ID for a delivery and a storage client (this dependency is being injected into this\\nexample for simplicity, but other alternatives are also possible), it will\\nretrieve DeliveryOrder which we can then ask to compose the message.\\nThis architecture provides convenience and makes it easier to adapt to changes, as it\\nprotects the kernel of the business logic from the external factors that can change.\\nImagine we want to change how the information is retrieved. How hard would that be? The\\napplication relies on an API, like the following one:\\ndsq = DeliveryStatusQuery(int(delivery_id), await DBClient())\\n\\n[ 299 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nSo it would be about just changing how the get() method works, adapting it to the new\\nimplementation detail. All we need is for this new object to return DeliveryOrder on\\nits get() method and that would be all. We can change the query, the ORM, the database,\\nand so on, and, in all cases, the code in the application does not need to change!\\n\\nAdapters\\nStill, without looking at the code in the packages, we can conclude that they work as\\ninterfaces for the technical details of the application.\\nIn fact, since we are seeing the application from a high-level perspective, without needing\\nto look at the code, we can imagine that inside those packages there must be an\\nimplementation of the adapter design pattern (introduced in Chapter 9, Common Design\\nPatterns). One or more of these objects is adapting an external implementation to the API\\ndefined by the application. This way, dependencies that want to work with the application\\nmust conform to the API, and an adapter will have to be made.\\nThere is one clue pertaining to this adapter in the code for the application though. Notice\\nhow the view is constructed. It inherits from a class named View that comes from our web\\npackage. We can deduce that this View is, in turn, a class derived from one of the web\\nframeworks that might be being used, creating an adapter by inheritance. The important\\nthing to note is that once this is done, the only object that matters is our View class, because,\\nin a way, we are creating our own framework, which is based on adapting an existing one\\n(but again changing the framework will mean just changing the adapters, not the entire\\napplication).\\n\\nThe services\\nTo create the service, we are going to launch the Python application inside a Docker\\ncontainer. Starting from a base image, the container will have to install the dependencies\\nfor the application to run, which also has dependencies at the operating system level.\\nThis is actually a choice because it depends on how the dependencies are used. If a package\\nwe use requires other libraries on the operating system to compile at installation time, we\\ncan avoid this simply by building a wheel for our platform of the library and installing this\\ndirectly. If the libraries are needed at runtime, then there is no choice but to make them part\\nof the image of the container.\\n\\n[ 300 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nNow, we discuss one of the many ways of preparing a Python application to be run inside a\\nDocker container. This is one of numerous alternatives for packaging a Python project into\\na container. First, we take a look at what the structure of the directories looks like:\\n.\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 Dockerfile\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 libs\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 README.rst\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 storage\\n\\xe2\\x94\\x82\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 web\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 Makefile\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 README.rst\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 setup.py\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 statusweb\\n\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80 __init__.py\\n\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80 service.py\\n\\nThe libs directory can be ignored since it\\'s just the place where the dependencies are\\nplaced (it\\'s displayed here to keep them in mind when they are referenced in the setup.py\\nfile, but they could be placed in a different repository and installed remotely via pip).\\nWe have Makefile with some helper commands, then the setup.py file, and the\\napplication itself inside the statusweb directory. A common difference between packaging\\napplications and libraries is that while the latter specify their dependencies in\\nthe setup.py file, the former have a requirements.txt file from where dependencies are\\ninstalled via pip install -r requirements.txt. Normally, we would do this in the\\nDockerfile, but in order to keep things simpler in this particular example, we will assume\\nthat taking the dependencies from the setup.py file is enough. This is because, besides this\\nconsideration, there are a lot more considerations to be taken into account when dealing\\nwith dependencies, such as freezing the version of the packages, tracking indirect\\ndependencies, using extra tools such as pipenv, and more topics that are beyond the scope\\nof the chapter. In addition, it is also customary to make the setup.py file read\\nfrom requirements.txt for consistency.\\nNow we have the content of the setup.py file, which states some details of the application:\\nfrom setuptools import find_packages, setup\\nwith open(\"README.rst\", \"r\") as longdesc:\\nlong_description = longdesc.read()\\n\\ninstall_requires = [\"web\", \"storage\"]\\nsetup(\\n\\n[ 301 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nname=\"delistatus\",\\ndescription=\"Check the status of a delivery order\",\\nlong_description=long_description,\\nauthor=\"Dev team\",\\nversion=\"0.1.0\",\\npackages=find_packages(),\\ninstall_requires=install_requires,\\nentry_points={\\n\"console_scripts\": [\\n\"status-service = statusweb.service:main\",\\n],\\n},\\n)\\n\\nThe first thing we notice is that the application declares its dependencies, which are the\\npackages we created and placed under libs/, namely web and storage, abstracting and\\nadapting to some external components. These packages, in turn, will have dependencies, so\\nwe will have to make sure the container installs all the required libraries when the image is\\nbeing created so that they can install successfully, and then this package afterward.\\nThe second thing we notice is the definition of the entry_points keyword argument\\npassed to the setup function. This is not strictly mandatory, but it\\'s a good idea to create an\\nentry point. When the package is installed in a virtual environment, it shares this directory\\nalong with all its dependencies. A virtual environment is a structure of directories with the\\ndependencies of a given project. It has many subdirectories, but the most important ones\\nare:\\n<virtual-env-root>/lib/<python-version>/site-packages\\n<virtual-env-root>/bin\\n\\nThe first one contains all the libraries installed in that virtual environment. If we were to\\ncreate a virtual environment with this project, that directory would contain the web,\\nand storage packages, along with all its dependencies, plus some extra basic ones and the\\ncurrent project itself.\\n\\n[ 302 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nThe second, /bin/, contains the binary files and commands available when that virtual\\nenvironment is active. By default, it would just be the version of Python, pip, and some\\nother basic commands. When we create an entry point, a binary with that declared name is\\nplaced there, and, as a result, we have that command available to run when the\\nenvironment is active. When this command is called, it will run the function that is\\nspecified with all the context of the virtual environment. That means it is a binary we can\\ncall directly without having to worry about whether the virtual environment is active, or\\nwhether the dependencies are installed in the path that is currently running.\\nThe definition is the following one:\\n\"status-service = statusweb.service:main\"\\n\\nThe left-hand side of the equals sign declares the name of the entry point. In this case, we\\nwill have a command named status-service available. The right-hand side declares how\\nthat command should be run. It requires the package where the function is defined,\\nfollowed by the function name after :. In this case, it will run the main function declared in\\nstatusweb/service.py.\\nThis is followed by a definition of the Dockerfile:\\nFROM python:3.6.6-alpine3.6\\nRUN apk add --update \\\\\\npython-dev \\\\\\ngcc \\\\\\nmusl-dev \\\\\\nmake\\nWORKDIR /app\\nADD . /app\\nRUN pip install /app/libs/web /app/libs/storage\\nRUN pip install /app\\nEXPOSE 8080\\nCMD [\"/usr/local/bin/status-service\"]\\n\\nThe image is built based on a lightweight Python image, and then the operating system\\ndependencies are installed so that our libraries can be installed. Following the previous\\nconsideration, this Dockerfile simply copies the libraries, but this might as well be\\ninstalled from a requirements.txt file accordingly. After all the pip install\\ncommands are ready, it copies the application in the working directory, and the entry point\\nfrom Docker (the CMD command, not to be confused with the Python one) calls the entry\\npoint of the package where we placed the function that launches the process.\\n\\n[ 303 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nAll the configuration is passed by environment variables, so the code for our service will\\nhave to comply with this norm.\\nIn a more complex scenario involving more services and dependencies, we will not just run\\nthe image of the created container, but instead declare a docker-compose.yml file with\\nthe definitions of all the services, base images, and how they are linked and interconnected.\\nNow that we have the container running, we can launch it and run a small test on it to get\\nan idea of how it works:\\n$ curl http://localhost:8080/status/1\\n{\"id\":1,\"status\":\"dispatched\",\"msg\":\"Order was dispatched on\\n2018-08-01T22:25:12+00:00\"}\\n\\nAnalysis\\nThere are many conclusions to be drawn from the previous implementation. While it might\\nseem like a good approach, there are cons that come with the benefits; after all, no\\narchitecture or implementation is perfect. This means that a solution such as this one cannot\\nbe good for all cases, so it will pretty much depend on the circumstances of the project, the\\nteam, the organization, and more.\\nWhile it\\'s true that the main idea of the solution is to abstract details as much as possible, as\\nwe shall see some parts cannot be fully abstracted away, and also the contracts between the\\nlayers imply an abstraction leak.\\n\\nThe dependency flow\\nNotice that dependencies flow in only one direction, as they move closer to the kernel,\\nwhere the business rules lie. This can be traced by looking at the import statements. The\\napplication imports everything it needs from storage, for example, and in no part is this\\ninverted.\\nBreaking this rule would create coupling. The way the code is arranged now means that\\nthere is a weak dependency between the application and storage. The API is such that we\\nneed an object with a get() method, and any storage that wants to connect to the\\napplication needs to implement this object according to this specification. The dependencies\\nare therefore inverted\\xe2\\x80\\x94it\\'s up to every storage to implement this interface, in order to\\ncreate an object according to what the application is expecting.\\n\\n[ 304 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nLimitations\\nNot everything can be abstracted away. In some cases, it\\'s simply not possible, and in\\nothers, it might not be convenient. Let\\'s start with the convenience aspect.\\nIn this example, there is an adapter of the web framework of choice to a clean API to be\\npresented to the application. In a more complex scenario, such a change might not be\\npossible. Even with this abstraction, parts of the library were still visible to the application.\\nAdapting an entire framework might not only be hard but also not possible in some cases.\\nIt\\'s not entirely a problem to be completely isolated from the web framework because,\\nsooner or later, we will need some of its features or technical details.\\nThe important takeaway here is not the adapter, but the idea of hiding technical details as\\nmuch as possible. That means, that the best thing that was displayed on the listing for the\\ncode of the application was not the fact that there was an adapter between our version of\\nthe web framework and the actual one, but instead the fact that the latter was not\\nmentioned by name in any part of the visible code. The service was made clear that web\\nwas just a dependency (a detail being imported), and revealed the intention behind what it\\nwas supposed to do. The goal is to reveal the intention (as in the code) and to defer details\\nas much as possible.\\nAs to what things cannot be isolated, those are the elements that are closest to the code. In\\nthis case, the web application was using the objects operating within them in an\\nasynchronous fashion. That is a hard constraint we cannot circumvent. It\\'s true that\\nwhatever is inside the storage package can be changed, refactored, and modified, but\\nwhatever these modifications might be, it still needs to preserve the interface, and that\\nincludes the asynchronous interface.\\n\\nTestability\\nAgain, much like with the code, the architecture can benefit from separating pieces into\\nsmaller components. The fact that dependencies are now isolated and controlled by\\nseparate components leaves us with a cleaner design for the main application, and now it\\'s\\neasier to ignore the boundaries to focus on testing the core of the application.\\nWe could create a patch for the dependencies, and write unit tests that are simpler (they\\nwon\\'t need a database), or to launch an entire web service, for instance. Working with pure\\ndomain objects means it will be easier to understand the code and the unit tests. Even the\\nadapters will not need that much testing because their logic should be very simple.\\n\\n[ 305 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nIntention revealing\\nThese details included keeping functions short, concerns separated, dependencies isolated,\\nand assigning the right meaning to abstractions in every part of the code. Intention\\nrevealing was a critical concept for our code\\xe2\\x80\\x94every name has to be wisely chosen, clearly\\ncommunicating what it\\'s supposed to do. Every function should tell a story.\\nA good architecture should reveal the intent of the system it entails. It should not mention\\nthe tools it\\'s built with; those are details, and as we discussed at length, details should be\\nhidden, encapsulated.\\n\\nSummary\\nThe principles for good software design apply on all levels. In the same way that we want\\nto write readable code, and for that we need to mind the intention revealing degree of the\\ncode, the architecture also has to express the intent of the problem it is trying to solve.\\nAll these ideas are interconnected. The same intention revealing that ensures our\\narchitecture is defined in terms of the domain problem also leads us to abstract details as\\nmuch as possible, create layers of abstraction, invert dependencies, and separate concerns.\\nWhen it comes to reusing code, Python packages are a great and flexible alternative.\\nCriteria, such as cohesion and the single responsibility principle (SRP), are the most\\nimportant considerations when deciding to create a package. In line with having\\ncomponents with cohesion and few responsibilities, the concept of microservices comes\\ninto play, and for that, we have seen how a service can be deployed in a Docker container\\nstarting from a packaged Python application.\\nAs with everything in software engineering, there are limitations, and there are exceptions.\\nIt will not always be possible to abstract things as much as we would like to or to\\ncompletely isolate dependencies. Sometimes, it will just not be possible (or practical) to\\ncomply with the principles explained here in the book. But that is probably the best piece of\\nadvice the reader should take from the book\\xe2\\x80\\x94they are just principles, not laws. If it\\'s not\\npossible, or practical, to abstract from a framework, it should not be a problem. Remember\\nwhat has been quoted from the zen of Python itself, throughout the book\\xe2\\x80\\x94practicality\\nbeats purity.\\n\\n[ 306 ]\\n\\n\\x0cClean Architecture\\n\\nChapter 10\\n\\nReferences\\nHere is a list of information you can refer to:\\nSCREAM: The Screaming Architecture (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8b8thlight.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bblog/\\xe2\\x80\\x8bunclebob/\\xe2\\x80\\x8b2011/\\xe2\\x80\\x8b09/\\xe2\\x80\\x8b30/\\xe2\\x80\\x8bScreaming-\\xe2\\x80\\x8bArchitecture.\\xe2\\x80\\x8bhtml)\\nCLEAN-01: The Clean Architecture (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8b8thlight.\\xe2\\x80\\x8bcom/\\xe2\\x80\\x8bblog/\\xe2\\x80\\x8buncle-\\xe2\\x80\\x8bbob/\\n2012/\\xe2\\x80\\x8b08/\\xe2\\x80\\x8b13/\\xe2\\x80\\x8bthe-\\xe2\\x80\\x8bclean-\\xe2\\x80\\x8barchitecture.\\xe2\\x80\\x8bhtml)\\nHEX: Hexagonal Architecture (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bstaging.\\xe2\\x80\\x8bcockburn.\\xe2\\x80\\x8bus/\\xe2\\x80\\x8bhexagonalarchitecture/\\xe2\\x80\\x8b)\\nPEP-508: Dependency specification for Python software packages (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bwww.\\npython.\\xe2\\x80\\x8borg/\\xe2\\x80\\x8bdev/\\xe2\\x80\\x8bpeps/\\xe2\\x80\\x8bpep-\\xe2\\x80\\x8b0508/\\xe2\\x80\\x8b)\\nPackaging and distributing projects in Python (https:/\\xe2\\x80\\x8b/\\xe2\\x80\\x8bpython-\\xe2\\x80\\x8bpackaginguser-\\xe2\\x80\\x8bguide.\\xe2\\x80\\x8breadthedocs.\\xe2\\x80\\x8bio/\\xe2\\x80\\x8bguides/\\xe2\\x80\\x8bdistributing-\\xe2\\x80\\x8bpackages-\\xe2\\x80\\x8busingsetuptools/\\xe2\\x80\\x8b#distributing-\\xe2\\x80\\x8bpackages\\n\\nSumming it all up\\nThe content of the book is a reference, a possible way of implementing a software solution\\nby following criteria. These criteria are explained through examples, and the rationale for\\nevery decision presented. The reader might very well disagree with the approach taken on\\nthe examples, and this is actually desirable: the more viewpoints, the richer the debate. But\\nregardless of opinions, it\\'s important to make clear that what is presented here is by no\\nmeans a strong directive, something that must be followed imperatively. Quite the\\nopposite, it\\'s a way of presenting the reader with a solution and a set of ideas that they\\nmight find helpful.\\nAs introduced at the beginning of the book, the goal of the book was not to give you recipes\\nor formulas that you can apply directly, but rather to make you develop critical thinking.\\nIdioms and syntax features come and go, they change over time. But ideas, and core\\nsoftware concepts, remain. With these tools given, and the examples provided, you should\\nhave a better understanding of what clean code means.\\nI sincerely hope the book has helped you become a better developer than you were before\\nyou started it, and I wish you the best of luck in your projects.\\n\\n[ 307 ]\\n\\n\\x0cOther Books You May Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt:\\n\\nSecret Recipes of the Python Ninja\\nCody Jackson\\nISBN: 978-1-78829-487-4\\nKnow the differences between .py and .pyc files\\nExplore the different ways to install and upgrade Python packages\\nUnderstand the working of the PyPI module that enhances built-in decorators\\nSee how coroutines are different from generators and how they can simulate\\nmultithreading\\nGrasp how the decimal module improves floating point numbers and their\\noperations\\nStandardize sub interpreters to improve concurrency\\nDiscover Python\\xe2\\x80\\x99s built-in docstring analyzer\\n\\n\\x0cOther Books You May Enjoy\\n\\nPython Programming Blueprints\\nDaniel Furtado, Marcus Pennington\\nISBN: 978-1-78646-816-1\\nLearn object-oriented and functional programming concepts while developing\\nprojects\\nThe dos and don\\'ts of storing passwords in a database\\nDevelop a fully functional website using the popular Django framework\\nUse the Beautiful Soup library to perform web scrapping\\nGet started with cloud computing by building microservice and serverless\\napplications in AWS\\nDevelop scalable and cohesive microservices using the Nameko framework\\nCreate service dependencies for Redis and PostgreSQL\\n\\n[ 309 ]\\n\\n\\x0cOther Books You May Enjoy\\n\\nLeave a review - let other readers know what\\nyou think\\nPlease share your thoughts on this book with others by leaving a review on the site that you\\nbought it from. If you purchased the book from Amazon, please leave us an honest review\\non this book\\'s Amazon page. This is vital so that other potential readers can see and use\\nyour unbiased opinion to make purchasing decisions, we can understand what our\\ncustomers think about our products, and our authors can see your feedback on the title that\\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\\nvaluable to other potential customers, our authors, and Packt. Thank you!\\n\\n[ 310 ]\\n\\n\\x0cIndex\\nA\\nabstractions 288\\nacronyms\\nabout 72\\nDRY/OAOO 72\\nEAFP/LBYL 76\\nKISS 75\\nYAGNI 74\\nanalysis\\ndependency flow 304\\nintention revealing 306\\nlimitations 305\\ntestability 305\\nannotations\\nabout 13, 16\\nusing, instead of docstrings 18\\nApplication Programming Interface (API) 56\\narguments\\ncompact function signatures 92\\ncopying, to functions 86\\nin functions 85, 91\\nin methods 85\\npassing, to Python 85\\nvariable arguments 89\\nvariable number 87\\nasynchronous programming 215, 217\\nautomatic checks\\nsetup 21\\n\\nB\\nbasic quality gates\\nenforcing, by tools configuration 20\\nbehavioral patterns\\nabout 270\\nchain of responsibility 270, 272\\ncommand pattern 273\\n\\nstate pattern 274, 279\\ntemplate method 272\\nbest practices, software design\\nabout 93\\ncode, structuring 96\\northogonality in software 94\\nBlack\\nreference 22\\nborg pattern\\nabout 260\\nbuilder pattern 262\\n\\nC\\nC3 linearization 84\\ncallable objects 48\\ncaveats, Python\\nabout 49\\nbuilt-in types, extending 51\\nmutable default arguments 50\\nclean architecture\\nabout 286\\nreference 307\\nclean code\\ncode formatting, role 9\\ncoding style guide, adhering to 10\\ndefining 8\\nimportance 8\\ncode coverage\\nabout 236\\nrest coverage, setting up 236\\ntest coverage, caveats 237\\ncode dependency, consequences\\nlow level of abstraction 71\\nno code reuse 71\\nripple effects 71\\ncode duplication, limitations\\nerror prone 72\\n\\n\\x0cexpensive 72\\nunreliability 72\\ncode reuse\\nadapters 300\\ncalling, from application 298\\ndomain models 296\\ncode simplification, through iterators\\nabout 197\\nnested loops 198\\nrepeated iterations 198\\ncode\\ndocumenting 12\\nevolving 244\\ncohesion 71\\ncomposition 77\\nconsiderations, descriptors\\nclass decorators, avoiding 176, 179\\ncode, reusing 175\\ncontainer iterable 43\\ncontainer objects 45\\ncontext managers\\nabout 29, 32\\nimplementing 32, 35\\ncoroutines\\nabout 203\\nadvanced coroutines 209\\ndata, receiving from sub-generator 213\\ndata, sending to sub-generator 213\\ndelegating, into smaller coroutines 210\\ngenerator interface, methods 204\\nvalue returned by sub-generator, returning 212\\nvalues, returning 209\\nyield form, use 211\\ncoupling 71\\ncreational patterns\\nabout 256\\nborg pattern 260, 262\\nfactories 256\\nshared state 257, 260\\nsingleton 257\\n\\nD\\ndata descriptors 163, 165\\ndecorate classes 127, 130\\ndecorate functions 126\\n\\ndecorator objects 134\\ndecorators\\nanalyzing 149\\nand separation of concerns 147\\narguments, passing 131\\ncode, tracing 136\\ncommon mistakes, avoiding 136\\ncreating 143, 145\\ndata, preserving about original wrapped object\\n136\\ndescriptors, implementing 185\\nDRY principle, using with 146\\nincorrect handling of side-effects 139\\nqualities 149\\nside-effects, need for 141\\ntypes 131\\nuses 135\\nusing, in Python 124\\nwith nested functions 132\\ndefensive programming\\nabout 60\\nassertions, using in Python 69\\nerror handling 61\\ndependency injection 122\\ndependency inversion principle (DIP)\\nabout 120, 289\\ndependencies, inverting 121\\nrigid dependencies 121\\ndescriptor, using in Python\\nbuilt-in decorators, for methods 184\\nfunctions and methods 180\\nslots 185\\ndescriptors\\nglobal shared state, issue 172\\nabout 152, 168\\nanalysis 180\\napplication 168\\navoiding 168\\nconsiderations 175\\ndata descriptors 165, 167\\nidiomatic implementation 169\\nimplementing, forms 172\\nimplementing, in decorators 185\\nmachinery 153, 156\\nnon-data descriptors 163, 165\\n\\n[ 312 ]\\n\\n\\x0cobject dictionary, accessing 173\\ntypes 163\\nusing, in Python 180\\nweak references, using 174\\ndesign by contract scenario\\nabout 56\\nconclusions 59\\ninvariants 57\\npostconditions 57, 59\\npreconditions 57, 58\\nPythonic contracts 59\\nside-effects 57\\ndesign patterns\\nabout 282\\nbehavioral patterns 269\\nconsiderations 254\\ncreational patterns 256\\ninfluence 282\\nstructural patterns 262\\nusing 255\\nusing, in code 283\\ndocstrings 13, 16\\ndocumentation 13\\nDon\\'t Repeat Yourself (DRY) principle\\nabout 72\\nusing, with decorators 146, 236\\nduck typing principle 117\\n\\nworking, in Python 85\\n\\nG\\ngenerators\\nabout 189, 191\\ncreating 189\\nexpressions 192\\nreferences 218\\ngod-objects 100\\n\\nH\\nHexagonal Architecture\\nreference 307\\nhint 16\\n\\nI\\n\\nE\\nEasier to Ask Forgiveness than Permission (EAFP)\\n76\\nerror handling\\nabout 61\\nexception handling 62\\nvalue substitution 61, 62\\nexception handling\\nabout 62\\nat right level of abstraction 64, 65\\nempty except blocks, avoiding 67\\noriginal exception, exploring 69\\ntracebacks exposure, avoiding 66\\n\\nF\\n\\nidioms for iteration\\nabout 193\\ncode, simplifying through iterators 197\\ngenerator, using 196\\nitertools 196\\nnext() function 195\\nindexes 26\\ninheritance\\nabout 77\\nanti-patterns 79, 81\\nbenefits 78\\ninterface 117\\ninterface segregation principle (ISP)\\nabout 117\\nguidelines 118, 119\\niterable objects\\nabout 40\\ncreating 40, 43\\nsequences, creating 43\\niteration\\nabout 193\\nidioms, using 193\\niterator pattern\\nin Python 200\\ninterface for iteration 200\\nsequence objects, using as iterables 201\\n\\nfunction arguments\\nand coupling 91\\n\\n[ 313 ]\\n\\n\\x0cK\\n\\nN\\n\\nKIS (Keep It Simple) 75\\n\\nname mangling 37\\nnon-data descriptors 163\\nnull object pattern 280\\n\\nL\\nLiskov\\'s substitution principle (LSP)\\nabout 110\\nissue detection, with tools 111\\nreviews 116\\nviolation, cases 113\\nLook Before You Leap (LBYL) 76\\nLSP issues\\nincompatible signatures, detecting with Pylint\\n113\\nincorrect datatypes, detecting in method\\nsignatures with Mypy 112\\n\\nO\\n\\nM\\nmagic methods 49\\nmakefiles 21\\nmethods, descriptor protocol\\n__delete__(self, instance) 159\\n__set__(self, instance, value) 158\\n__set_name__(self, owner, name) 161\\nabout 156\\nmethods, generator interface\\nabout 204\\nclose() method 204\\nsend(value) 206\\nthrow(ex_type[, ex_value[, ex_traceback]]) 205\\nmock objects\\nabout 238\\nusing 239\\nmock\\nabout 239, 240\\ntypes 240\\nmultiple inheritance, Python\\nMethod Resolution Order (MRO) 82\\nmixins 84\\nmutation testing 248\\nMypy\\nreference 20\\nused, for detecting incorrect datatypes in method\\nsignatures 111\\nused, for type hinting 20\\n\\nObject Relational Mapper SQLAlchemy (ORM\\nSQLAlchemy) 274\\nobjects\\nattributes 35\\ndynamic attributes 46\\nmethods 35\\nproperties 35, 38, 39\\nOnce and Only Once (OAOO) 72\\nopen/closed principle (OCP)\\nabout 103, 109\\nevents system, extending 107\\nevents system, refactoring for extensibility 105\\nmaintainability perils example 103, 105\\n\\nP\\npatching 239\\nPEP-8 characteristics\\ncode quality 12\\nconsistency 12\\ngrepability 11\\nPortable Operating System Interface (POSIX) 269\\nproduction code 246\\nproperty-based testing 248\\nPylint\\nused, for checking code 21\\nused, for detecting incompatible signatures 113\\npytest\\nabout 232\\nfixtures 235\\nparametrized tests 234\\nused, for writing test cases 233\\nPython\\nassertions, using 69\\ncaveats 49\\ndecorators, using 125\\ndescriptors, using 180\\ndesign patterns, considerations 254\\niterator pattern 200\\n\\n[ 314 ]\\n\\n\\x0cmultiple inheritance 82\\nprojects, reference 307\\nunderscores 35\\n\\nR\\nred-green-refactor cycle 251\\nrefactoring 244\\nripple effect 70\\n\\nS\\nScreaming Architecture\\nreference 307\\nseparation of concerns (SoC)\\nabout 70, 287, 294\\ncohesion 71\\ncoupling 71\\nsequences\\ncreating 28\\nsingle responsibility principle (SRP)\\nabout 99\\nclass, with multiple responsibilities 100\\nresponsibilities, distributing 102\\nslices 26\\nsoftware components\\ncontainers 294\\npackages 290, 292\\nsoftware design\\nbest practices 93\\nstructural patterns\\nabout 263\\nadapter pattern 263\\ncomposite pattern 264\\ndecorator pattern 266\\nfacade pattern 268\\n\\nT\\n\\ntest doubles\\nuse case 241\\ntest-driven development (TDD) 251\\ntestability 222\\ntesting 226\\ntesting boundaries\\ndefining 225\\ntesting\\ntools 226\\n\\nU\\nunit testing\\nabout 247\\nand agile software development 222\\nand software design 222\\nframeworks 226\\nlibraries 226\\nmutation testing 248\\nproperty-based testing 248\\npytest 232\\nunittest module 228\\nunittest module\\nabout 228\\nparametrized tests 230\\nuse case\\nabout 295\\nanalysis 304\\ncode reuse 296\\nservices 300, 303\\nuses, decorators\\nparameters, transforming 135\\n\\nW\\nwrapped object 125\\n\\nY\\nYAGNI (You Ain\\'t Gonna Need It) 74\\n\\n\\x0c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_from_book=text.decode(\"utf-8\")\n",
    "words_from_book.replace(\"\\n\",\" \")\n",
    "trysom=words_from_book\n",
    "words_from_book=words_from_book.split()\n",
    "f= open(\"trysom.txt\",\"w+\")\n",
    "f.write(trysom)\n",
    "f.close() \n",
    "for k in range(len(words_from_book)):\n",
    "    if not words_from_book[k].isalpha():\n",
    "        words_from_book[k]=\"\"\n",
    "    #words_from_book[k]=re.sub(\"[^a-zA-Z]+\", \"\", words_from_book[k])  \n",
    "    words_from_book[k]=words_from_book[k].lower()\n",
    "words_from_book = list(filter(None, words_from_book)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean',\n",
       " 'code',\n",
       " 'in',\n",
       " 'python',\n",
       " 'refactor',\n",
       " 'your',\n",
       " 'legacy',\n",
       " 'code',\n",
       " 'base',\n",
       " 'mariano',\n",
       " 'anaya',\n",
       " 'birmingham',\n",
       " 'mumbai',\n",
       " 'clean',\n",
       " 'code',\n",
       " 'in',\n",
       " 'python',\n",
       " 'copyright',\n",
       " 'packt',\n",
       " 'publishing',\n",
       " 'all',\n",
       " 'rights',\n",
       " 'no',\n",
       " 'part',\n",
       " 'of',\n",
       " 'this',\n",
       " 'book',\n",
       " 'may',\n",
       " 'be',\n",
       " 'stored',\n",
       " 'in',\n",
       " 'a',\n",
       " 'retrieval',\n",
       " 'or',\n",
       " 'transmitted',\n",
       " 'in',\n",
       " 'any',\n",
       " 'form',\n",
       " 'or',\n",
       " 'by',\n",
       " 'any',\n",
       " 'without',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'written',\n",
       " 'permission',\n",
       " 'of',\n",
       " 'the',\n",
       " 'except',\n",
       " 'in',\n",
       " 'the',\n",
       " 'case',\n",
       " 'of',\n",
       " 'brief',\n",
       " 'quotations',\n",
       " 'embedded',\n",
       " 'in',\n",
       " 'critical',\n",
       " 'articles',\n",
       " 'or',\n",
       " 'every',\n",
       " 'effort',\n",
       " 'has',\n",
       " 'been',\n",
       " 'made',\n",
       " 'in',\n",
       " 'the',\n",
       " 'preparation',\n",
       " 'of',\n",
       " 'this',\n",
       " 'book',\n",
       " 'to',\n",
       " 'ensure',\n",
       " 'the',\n",
       " 'accuracy',\n",
       " 'of',\n",
       " 'the',\n",
       " 'information',\n",
       " 'the',\n",
       " 'information',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'sold',\n",
       " 'without',\n",
       " 'either',\n",
       " 'express',\n",
       " 'or',\n",
       " 'neither',\n",
       " 'the',\n",
       " 'nor',\n",
       " 'packt',\n",
       " 'publishing',\n",
       " 'or',\n",
       " 'its',\n",
       " 'dealers',\n",
       " 'and',\n",
       " 'will',\n",
       " 'be',\n",
       " 'held',\n",
       " 'liable',\n",
       " 'for',\n",
       " 'any',\n",
       " 'damages',\n",
       " 'caused',\n",
       " 'or',\n",
       " 'alleged',\n",
       " 'to',\n",
       " 'have',\n",
       " 'been',\n",
       " 'caused',\n",
       " 'directly',\n",
       " 'or',\n",
       " 'indirectly',\n",
       " 'by',\n",
       " 'this',\n",
       " 'packt',\n",
       " 'publishing',\n",
       " 'has',\n",
       " 'endeavored',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'trademark',\n",
       " 'information',\n",
       " 'about',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'companies',\n",
       " 'and',\n",
       " 'products',\n",
       " 'mentioned',\n",
       " 'in',\n",
       " 'this',\n",
       " 'book',\n",
       " 'by',\n",
       " 'the',\n",
       " 'appropriate',\n",
       " 'use',\n",
       " 'of',\n",
       " 'packt',\n",
       " 'publishing',\n",
       " 'cannot',\n",
       " 'guarantee',\n",
       " 'the',\n",
       " 'accuracy',\n",
       " 'of',\n",
       " 'this',\n",
       " 'commissioning',\n",
       " 'merint',\n",
       " 'mathew',\n",
       " 'acquisition',\n",
       " 'denim',\n",
       " 'pinto',\n",
       " 'content',\n",
       " 'development',\n",
       " 'priyanka',\n",
       " 'sawant',\n",
       " 'technical',\n",
       " 'gaurav',\n",
       " 'gala',\n",
       " 'copy',\n",
       " 'safis',\n",
       " 'editing',\n",
       " 'project',\n",
       " 'vaidehi',\n",
       " 'sawant',\n",
       " 'safis',\n",
       " 'editing',\n",
       " 'rekha',\n",
       " 'nair',\n",
       " 'jason',\n",
       " 'monteiro',\n",
       " 'production',\n",
       " 'shantanu',\n",
       " 'zagade',\n",
       " 'first',\n",
       " 'august',\n",
       " 'production',\n",
       " 'published',\n",
       " 'by',\n",
       " 'packt',\n",
       " 'publishing',\n",
       " 'livery',\n",
       " 'place',\n",
       " 'livery',\n",
       " 'street',\n",
       " 'birmingham',\n",
       " 'isbn',\n",
       " 'to',\n",
       " 'my',\n",
       " 'family',\n",
       " 'and',\n",
       " 'for',\n",
       " 'their',\n",
       " 'unconditional',\n",
       " 'love',\n",
       " 'and',\n",
       " 'mariano',\n",
       " 'anaya',\n",
       " 'mapt',\n",
       " 'is',\n",
       " 'an',\n",
       " 'online',\n",
       " 'digital',\n",
       " 'library',\n",
       " 'that',\n",
       " 'gives',\n",
       " 'you',\n",
       " 'full',\n",
       " 'access',\n",
       " 'to',\n",
       " 'over',\n",
       " 'books',\n",
       " 'and',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'industry',\n",
       " 'leading',\n",
       " 'tools',\n",
       " 'to',\n",
       " 'help',\n",
       " 'you',\n",
       " 'plan',\n",
       " 'your',\n",
       " 'personal',\n",
       " 'development',\n",
       " 'and',\n",
       " 'advance',\n",
       " 'your',\n",
       " 'for',\n",
       " 'more',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'our',\n",
       " 'why',\n",
       " 'spend',\n",
       " 'less',\n",
       " 'time',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'more',\n",
       " 'time',\n",
       " 'coding',\n",
       " 'with',\n",
       " 'practical',\n",
       " 'ebooks',\n",
       " 'and',\n",
       " 'videos',\n",
       " 'from',\n",
       " 'over',\n",
       " 'industry',\n",
       " 'professionals',\n",
       " 'improve',\n",
       " 'your',\n",
       " 'learning',\n",
       " 'with',\n",
       " 'skill',\n",
       " 'plans',\n",
       " 'built',\n",
       " 'especially',\n",
       " 'for',\n",
       " 'you',\n",
       " 'get',\n",
       " 'a',\n",
       " 'free',\n",
       " 'ebook',\n",
       " 'or',\n",
       " 'video',\n",
       " 'every',\n",
       " 'month',\n",
       " 'mapt',\n",
       " 'is',\n",
       " 'fully',\n",
       " 'searchable',\n",
       " 'copy',\n",
       " 'and',\n",
       " 'and',\n",
       " 'bookmark',\n",
       " 'content',\n",
       " 'did',\n",
       " 'you',\n",
       " 'know',\n",
       " 'that',\n",
       " 'packt',\n",
       " 'offers',\n",
       " 'ebook',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'every',\n",
       " 'book',\n",
       " 'with',\n",
       " 'pdf',\n",
       " 'and',\n",
       " 'epub',\n",
       " 'files',\n",
       " 'you',\n",
       " 'can',\n",
       " 'upgrade',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ebook',\n",
       " 'version',\n",
       " 'at',\n",
       " 'and',\n",
       " 'as',\n",
       " 'a',\n",
       " 'print',\n",
       " 'book',\n",
       " 'you',\n",
       " 'are',\n",
       " 'entitled',\n",
       " 'to',\n",
       " 'a',\n",
       " 'discount',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ebook',\n",
       " 'get',\n",
       " 'in',\n",
       " 'touch',\n",
       " 'with',\n",
       " 'us',\n",
       " 'at',\n",
       " 'for',\n",
       " 'more',\n",
       " 'at',\n",
       " 'you',\n",
       " 'can',\n",
       " 'also',\n",
       " 'read',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'free',\n",
       " 'technical',\n",
       " 'sign',\n",
       " 'up',\n",
       " 'for',\n",
       " 'a',\n",
       " 'range',\n",
       " 'of',\n",
       " 'free',\n",
       " 'and',\n",
       " 'receive',\n",
       " 'exclusive',\n",
       " 'discounts',\n",
       " 'and',\n",
       " 'offers',\n",
       " 'on',\n",
       " 'packt',\n",
       " 'books',\n",
       " 'and',\n",
       " 'contributors',\n",
       " 'about',\n",
       " 'the',\n",
       " 'author',\n",
       " 'mariano',\n",
       " 'anaya',\n",
       " 'is',\n",
       " 'a',\n",
       " 'software',\n",
       " 'engineer',\n",
       " 'who',\n",
       " 'spends',\n",
       " 'most',\n",
       " 'of',\n",
       " 'his',\n",
       " 'time',\n",
       " 'creating',\n",
       " 'software',\n",
       " 'with',\n",
       " 'python',\n",
       " 'and',\n",
       " 'mentoring',\n",
       " 'fellow',\n",
       " 'main',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'interests',\n",
       " 'besides',\n",
       " 'python',\n",
       " 'are',\n",
       " 'software',\n",
       " 'functional',\n",
       " 'distributed',\n",
       " 'and',\n",
       " 'speaking',\n",
       " 'at',\n",
       " 'he',\n",
       " 'was',\n",
       " 'a',\n",
       " 'speaker',\n",
       " 'at',\n",
       " 'euro',\n",
       " 'python',\n",
       " 'and',\n",
       " 'to',\n",
       " 'know',\n",
       " 'more',\n",
       " 'about',\n",
       " 'you',\n",
       " 'can',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'his',\n",
       " 'github',\n",
       " 'account',\n",
       " 'with',\n",
       " 'the',\n",
       " 'username',\n",
       " 'his',\n",
       " 'speakerdeck',\n",
       " 'username',\n",
       " 'is',\n",
       " 'about',\n",
       " 'the',\n",
       " 'reviewer',\n",
       " 'nimesh',\n",
       " 'kiran',\n",
       " 'verma',\n",
       " 'has',\n",
       " 'a',\n",
       " 'dual',\n",
       " 'degree',\n",
       " 'in',\n",
       " 'maths',\n",
       " 'and',\n",
       " 'computing',\n",
       " 'from',\n",
       " 'iit',\n",
       " 'delhi',\n",
       " 'and',\n",
       " 'has',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'companies',\n",
       " 'such',\n",
       " 'as',\n",
       " 'paytm',\n",
       " 'and',\n",
       " 'icici',\n",
       " 'for',\n",
       " 'about',\n",
       " 'years',\n",
       " 'in',\n",
       " 'software',\n",
       " 'development',\n",
       " 'and',\n",
       " 'data',\n",
       " 'he',\n",
       " 'a',\n",
       " 'upwards',\n",
       " 'fintech',\n",
       " 'and',\n",
       " 'presently',\n",
       " 'serves',\n",
       " 'as',\n",
       " 'its',\n",
       " 'he',\n",
       " 'loves',\n",
       " 'coding',\n",
       " 'and',\n",
       " 'has',\n",
       " 'mastery',\n",
       " 'in',\n",
       " 'python',\n",
       " 'and',\n",
       " 'its',\n",
       " 'popular',\n",
       " 'django',\n",
       " 'and',\n",
       " 'he',\n",
       " 'extensively',\n",
       " 'leverages',\n",
       " 'amazon',\n",
       " 'web',\n",
       " 'design',\n",
       " 'sql',\n",
       " 'and',\n",
       " 'nosql',\n",
       " 'to',\n",
       " 'build',\n",
       " 'scalable',\n",
       " 'and',\n",
       " 'low',\n",
       " 'latency',\n",
       " 'to',\n",
       " 'my',\n",
       " 'nutan',\n",
       " 'kiran',\n",
       " 'who',\n",
       " 'made',\n",
       " 'me',\n",
       " 'what',\n",
       " 'i',\n",
       " 'am',\n",
       " 'today',\n",
       " 'and',\n",
       " 'gave',\n",
       " 'the',\n",
       " 'confidence',\n",
       " 'to',\n",
       " 'pursue',\n",
       " 'all',\n",
       " 'my',\n",
       " 'thanks',\n",
       " 'and',\n",
       " 'who',\n",
       " 'motivated',\n",
       " 'me',\n",
       " 'to',\n",
       " 'steal',\n",
       " 'time',\n",
       " 'for',\n",
       " 'this',\n",
       " 'book',\n",
       " 'when',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'i',\n",
       " 'was',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'spending',\n",
       " 'it',\n",
       " 'with',\n",
       " 'ulhas',\n",
       " 'and',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'packt',\n",
       " 'support',\n",
       " 'was',\n",
       " 'thanks',\n",
       " 'varsha',\n",
       " 'shetty',\n",
       " 'for',\n",
       " 'introducing',\n",
       " 'me',\n",
       " 'to',\n",
       " 'packt',\n",
       " 'is',\n",
       " 'searching',\n",
       " 'for',\n",
       " 'authors',\n",
       " 'like',\n",
       " 'you',\n",
       " 'if',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'becoming',\n",
       " 'an',\n",
       " 'author',\n",
       " 'for',\n",
       " 'please',\n",
       " 'visit',\n",
       " 'and',\n",
       " 'apply',\n",
       " 'we',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'developers',\n",
       " 'and',\n",
       " 'tech',\n",
       " 'just',\n",
       " 'like',\n",
       " 'to',\n",
       " 'help',\n",
       " 'them',\n",
       " 'share',\n",
       " 'their',\n",
       " 'insight',\n",
       " 'with',\n",
       " 'the',\n",
       " 'global',\n",
       " 'tech',\n",
       " 'you',\n",
       " 'can',\n",
       " 'make',\n",
       " 'a',\n",
       " 'general',\n",
       " 'apply',\n",
       " 'for',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'hot',\n",
       " 'topic',\n",
       " 'that',\n",
       " 'we',\n",
       " 'are',\n",
       " 'recruiting',\n",
       " 'an',\n",
       " 'author',\n",
       " 'or',\n",
       " 'submit',\n",
       " 'your',\n",
       " 'own',\n",
       " 'table',\n",
       " 'of',\n",
       " 'contents',\n",
       " 'preface',\n",
       " 'chapter',\n",
       " 'code',\n",
       " 'and',\n",
       " 'tools',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'clean',\n",
       " 'code',\n",
       " 'the',\n",
       " 'importance',\n",
       " 'of',\n",
       " 'having',\n",
       " 'clean',\n",
       " 'code',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'code',\n",
       " 'formatting',\n",
       " 'in',\n",
       " 'clean',\n",
       " 'code',\n",
       " 'adhering',\n",
       " 'to',\n",
       " 'a',\n",
       " 'coding',\n",
       " 'style',\n",
       " 'guide',\n",
       " 'on',\n",
       " 'your',\n",
       " 'project',\n",
       " 'docstrings',\n",
       " 'and',\n",
       " 'annotations',\n",
       " 'docstrings',\n",
       " 'annotations',\n",
       " 'do',\n",
       " 'annotations',\n",
       " 'replace',\n",
       " 'configuring',\n",
       " 'the',\n",
       " 'tools',\n",
       " 'for',\n",
       " 'enforcing',\n",
       " 'basic',\n",
       " 'quality',\n",
       " 'gates',\n",
       " 'type',\n",
       " 'hinting',\n",
       " 'with',\n",
       " 'mypy',\n",
       " 'checking',\n",
       " 'the',\n",
       " 'code',\n",
       " 'with',\n",
       " 'pylint',\n",
       " 'setup',\n",
       " 'for',\n",
       " 'automatic',\n",
       " 'checks',\n",
       " 'summary',\n",
       " 'chapter',\n",
       " 'pythonic',\n",
       " 'code',\n",
       " 'indexes',\n",
       " 'and',\n",
       " 'slices',\n",
       " 'creating',\n",
       " 'your',\n",
       " 'own',\n",
       " 'sequences',\n",
       " 'context',\n",
       " 'managers',\n",
       " 'implementing',\n",
       " 'context',\n",
       " 'managers',\n",
       " 'and',\n",
       " 'different',\n",
       " 'types',\n",
       " 'of',\n",
       " 'methods',\n",
       " 'for',\n",
       " 'objects',\n",
       " 'underscores',\n",
       " 'in',\n",
       " 'python',\n",
       " 'properties',\n",
       " 'iterable',\n",
       " 'objects',\n",
       " 'creating',\n",
       " 'iterable',\n",
       " 'objects',\n",
       " 'creating',\n",
       " 'sequences',\n",
       " 'container',\n",
       " 'objects',\n",
       " 'dynamic',\n",
       " 'attributes',\n",
       " 'for',\n",
       " 'objects',\n",
       " 'callable',\n",
       " 'objects',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'magic',\n",
       " 'methods',\n",
       " 'caveats',\n",
       " 'in',\n",
       " 'python',\n",
       " 'mutable',\n",
       " 'default',\n",
       " 'arguments',\n",
       " 'extending',\n",
       " 'types',\n",
       " 'summary',\n",
       " 'references',\n",
       " 'chapter',\n",
       " 'general',\n",
       " 'traits',\n",
       " 'of',\n",
       " 'good',\n",
       " 'code',\n",
       " 'design',\n",
       " 'by',\n",
       " 'contract',\n",
       " 'table',\n",
       " 'of',\n",
       " 'contents',\n",
       " 'preconditions',\n",
       " 'postconditions',\n",
       " 'pythonic',\n",
       " 'contracts',\n",
       " 'design',\n",
       " 'by',\n",
       " 'contract',\n",
       " 'conclusions',\n",
       " 'defensive',\n",
       " 'programming',\n",
       " 'error',\n",
       " 'handling',\n",
       " 'value',\n",
       " 'substitution',\n",
       " 'exception',\n",
       " 'handling',\n",
       " 'handle',\n",
       " 'exceptions',\n",
       " 'at',\n",
       " 'the',\n",
       " 'right',\n",
       " 'level',\n",
       " 'of',\n",
       " 'abstraction',\n",
       " 'do',\n",
       " 'not',\n",
       " 'expose',\n",
       " 'tracebacks',\n",
       " 'avoid',\n",
       " 'empty',\n",
       " 'except',\n",
       " 'blocks',\n",
       " 'include',\n",
       " 'the',\n",
       " 'original',\n",
       " 'exception',\n",
       " 'using',\n",
       " 'assertions',\n",
       " 'in',\n",
       " 'python',\n",
       " 'separation',\n",
       " 'of',\n",
       " 'concerns',\n",
       " 'cohesion',\n",
       " 'and',\n",
       " 'coupling',\n",
       " 'acronyms',\n",
       " 'to',\n",
       " 'live',\n",
       " 'by',\n",
       " 'yagni',\n",
       " 'kis',\n",
       " 'composition',\n",
       " 'and',\n",
       " 'inheritance',\n",
       " 'when',\n",
       " 'inheritance',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'decision',\n",
       " 'for',\n",
       " 'inheritance',\n",
       " 'multiple',\n",
       " 'inheritance',\n",
       " 'in',\n",
       " 'python',\n",
       " 'method',\n",
       " 'resolution',\n",
       " 'order',\n",
       " 'mixins',\n",
       " 'arguments',\n",
       " 'in',\n",
       " 'functions',\n",
       " 'and',\n",
       " 'methods',\n",
       " 'how',\n",
       " 'function',\n",
       " 'arguments',\n",
       " 'work',\n",
       " 'in',\n",
       " 'python',\n",
       " 'how',\n",
       " 'arguments',\n",
       " 'are',\n",
       " 'copied',\n",
       " 'to',\n",
       " 'functions',\n",
       " 'variable',\n",
       " 'number',\n",
       " 'of',\n",
       " 'arguments',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'arguments',\n",
       " 'in',\n",
       " 'functions',\n",
       " 'function',\n",
       " 'arguments',\n",
       " 'and',\n",
       " 'coupling',\n",
       " 'compact',\n",
       " 'function',\n",
       " 'signatures',\n",
       " 'that',\n",
       " 'take',\n",
       " 'too',\n",
       " 'many',\n",
       " 'arguments',\n",
       " 'final',\n",
       " 'remarks',\n",
       " 'on',\n",
       " 'good',\n",
       " 'practices',\n",
       " 'for',\n",
       " 'software',\n",
       " 'design',\n",
       " 'orthogonality',\n",
       " 'in',\n",
       " 'software',\n",
       " 'structuring',\n",
       " 'the',\n",
       " 'code',\n",
       " 'summary',\n",
       " 'references',\n",
       " 'chapter',\n",
       " 'the',\n",
       " 'solid',\n",
       " 'principles',\n",
       " 'single',\n",
       " 'responsibility',\n",
       " 'principle',\n",
       " 'a',\n",
       " 'class',\n",
       " 'with',\n",
       " 'too',\n",
       " 'many',\n",
       " 'responsibilities',\n",
       " 'distributing',\n",
       " 'responsibilities',\n",
       " 'the',\n",
       " 'principle',\n",
       " 'example',\n",
       " 'of',\n",
       " 'maintainability',\n",
       " 'perils',\n",
       " 'for',\n",
       " 'not',\n",
       " 'following',\n",
       " 'the',\n",
       " 'principle',\n",
       " 'refactoring',\n",
       " 'the',\n",
       " 'events',\n",
       " 'system',\n",
       " 'for',\n",
       " 'extensibility',\n",
       " 'extending',\n",
       " 'the',\n",
       " 'events',\n",
       " 'system',\n",
       " 'ii',\n",
       " 'table',\n",
       " 'of',\n",
       " 'contents',\n",
       " 'final',\n",
       " 'thoughts',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ocp',\n",
       " 'substitution',\n",
       " 'principle',\n",
       " 'detecting',\n",
       " 'lsp',\n",
       " 'issues',\n",
       " 'with',\n",
       " 'tools',\n",
       " 'detecting',\n",
       " 'incorrect',\n",
       " 'datatypes',\n",
       " 'in',\n",
       " 'method',\n",
       " 'signatures',\n",
       " 'with',\n",
       " 'mypy',\n",
       " 'detecting',\n",
       " 'incompatible',\n",
       " 'signatures',\n",
       " 'with',\n",
       " 'pylint',\n",
       " 'more',\n",
       " 'subtle',\n",
       " 'cases',\n",
       " 'of',\n",
       " 'lsp',\n",
       " 'violations',\n",
       " 'remarks',\n",
       " 'on',\n",
       " 'the',\n",
       " 'lsp',\n",
       " 'interface',\n",
       " 'segregation',\n",
       " 'an',\n",
       " 'interface',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'too',\n",
       " 'much',\n",
       " 'the',\n",
       " 'smaller',\n",
       " 'the',\n",
       " 'the',\n",
       " 'better',\n",
       " 'how',\n",
       " 'small',\n",
       " 'should',\n",
       " 'an',\n",
       " 'interface',\n",
       " 'dependency',\n",
       " 'inversion',\n",
       " 'a',\n",
       " 'case',\n",
       " 'of',\n",
       " 'rigid',\n",
       " 'dependencies',\n",
       " 'inverting',\n",
       " 'the',\n",
       " 'dependencies',\n",
       " 'summary',\n",
       " 'references',\n",
       " 'chapter',\n",
       " 'using',\n",
       " 'decorators',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'our',\n",
       " 'code',\n",
       " 'what',\n",
       " 'are',\n",
       " 'decorators',\n",
       " 'in',\n",
       " 'decorate',\n",
       " 'functions',\n",
       " 'decorate',\n",
       " 'classes',\n",
       " 'other',\n",
       " 'types',\n",
       " 'of',\n",
       " 'decorator',\n",
       " 'passing',\n",
       " 'arguments',\n",
       " 'to',\n",
       " 'decorators',\n",
       " 'decorators',\n",
       " 'with',\n",
       " 'nested',\n",
       " 'functions',\n",
       " 'decorator',\n",
       " 'objects',\n",
       " 'good',\n",
       " 'uses',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_from_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79443\n",
      "100\n",
      "3690\n",
      "['suitable', 'sent', 'intend', 'trouble', 'catching', 'destination', 'reconstruct', 'enforced', 'activated', 'self', 'operator', 'compromising', 'rarely', 'numbersequence', 'miss', 'forgets', 'status', 'back', 'deduce', 'collector', 'blind', 'fields', 'luxury', 'willing', 'why', 'rewritten', 'fintech', 'feed', 'extract', 'explain', 'imposes', 'included', 'speakerdeck', 'maintained', 'bound', 'tend', 'four', 'take', 'assuming', 'realizing', 'setattr', 'contextmanager', 'certainly', 'reduce', 'util', 'gone', 'variations', 'transparently', 'mastering', 'interval', 'post', 'reduces', 'incrementally', 'mock', 'academic', 'configuration', 'collaborator', 'err', 'upwards', 'decorated', 'classification', 'detour', 'effective', 'mind', 'wrong', 'tdd', 'taxonomies', 'demonstrate', 'forgetting', 'matches', 'dependency', 'predict', 'sequences', 'forth', 'way', 'async', 'prove', 'clause', 'underlying', 'jumping', 'relationship', 'very', 'saying', 'correspond', 'prices', 'ror', 'usernamelookup', 'extending', 'position', 'rejected', 'carried', 'followed', 'numerous', 'former', 'distributed', 'heuristics', 'multiple', 'monteiro', 'tests', 'callers', 'constraints', 'copyright', 'decision', 'unchanged', 'customization', 'u', 'sincerely', 'demonstrative', 'move', 'unconditional', 'preinstalled', 'choice', 'today', 'features', 'verbose', 'worthy', 'established', 'happen', 'entitled', 'hidefield', 'assertraisesregex', 'responses', 'subclasses', 'collect', 'preparation', 'precedence', 'execute', 'try', 'curl', 'days', 'toward', 'filenotfounderror', 'ideas', 'room', 'password', 'relies', 'street', 'http', 'come', 'over', 'leap', 'gather', 'diabolical', 'sets', 'resume', 'proficient', 'callable', 'error', 'off', 'largest', 'riskier', 'outcome', 'did', 'analogy', 'float', 'encapsulation', 'respects', 'parsed', 'satisfies', 'poses', 'trace', 'hire', 'disappear', 'makes', 'causes', 'w', 'frameworks', 'rules', 'exited', 'accuracy', 'nonpythonic', 'value', 'perform', 'devised', 'stopped', 'independently', 'joining', 'nimesh', 'refactor', 'assumption', 'basics', 'hence', 'differences', 'recurrent', 'principal', 'dependent', 'thought', 'inherited', 'fetch', 'sense', 'exploiting', 'comments', 'lacks', 'evaluation', 'configurable', 'minutes', 'rightful', 'person', 'objects', 'does', 'decoration', 'strictly', 'parse', 'rebinding', 'even', 'goal', 'checklist', 'statistics', 'prone', 'erich', 'pythonic', 'affect', 'buildstatus', 'root', 'issues', 'behavior', 'released', 'runtime', 'model', 'math', 'artifacts', 'category', 'returns', 'happening', 'clarifying', 'resort', 'cohesive', 'clarify', 'relating', 'inadvertent', 'risk', 'reviews', 'holding', 'raw', 'serve', 'assertions', 'anticipating', 'facilitates', 'recommended', 'print', 'begs', 'aim', 'parser', 'upgrade', 'construct', 'programs', 'coupled', 'early', 'patterns', 'composed', 'versus', 'exaggerated', 'wrapper', 'seq', 'break', 'notification', 'stated', 'terminates', 'def', 'pretty', 'logical', 'sub', 'essential', 'typing', 'robustness', 'deserves', 'significantly', 'diamond', 'return', 'engage', 'request', 'rationale', 'resolves', 'announcing', 'k', 'accept', 'asserts', 'transmitted', 'decisions', 'csv', 'redirecting', 'changing', 'plan', 'popular', 'searching', 'encountered', 'publishing', 'engineers', 'concrete', 'installed', 'deliberately', 'sql', 'discover', 'thoughts', 'characteristics', 'reflect', 'suspension', 'serving', 'scream', 'words', 'answer', 'seen', 'pragmatically', 'raised', 'solved', 'modifying', 'careful', 'story', 'level', 'wrapt', 'catch', 'mainly', 'reuse', 'assumed', 'unzip', 'somewhere', 'armed', 'misleadingly', 'intention', 're', 'suit', 'impose', 'contents', 'implementational', 'benefits', 'bear', 'duplicate', 'claim', 'account', 'correctness', 'closure', 'index', 'second', 'linter', 'behind', 'messages', 'contained', 'thousands', 'hierarchy', 'exactly', 'caveat', 'besides', 'embrace', 'heavily', 'designs', 'passwords', 'normalizing', 'speaks', 'asynchronous', 'loves', 'applied', 'forms', 'consist', 'composite', 'typical', 'abuse', 'modify', 'mapper', 'internally', 'conceptual', 'circular', 'recipe', 'triggers', 'make', 'doing', 'arguments', 'copies', 'effort', 'putting', 'complicate', 'nicest', 'mask', 'differ', 'strategically', 'changes', 'overhead', 'idioms', 'builders', 'minus', 'conducted', 'lambda', 'everywhere', 'votes', 'detailed', 'stars', 'inverse', 'warn', 'email', 'adopting', 'going', 'invariant', 'span', 'david', 'discusses', 'communication', 'resembling', 'importance', 'expensive', 'tool', 'reasons', 'actually', 'concretely', 'placing', 'defaultdict', 'preparing', 'decouple', 'signaling', 'sometimes', 'proposes', 'manufacturing', 'record', 'agile', 'recognizing', 'iv', 'satellite', 'alongside', 'comes', 'countless', 'lessons', 'motivations', 'separating', 'easiest', 'end', 'received', 'inconsistencies', 'interfaces', 'apis', 'redis', 'defective', 'unlike', 'accepts', 'implements', 'opposite', 'singleton', 'yielding', 'indexed', 'blog', 'counts', 'closed', 'delegated', 'desired', 'arrival', 'extension', 'specific', 'per', 'semantic', 'ending', 'used', 'combinations', 'naturally', 'wrap', 'analyzers', 'nt', 'enable', 'excluded', 'configured', 'square', 'grid', 'determining', 'proper', 'simplified', 'epub', 'setter', 'clearer', 'validated', 'syslog', 'releasing', 'finding', 'compliance', 'handler', 'wonder', 'bring', 'file', 'notifications', 'assigning', 'consequences', 'special', 'talk', 'call', 'assertionerror', 'users', 'pep', 'merge', 'blocks', 'load', 'entire', 'allowed', 'road', 'forward', 'delay', 'rename', 'secondary', 'opens', 'focused', 'observation', 'disagrees', 'reused', 'counting', 'acts', 'samples', 'reusable', 'property', 'derives', 'regardless', 'distinguish', 'concurrency', 'virtual', 'reopened', 'forcing', 'came', 'patching', 'prior', 'fair', 'forget', 'include', 'cleaner', 'counter', 'execution', 'correcting', 'perhaps', 'remember', 'feature', 'deeply', 'game', 'alive', 'belongs', 'survive', 'onscreen', 'worth', 'unbiased', 'exceptional', 'userunknown', 'cases', 'inspecting', 'creational', 'standing', 'afford', 'realize', 'verification', 'mathew', 'produces', 'quickly', 'binary', 'experiment', 'feel', 'express', 'interests', 'deco', 'areas', 'ignored', 'enum', 'sufficient', 'someone', 'beautiful', 'folder', 'displayed', 'temporal', 'main', 'five', 'duplication', 'arrive', 'orthogonality', 'students', 'cheat', 'referred', 'emerge', 'hello', 'transition', 'contain', 'safis', 'average', 'addresses', 'exclusively', 'tactics', 'mistake', 'types', 'shape', 'le', 'ourselves', 'callback', 'cls', 'stopiteration', 'losing', 'distinguishes', 'entails', 'logout', 'perils', 'attention', 'invalid', 'documented', 'opaque', 'basis', 'case', 'internals', 'visible', 'default', 'chosen', 'nosql', 'ulhas', 'companies', 'probably', 'asked', 'sentence', 'craftsmanship', 'favor', 'woolf', 'mutually', 'threads', 'metrics', 'demand', 'interacting', 'libraries', 'alternative', 'decorate', 'manipulating', 'them', 'microservice', 'bigger', 'total', 'whole', 'security', 'typehint', 'bottom', 'focus', 'drew', 'enterprise', 'software', 'xmleventparser', 'estimation', 'replicated', 'steps', 'mrstatus', 'becomes', 'jeopardize', 'prefer', 'mocks', 'quacks', 'pursue', 'star', 'described', 'requested', 'derived', 'simplify', 'pattern', 'query', 'handle', 'discovering', 'presently', 'subcoroutine', 'testability', 'simple', 'built', 'interaction', 'registered', 'aspects', 'disadvantage', 'opt', 'marcus', 'integration', 'revisit', 'pull', 'appears', 'select', 'implicit', 'communicate', 'gets', 'nightmare', 'actions', 'customers', 'elif', 'good', 'nicer', 'toplevel', 'point', 'idiomatic', 'elevate', 'windows', 'beginning', 'illustrating', 'descriptor', 'signal', 'cast', 'correct', 'refer', 'debugger', 'associate', 'dbc', 'pick', 'unreliability', 'implies', 'fewer', 'deterministic', 'jargon', 'labeling', 'respective', 'manual', 'append', 'video', 'worry', 'prevent', 'holds', 'face', 'guidelines', 'step', 'condition', 'imported', 'swallow', 'tell', 'sounds', 'redirected', 'validate', 'declarative', 'around', 'acceptance', 'working', 'connecting', 'vaidehi', 'repeatedly', 'attempt', 'production', 'expertise', 'lieu', 'writing', 'emphasizing', 'delegates', 'away', 'stands', 'maximum', 'decide', 'microservices', 'controlled', 'liskov', 'empty', 'crp', 'developed', 'interacts', 'popularized', 'simpler', 'both', 'speaker', 'refers', 'manager', 'leak', 'eafp', 'bypass', 'machinery', 'escape', 'happened', 'private', 'got', 'bad', 'little', 'recreate', 'consumes', 'logic', 'looked', 'mandatory', 'aside', 'composition', 'abstract', 'numbers', 'treated', 'driver', 'study', 'rather', 'serves', 'customer', 'connects', 'selecting', 'transactionalpolicy', 'ebook', 'took', 'pythonistas', 'informative', 'usually', 'decides', 'definite', 'intersection', 'enough', 'considered', 'accepting', 'somewhat', 'against', 'core', 'systemmonitor', 'however', 'forgiveness', 'ii', 'matters', 'malicious', 'insights', 'replaces', 'walks', 'registering', 'instruction', 'enumeration', 'addition', 'analyzes', 'ignoring', 'distinctions', 'management', 'parametrize', 'paid', 'anomaly', 'logger', 'application', 'explanation', 'solely', 'address', 'initializing', 'hinting', 'pypi', 'semantics', 'honest', 'descriptorclass', 'long', 'adapters', 'terms', 'generally', 'state', 'notifying', 'remaining', 'directly', 'scattered', 'purchasing', 'interpreter', 'content', 'diagram', 'link', 'accomplish', 'orm', 'recipes', 'glimpse', 'composable', 'responding', 'portrayed', 'minimizes', 'offers', 'acquire', 'terrible', 'proportion', 'test', 'streamer', 'eliminate', 'convention', 'instantiated', 'resonating', 'else', 'determine', 'honors', 'simply', 'indepth', 'init', 'asyncio', 'partially', 'modern', 'known', 'shares', 'connection', 'still', 'imagine', 'mocking', 'hosted', 'union', 'noticing', 'extend', 'separately', 'sit', 'bias', 'unmet', 'separation', 'runs', 'added', 'lack', 'structured', 'biggest', 'contributed', 'reducing', 'creation', 'performance', 'attempts', 'website', 'performed', 'isolation', 'beazley', 'limitations', 'amount', 'throwing', 'row', 'markers', 'briefly', 'provides', 'policy', 'install', 'calculating', 'strings', 'iit', 'alter', 'forbid', 'httperror', 'smallest', 'journeyman', 'bazaar', 'kill', 'studying', 'farther', 'wish', 'vectors', 'spaces', 'constraint', 'experienced', 'chess', 'methods', 'determined', 'longer', 'section', 'consecutive', 'comparison', 'relation', 'number', 'purchases', 'dictionary', 'articles', 'iterates', 'notified', 'itself', 'covers', 'cmd', 'unreasonable', 'sound', 'coroutine', 'ignore', 'wants', 'delivery', 'waste', 'handy', 'livery', 'parts', 'tried', 'cities', 'because', 'aor', 'disjoint', 'essence', 'enhancing', 'docstrings', 'violating', 'type', 'loops', 'love', 'could', 'preferable', 'novice', 'pretend', 'wished', 'qualified', 'adhering', 'entering', 'parallelizing', 'requirements', 'failure', 'totals', 'enlightening', 'into', 'must', 'considerable', 'mere', 'being', 'functionalities', 'stored', 'gaurav', 'receiving', 'costs', 'telling', 'applies', 'rewrite', 'denoting', 'lines', 'considering', 'introduces', 'attribute', 'practitioners', 'iteration', 'subtypes', 'tells', 'substitution', 'changed', 'parameter', 'interesting', 'suggest', 'convert', 'slices', 'ones', 'inconsistent', 'absolutely', 'executed', 'crucial', 'corresponding', 'maintain', 'formatting', 'least', 'overshadowed', 'boxes', 'globally', 'delivering', 'alternatives', 'taxes', 'life', 'goes', 'throughout', 'nested', 'intellectual', 'safely', 'theory', 'meaning', 'del', 'inherits', 'reification', 'outer', 'flag', 'contributing', 'appropriate', 'dual', 'meanings', 'emphasizes', 'rigid', 'replaced', 'idly', 'matching', 'comparisons', 'reasonably', 'occurred', 'defining', 'theoretical', 'transitions', 'satisfy', 'partitions', 'wherever', 'referring', 'metaclasses', 'workdir', 'descriptors', 'down', 'order', 'returned', 'maintainable', 'deciding', 'retrievable', 'si', 'paying', 'altered', 'referenced', 'acquisition', 'remove', 'managers', 'computations', 'clicking', 'trust', 'spending', 'decimal', 'persist', 'transport', 'acting', 'ways', 'postconditions', 'batch', 'became', 'o', 'check', 'notify', 'illustrate', 'hub', 'mentioned', 'times', 'printf', 'things', 'awaitable', 'correctly', 'vary', 'minimize', 'restrictions', 'postcondition', 'classes', 'continuous', 'requirement', 'abc', 'abusing', 'discussed', 'rigidity', 'ad', 'table', 'initialization', 'potentially', 'higher', 'obtain', 'populated', 'rich', 'resulting', 'team', 'proceeds', 'mixins', 'dependencies', 'rare', 'understandable', 'supporting', 'tied', 'remotely', 'convenient', 'broader', 'unix', 'illustration', 'programmer', 'causing', 'mentoring', 'streaming', 'thus', 'steve', 'enforce', 'change', 'identical', 'suspend', 'flaw', 'concentration', 'unless', 'dumped', 'truth', 'ask', 'explains', 'implementing', 'fairly', 'decoupling', 'emerges', 'modification', 'passes', 'thinking', 'thank', 'iterated', 'widely', 'claims', 'unique', 'gathered', 'store', 'granular', 'portraying', 'conference', 'normalization', 'switch', 'coming', 'choose', 'clear', 'occur', 'entities', 'lapse', 'fitting', 'date', 'carrying', 'silence', 'display', 'adapting', 'describes', 'range', 'mimicking', 'premise', 'inverting', 'disable', 'mitigation', 'practices', 'desirable', 'resource', 'serializing', 'presence', 'enhancedquerydecorator', 'emphasis', 'bundle', 'focuses', 'str', 'logineventserializer', 'liability', 'dialog', 'technology', 'share', 'reserved', 'fragility', 'accurate', 'blindly', 'enhance', 'repeated', 'cleaning', 'current', 'foundations', 'emphasized', 'abstracts', 'sooner', 'discussion', 'wrapped', 'register', 'processed', 'adds', 'divided', 'context', 'fails', 'summarized', 'videos', 'paused', 'killed', 'greater', 'instance', 'delimiting', 'high', 'isp', 'tree', 'screaming', 'via', 'remarks', 'thanks', 'conform', 'assimilate', 'indexes', 'freezing', 'strong', 'contributors', 'arguably', 'comply', 'inform', 'doctest', 'produce', 'improve', 'multiply', 'endpoint', 'expanding', 'signatures', 'gained', 'minimal', 'adhere', 'bug', 'returning', 'meant', 'inheritance', 'lead', 'student', 'stats', 'replace', 'among', 'answered', 'interested', 'major', 'representation', 'inputs', 'created', 'workflow', 'inconvenience', 'proxying', 'near', 'completely', 'ultimately', 'pairs', 'different', 'standards', 'pragmatic', 'kiss', 'richard', 'expected', 'fundamentally', 'indicate', 'text', 'many', 'evidence', 'efforts', 'decades', 'solution', 'shortcut', 'dynamic', 'quick', 'directory', 'drawn', 'situations', 'subtlety', 'sensitive', 'andrew', 'testable', 'multithreading', 'declaring', 'real', 'checking', 'itertools', 'relatively', 'dynamically', 'list', 'remained', 'constructs', 'responsibilities', 'interpolation', 'recruiting', 'exploring', 'possible', 'acronym', 'compile', 'calling', 'allow', 'larger', 'purposes', 'force', 'statements', 'simulated', 'decided', 'keeps', 'piece', 'difference', 'principles', 'corrective', 'work', 'installing', 'preserve', 'programmers', 'true', 'makefile', 'closures', 'facing', 'annotating', 'determines', 'event', 'constructing', 'reveals', 'importing', 'assert', 'approaches', 'responsibility', 'effected', 'invoked', 'suggests', 'responsible', 'ranked', 'id', 'propagated', 'vice', 'retrieve', 'same', 'examining', 'hunt', 'collections', 'models', 'traits', 'underneath', 'isolated', 'caveats', 'fundamental', 'opposed', 'evolve', 'titled', 'every', 'introduction', 'injection', 'existing', 'mutate', 'streamed', 'importantly', 'apk', 'reusing', 'failed', 'width', 'short', 'names', 'complying', 'inherit', 'chained', 'opinion', 'chose', 'depends', 'violation', 'container', 'avoiding', 'outlines', 'classify', 'protocols', 'git', 'scale', 'apptool', 'expression', 'src', 'language', 'oriented', 'easier', 'particularities', 'dict', 'centralizing', 'backup', 'helpful', 'assigned', 'accordingly', 'less', 'conventions', 'purchase', 'yielded', 'confuse', 'best', 'tokens', 'unittest', 'proof', 'result', 'safest', 'commented', 'gathering', 'iterator', 'consistent', 'show', 'weaken', 'journey', 'ever', 'agent', 'setuptools', 'alone', 'explanations', 'original', 'transactions', 'detecting', 'follow', 'yet', 'development', 'remain', 'past', 'facades', 'incorrectly', 'surmount', 'round', 'combination', 'acronyms', 'observed', 'aimed', 'triggering', 'anything', 'ensures', 'carry', 'ipython', 'saving', 'evolved', 'realized', 'arrange', 'providing', 'powers', 'august', 'before', 'enforcing', 'capable', 'adapter', 'heuristic', 'grep', 'assign', 'isolating', 'unaware', 'caution', 'protocol', 'might', 'exercise', 'vi', 'please', 'kinds', 'settling', 'output', 'whatever', 'lint', 'concern', 'sqlalchemy', 'ruins', 'hoc', 'latest', 'damages', 'json', 'emailed', 'confusion', 'require', 'failures', 'deeper', 'coined', 'composing', 'comprehend', 'bought', 'assume', 'decoupled', 'personal', 'pass', 'debugging', 'dropped', 'respond', 'consistency', 'brian', 'necessary', 'accepted', 'selective', 'amazon', 'response', 'except', 'picture', 'specification', 'predictable', 'resolved', 'sight', 'fixture', 'runners', 'cleanup', 'extensible', 'policies', 'modified', 'qualities', 'audit', 'stop', 'shared', 'protect', 'commands', 'experience', 'vars', 'accurately', 'worst', 'ability', 'works', 'supertype', 'believe', 'operators', 'cursor', 'permission', 'behaves', 'become', 'consuming', 'transforming', 'speak', 'identify', 'ease', 'handling', 'outside', 'altering', 'shetty', 'yourself', 'encourage', 'reviewing', 'extends', 'producing', 'building', 'includes', 'entirely', 'allows', 'reveal', 'concise', 'dataclasses', 'facts', 'dealers', 'saves', 'propose', 'automating', 'dictionaries', 'executing', 'define', 'spends', 'replicating', 'violations', 'gap', 'community', 'behavioral', 'enables', 'superfluous', 'results', 'designed', 'tasks', 'taking', 'stressed', 'comprehensive', 'allowing', 'earlier', 'solve', 'poor', 'relying', 'limits', 'offer', 'mypy', 'whichever', 'lsp', 'relate', 'distinctively', 'existence', 'factories', 'updated', 'ides', 'across', 'explicit', 'sentinel', 'falling', 'collide', 'parameters', 'insight', 'scenario', 'variants', 'mergerequeststate', 'pieces', 'actual', 'infinite', 'denotes', 'simplest', 'recognize', 'abstraction', 'fix', 'hidden', 'converted', 'goals', 'urls', 'map', 'decorator', 'assessment', 'grouped', 'copied', 'naming', 'reach', 'simplifies', 'optionally', 'declares', 'space', 'generator', 'strategies', 'fail', 'effects', 'complies', 'input', 'another', 'confident', 'care', 'argument', 'invoke', 'assignment', 'consumer', 'm', 'images', 'styles', 'smell', 'target', 'lot', 'rocks', 'listed', 'summing', 'achieving', 'signals', 'matter', 'musings', 'recall', 'those', 'plain', 'indirectly', 'implemented', 'word', 'euro', 'full', 'once', 'debug', 'found', 'component', 'database', 'proved', 'linearization', 'compatibility', 'packaged', 'misunderstood', 'answers', 'simplifying', 'bunch', 'tips', 'maths', 'initially', 'performing', 'queryenhancer', 'tech', 'dramatically', 'distributing', 'highest', 'ensure', 'preconditions', 'erroneous', 'anticipate', 'states', 'attainment', 'maybe', 'unknown', 'clientclass', 'api', 'delve', 'mixin', 'owns', 'evaluate', 'factors', 'compared', 'tk', 'lists', 'consumed', 'want', 'registers', 'improperly', 'led', 'grepability', 'column', 'processor', 'readable', 'plans', 'thing', 'principle', 'negative', 'needless', 'converting', 'directories', 'readme', 'birmingham', 'reason', 'links', 'userloginevent', 'improved', 'paper', 'middle', 'comprehensively', 'traversal', 'pillars', 'continuity', 'linux', 'downside', 'transparent', 'normally', 'readability', 'dbdriver', 'dangerous', 'files', 'labeled', 'idea', 'implicitly', 'strive', 'raise', 'datetime', 'arg', 'containing', 'introducing', 'tools', 'loop', 'internal', 'repetitive', 'figure', 'products', 'defensive', 'restriction', 'preserved', 'properly', 'helps', 'innermost', 'structural', 'serialization', 'entries', 'several', 'adapt', 'marker', 'opportunity', 'rewind', 'suddenly', 'independent', 'approach', 'corrected', 'lat', 'transform', 'judge', 'accessible', 'ninja', 'people', 'highlevel', 'essays', 'dockerfile', 'loginevent', 'problematic', 'iterative', 'mention', 'compact', 'prints', 'subtle', 'optional', 'y', 'ranges', 'printed', 'syntax', 'lesson', 'loaded', 'author', 'covered', 'gcc', 'compose', 'stress', 'numbering', 'expressions', 'golang', 'improving', 'reflected', 'enjoy', 'eliminating', 'specify', 'slip', 'quoting', 'handlers', 'ultimate', 'while', 'operation', 'sort', 'perception', 'similar', 'moving', 'patter', 'car', 'developer', 'consideration', 'pdf', 'obtained', 'lucky', 'invoking', 'answering', 'save', 'containers', 'mix', 'versatile', 'solutions', 'science', 'indentation', 'industry', 'broad', 'script', 'attributeerror', 'latency', 'securely', 'magic', 'previously', 'lazy', 'intent', 'constants', 'interchanging', 'raymond', 'zoom', 'backward', 'approvals', 'session', 'packing', 'errata', 'cn', 'ip', 'relevant', 'delegate', 'activityreader', 'shown', 'customexception', 'extreme', 'proposed', 'kiran', 'side', 'undoing', 'maps', 'parties', 'superclass', 'events', 'hints', 'critical', 'keeping', 'intended', 'respect', 'translated', 'favored', 'overriding', 'utmost', 'corresponds', 'stakeholders', 'glance', 'selected', 'installs', 'leveraging', 'recommend', 'option', 'plenty', 'yields', 'coding', 'rekha', 'books', 'levels', 'connectionerror', 'repeatable', 'finally', 'exceptions', 'd', 'cache', 'omitted', 'clearest', 'capabilities', 'finished', 'dot', 'defines', 'secure', 'depended', 'compare', 'implement', 'clients', 'debt', 'nonredundancy', 'expresses', 'path', 'specialization', 'possibilities', 'interact', 'threshold', 'invite', 'voting', 'wasting', 'normalize', 'tricks', 'exposing', 'parenthesis', 'eyes', 'resolution', 'substituting', 'two', 'suggested', 'mumbai', 'structuring', 'visited', 'connector', 'guide', 'varsha', 'linting', 'editing', 'players', 'secret', 'applying', 'orchestrate', 'guarantees', 'filters', 'nor', 'push', 'connect', 'definitely', 'statuses', 'afterward', 'cohesion', 'meaningful', 'arranged', 'roughly', 'productive', 'develop', 'flagging', 'replicate', 'unknownuser', 'conveniently', 'highlighted', 'subscriptable', 'hard', 'unreachable', 'robert', 'layout', 'gracefully', 'analogies', 'schema', 'closest', 'duplicated', 'powerful', 'justifies', 'flow', 'xml', 'establish', 'equals', 'relates', 'isbn', 'environment', 'despite', 'dare', 'indexing', 'derive', 'newer', 'motivation', 'suppose', 'zagade', 'stack', 'leading', 'cc', 'trigger', 'encapsulate', 'hiding', 'productivity', 'famous', 'mistakes', 'accomplished', 'weakref', 'discussing', 'machine', 'cons', 'modifier', 'certain', 'alleged', 'add', 'complement', 'gang', 'smells', 'leverages', 'hexagonal', 'vaguely', 'welcome', 'merint', 'criterion', 'uses', 'checked', 'message', 'image', 'incorrect', 'formulas', 'effectively', 'applicable', 'write', 'opinionated', 'organization', 'singletons', 'abstractions', 'workable', 'artifact', 'join', 'embedded', 'spend', 'subclass', 'endless', 'problem', 'incurring', 'location', 'accumulated', 'aforementioned', 'privileges', 'stick', 'sequence', 'vote', 'adopts', 'traversed', 'interpret', 'exact', 'flatting', 'demonstrated', 'views', 'encouraging', 'tightly', 'detected', 'particularity', 'puts', 'subsections', 'keyword', 'poorly', 'nair', 'analyzer', 'quite', 'rule', 'filtered', 'arranging', 'annotate', 'style', 'price', 'remark', 'iii', 'lie', 'aod', 'regression', 'verified', 'inside', 'refactored', 'repetition', 'much', 'year', 'maintainability', 'again', 'translates', 'gates', 'owner', 'architecture', 'disagree', 'know', 'roots', 'formattime', 'explore', 'understanding', 'specialize', 'automate', 'retrieving', 'huge', 'analyzed', 'wheel', 'set', 'adding', 'missing', 'eventually', 'sign', 'initial', 'modifications', 'sections', 'cover', 'function', 'give', 'sharing', 'giving', 'immutable', 'explored', 'penalization', 'suitability', 'errors', 'month', 'aligned', 'expecting', 'totally', 'measure', 'part', 'open', 'technical', 'closer', 'generic', 'loading', 'deliveryorder', 'through', 'official', 'array', 'subgenerators', 'entity', 'formats', 'likely', 'wake', 'suspending', 'wondering', 'manipulate', 'twice', 'supported', 'override', 'unify', 'extracting', 'series', 'storing', 'scope', 'binding', 'ends', 'memory', 'discovered', 'handles', 'brings', 'productbundle', 'originally', 'values', 'question', 'invest', 'scheduling', 'each', 'trusting', 'accidental', 'element', 'complicated', 'points', 'contract', 'term', 'passing', 'computation', 'making', 'tremendously', 'name', 'masters', 'engineer', 'disadvantages', 'submit', 'server', 'kis', 'implementation', 'entered', 'explanatory', 'experts', 'skill', 'signature', 'delegation', 'flexible', 'strict', 'reassign', 'familiar', 'reserve', 'particularly', 'helper', 'alice', 'inspiring', 'github', 'trivial', 'edge', 'becoming', 'bare', 'jump', 'seeing', 'manner', 'hope', 'barbara', 'raising', 'trait', 'combining', 'surely', 'practice', 'according', 'conclusions', 'endeavored', 'communicating', 'catalog', 'queries', 'converts', 'minimum', 'cathedral', 'customary', 'indirect', 'modules', 'packt', 'viable', 'revealed', 'n', 'daniel', 'avg', 'creating', 'polymorphism', 'flags', 'follows', 'design', 'based', 'inspection', 'started', 'syntactically', 'notes', 'f', 'packaging', 'run', 'items', 'leaving', 'fits', 'lowest', 'asking', 't', 'consume', 'shall', 'conceived', 'shantanu', 'method', 'carefully', 'encourages', 'groups', 'confidence', 'reinvent', 'valid', 'version', 'detail', 'makefiles', 'regard', 'slot', 'improves', 'big', 'libs', 'tolerant', 'conn', 'visit', 'therefore', 'enforcement', 'incredibly', 'newly', 'junit', 'support', 'family', 'typed', 'controlling', 'declare', 'summarize', 'tackles', 'options', 'library', 'suffer', 'kind', 'although', 'coupling', 'happens', 'divide', 'resemblance', 'martin', 'bugs', 'parent', 'zero', 'fixed', 'mr', 'unambiguously', 'triggered', 'split', 'leaves', 'versions', 'symbol', 'fallacies', 'soon', 'extended', 'info', 'available', 'desire', 'expressing', 'components', 'let', 'height', 'submission', 'calculate', 'polymorphic', 'hot', 'avid', 'trying', 'pays', 'maintaining', 'possibility', 'vlissides', 'caller', 'substate', 'sawant', 'calculates', 'resumed', 'none', 'automatically', 'valuable', 'invokes', 'generate', 'h', 'involving', 'right', 'metric', 'administration', 'diagnose', 'due', 'bobby', 'parametrized', 'structure', 'designing', 'obligation', 'note', 'clue', 'broken', 'interface', 'arithmetic', 'upsides', 'transpires', 'statement', 'pytests', 'statusweb', 'notice', 'creates', 'customizing', 'marked', 'odd', 'storage', 'maintains', 'establishes', 'configuring', 'ci', 'leaky', 'demanding', 'os', 'considerably', 'code', 'plays', 'guaranteed', 'data', 'nothing', 'edition', 'addressing', 'safe', 'layers', 'wrote', 'match', 'logging', 'malformed', 'anticipated', 'launching', 'slowly', 'improvements', 'depend', 'advantage', 'raises', 'compute', 'pointing', 'neither', 'shipped', 'connections', 'undesirable', 'performs', 'offender', 'identified', 'widespread', 'alias', 'inception', 'reader', 'exam', 'net', 'scaffold', 'rows', 'demands', 'practical', 'getting', 'consider', 'mechanisms', 'top', 'serializer', 'decorators', 'resources', 'harder', 'verma', 'lives', 'exercised', 'checks', 'system', 'group', 'asks', 'left', 'taken', 'abstracted', 'black', 'mixed', 'green', 'traced', 'delegating', 'talking', 'queried', 'traverses', 'trademark', 'removing', 'mirror', 'named', 'forces', 'understood', 'improvement', 'clean', 'testing', 'expressed', 'contribute', 'floating', 'highly', 'figuring', 'brackets', 'couple', 'indicating', 'domain', 'mark', 'reviewer', 'docker', 'opinions', 'engineering', 'gain', 'upon', 'fetching', 'read', 'difficult', 'under', 'report', 'learning', 'constantly', 'cycle', 'agreed', 'setting', 'intermediate', 'efficient', 'computer', 'occurrence', 'significant', 'looks', 'otherwise', 'pages', 'sitting', 'static', 'issuing', 'better', 'assess', 'merged', 'structures', 'investigate', 'deal', 'protecting', 'assumptions', 'expressive', 'introduced', 'asynchronously', 'fingerprint', 'suffix', 'authentication', 'semantically', 'allocated', 'definitions', 'apart', 'balance', 'copy', 'grouping', 'describe', 'nobody', 'occurs', 'silently', 'bstatus', 'enhanced', 'technically', 'sheet', 'steady', 'inversion', 'thomas', 'methodology', 'immediately', 'leads', 'blueprints', 'attackers', 'datatypes', 'deriving', 'clearly', 'int', 'resulted', 'adherence', 'precondition', 'compatible', 'annotation', 'dummy', 'layer', 'saw', 'restrictive', 'orthogonal', 'borg', 'platform', 'explode', 'relational', 'authors', 'hand', 'co', 'sending', 'reads', 'flexibility', 'revolutionary', 'variables', 'whenever', 'transformed', 'universally', 'filtering', 'product', 'stating', 'various', 'multiples', 'fully', 'sdist', 'leave', 'clarification', 'recommendations', 'generators', 'fortunate', 'had', 'factory', 'rightfully', 'usefulness', 'currently', 'sure', 'advice', 'complexity', 'suite', 'paths', 'richer', 'sugar', 'occasionally', 'consistently', 'packed', 'start', 'achieved', 'shifted', 'expose', 'published', 'particular', 'gonna', 'made', 'specifies', 'insert', 'r', 'achieve', 'controlledexception', 'carries', 'explained', 'honor', 'entail', 'loader', 'definition', 'waiting', 'jason', 'turns', 'mentions', 'invert', 'starting', 'references', 'track', 'indication', 'notoriously', 'direct', 'obstacle', 'instead', 'meyer', 'beyond', 'refactoring', 'chapters', 'jeopardizing', 'seem', 'scrapping', 'derivative', 'document', 'score', 'external', 'naive', 'towards', 'compliant', 'unpack', 'receive', 'mangling', 'representing', 'recalling', 'projects', 'sake', 'boolean', 'interpreted', 'gitfetcher', 'eventstreamer', 'coi', 'brevity', 'reference', 'module', 'reuses', 'reappear', 'managing', 'build', 'formalizing', 'foundation', 'well', 'engine', 'years', 'plugins', 'computes', 'rest', 'proceed', 'velocity', 'identifying', 'understand', 'wraps', 'knows', 'drawback', 'spot', 'hyphens', 'means', 'declared', 'sole', 'revolve', 'recent', 'breaks', 'peculiar', 'moment', 'stage', 'eric', 'capture', 'constant', 'affected', 'exercises', 'ago', 'inject', 'feedback', 'showing', 'autodoc', 'srp', 'comment', 'namely', 'third', 'vast', 'retrieval', 'functional', 'where', 'pycodestyle', 'though', 'intervals', 'registry', 'able', 'letting', 'decode', 'achieves', 'affecting', 'operating', 'create', 'conditions', 'handled', 'showoriginal', 'dictquery', 'hide', 'gala', 'entailing', 'smaller', 'mechanism', 'increasing', 'accidentally', 'involve', 'needs', 'driven', 'exception', 'implementations', 'identifier', 'safety', 'segregation', 'distinction', 'boilerplate', 'chaining', 'dividing', 'justify', 'without', 'substituted', 'isolate', 'branches', 'including', 'aspect', 'contains', 'nutan', 'duck', 'previous', 'spare', 'harms', 'automated', 'supplier', 'replacing', 'concepts', 'transitioning', 'important', 'unexpected', 'slow', 'predicate', 'statically', 'few', 'ordernotfounderror', 'investment', 'held', 'commonly', 'maintenance', 'coincidence', 'present', 'timedelta', 'novices', 'successful', 'incompatibilities', 'begin', 'aws', 'memorization', 'progress', 'formally', 'conversion', 'empirical', 'encounter', 'form', 'slightly', 'caused', 'loses', 'presenting', 'indices', 'wellknown', 'mapped', 'avoids', 'walking', 'illegal', 'reprise', 'really', 'literal', 'ide', 'auxiliary', 'examine', 'power', 'flat', 'transformation', 'resolving', 'patch', 'antipattern', 'simulate', 'tracing', 'portable', 'targets', 'externally', 'enforces', 'calls', 'fragments', 'g', 'ratio', 'python', 'gives', 'learned', 'wide', 'variable', 'excluding', 'places', 'lose', 'specifically', 'retries', 'mutable', 'prefixed', 'suppress', 'initialize', 'sorts', 'subject', 'turn', 'valueerror', 'wisely', 'invisible', 'close', 'stopping', 'underscores', 'manages', 'objective', 'islice', 'memorize', 'fault', 'takeaway', 'agree', 'units', 'constitutes', 'encapsulates', 'realistic', 'practicality', 'extensively', 'safer', 'incurred', 'consequence', 'task', 'obverse', 'nice', 'operations', 'impact', 'influence', 'staticmethod', 'exclude', 'too', 'motivated', 'properties', 'contextlib', 'organizing', 'petition', 'elegant', 'scenarios', 'asssertraises', 'authority', 'project', 'conceptually', 'workings', 'gave', 'solves', 'inflexibility', 'object', 'precisely', 'builder', 'nature', 'advantages', 'cody', 'computing', 'driving', 'cpython', 'impacts', 'l', 'everyone', 'yield', 'ebooks', 'late', 'schedule', 'basically', 'unintended', 'revisited', 'quality', 'surroundings', 'process', 'cod', 'integrated', 'just', 'estimate', 'automatic', 'criteria', 'cause', 'until', 'fellow', 'render', 'assemble', 'downloads', 'considerations', 'emerged', 'wrapping', 'extra', 'elegantly', 'look', 'problems', 'client', 'practically', 'defend', 'purchased', 'template', 'solid', 'favors', 'philosophy', 'passed', 'quoted', 'depending', 'cosmetic', 'bundles', 'bl', 'world', 'noticed', 'somehow', 'analogous', 'myclass', 'keyerror', 'explores', 'unlikely', 'yagni', 'opted', 'alert', 'instructions', 'redirect', 'icici', 'b', 'damage', 'posix', 'languages', 'perfectly', 'login', 'brief', 'illustrated', 'raison', 'example', 'preceding', 'investing', 'after', 'encapsulating', 'equivalent', 'daterangesequence', 'defer', 'fixtures', 'vital', 'impossible', 'exists', 'corrects', 'mindset', 'argue', 'apparently', 'wait', 'functions', 'latitude', 'robust', 'belong', 'requesting', 'assure', 'done', 'digital', 'v', 'thumb', 'user', 'whether', 'hands', 'administrative', 'parameterize', 'further', 'obvious', 'measuring', 'crux', 'addressed', 'ralph', 'listing', 'paragraph', 'exempt', 'soup', 'public', 'command', 'beats', 'then', 'issue', 'resolve', 'exchange', 'small', 'failing', 'align', 'printing', 'steal', 'repeating', 'culprit', 'remembered', 'package', 'supports', 'rights', 'summary', 'role', 'joys', 'monostate', 'later', 'impression', 'guarantee', 'download', 'celery', 'harm', 'kwargs', 'procedures', 'custom', 'enrich', 'additional', 'older', 'legacy', 'zen', 'mapt', 'needing', 'useful', 'constructed', 'tag', 'honored', 'exposed', 'rail', 'zip', 'asserting', 'produced', 'latter', 'exceedingly', 'underscore', 'separates', 'following', 'fancy', 'comparatively', 'below', 'expect', 'always', 'timeout', 'functools', 'having', 'rely', 'collaborates', 'hypothesis', 'manually', 'reporting', 'generating', 'transaction', 'inverted', 'iterate', 'framework', 'dsq', 'consists', 'elements', 'explicitly', 'configure', 'conditional', 'legal', 'advanced', 'truly', 'scalar', 'syntactic', 'tempted', 'mergerequeststatus', 'dbrows', 'mean', 'majority', 'convenience', 'provide', 'specified', 'producer', 'releases', 'slots', 'differentiates', 'fallback', 'throw', 'successive', 'bind', 'complex', 'delivered', 'constitute', 'routine', 'launch', 'equally', 'said', 'tries', 'string', 'builtin', 'removed', 'generically', 'construction', 'lbyl', 'doubles', 'represented', 'requiring', 'future', 'mapping', 'learn', 'leader', 'extensibility', 'appear', 'readers', 'branch', 'compartmentalization', 'reasoning', 'line', 'front', 'increases', 'propagation', 'aiming', 'keys', 'lies', 'varying', 'sessions', 'think', 'fall', 'never', 'serialize', 'proven', 'launches', 'imply', 'iterables', 'syntaxerror', 'critically', 'reading', 'dos', 'solving', 'parallelisms', 'double', 'she', 'cell', 'program', 'canonical', 'involved', 'within', 'downloaded', 'pinto', 'format', 'packages', 'coincidentally', 'either', 'moved', 'day', 'pip', 'strategy', 'await', 'control', 'partial', 'admin', 'accessed', 'potential', 'ready', 'incompatible', 'whereas', 'onto', 'differentiate', 'truer', 'complementary', 'null', 'translate', 'broke', 'mechanics', 'needed', 'often', 'mnemonic', 'preserving', 'party', 'convoluted', 'shows', 'enters', 'stmts', 'book', 'running', 'applications', 'live', 'looking', 'benefit', 'configurations', 'discrepancy', 'retrying', 'iterations', 'defects', 'deliver', 'pros', 'confused', 'sum', 'commissioning', 'jackson', 'says', 'paytm', 'advisable', 'mutant', 'suffers', 'say', 'deployed', 'conclude', 'standardize', 'propagate', 'greatest', 'setups', 'caught', 'tuples', 'resemble', 'liable', 'examples', 'mcconnell', 'weaker', 'delete', 'resembles', 'classical', 'closely', 'magicmock', 'cloud', 'developing', 'records', 'mariano', 'approved', 'whose', 'coords', 'dealing', 'interpretation', 'meet', 'preventing', 'monolithic', 'enjoyed', 'descriptive', 'especially', 'nth', 'actionable', 'indexerror', 'recovered', 'enhances', 'members', 'professionals', 'p', 'classmethod', 'pylint', 'play', 'envisioning', 'worked', 'remote', 'welldefined', 'mutating', 'encouraged', 'box', 'defends', 'knowing', 'offending', 'dyn', 'silent', 'already', 'placed', 'represent', 'db', 'reflects', 'documents', 'basic', 'treat', 'stores', 'themselves', 'fist', 'mergerequest', 'separated', 'scalable', 'easy', 'highlight', 'charge', 'basehttprequesthandler', 'fd', 'frequently', 'logoutevent', 'interpreters', 'protects', 'age', 'together', 'kernel', 'explaining', 'centralized', 'low', 'easily', 'worrying', 'logs', 'hid', 'traceback', 'combined', 'given', 'discuss', 'devise', 'three', 'max', 'advance', 'helped', 'most', 'slice', 'review', 'peculiarities', 'validation', 'mentioning', 'others', 'delayed', 'field', 'heavyweight', 'adaptable', 'adopt', 'attempting', 'separate', 'chances', 'receives', 'disclose', 'fact', 'general', 'should', 'guideline', 'ripple', 'purpose', 'lay', 'timestamp', 'own', 'dsn', 'upside', 'action', 'positions', 'validations', 'fine', 'regular', 'utterly', 'global', 'regards', 'indeed', 'plus', 'subtype', 'large', 'organize', 'stricter', 'indirection', 'simplification', 'parsing', 'sensitivity', 'pending', 'abstracting', 'spots', 'log', 'iso', 'collisions', 'resuming', 'faced', 'grateful', 'tracebacks', 'retrieves', 'monitored', 'perspective', 'machines', 'single', 'algorithm', 'invented', 'abandoned', 'revealing', 'far', 'programming', 'iterable', 'distracting', 'key', 'installation', 'covering', 'sometime', 'organized', 'equality', 'docstring', 'traveler', 'silver', 'called', 'act', 'generatorexit', 'send', 'setdefault', 'hierarchies', 'complete', 'capability', 'acceptable', 'breaking', 'relied', 'notions', 'activities', 'moves', 'platforms', 'searchable', 'toll', 'bookmark', 'loads', 'risks', 'effect', 'priority', 'traditional', 'retry', 'topics', 'john', 'paradigm', 'faulty', 'postgresql', 'weakkeydictionary', 'situation', 'kept', 'circumstances', 'title', 'aware', 'adapted', 'integrate', 'methodtype', 'pay', 'release', 'item', 'userlist', 'presented', 'associated', 'validating', 'operate', 'preface', 'coord', 'formal', 'need', 'successor', 'discount', 'modifies', 'temporary', 'tie', 'interprets', 'coverage', 'linked', 'runner', 'pypy', 'emphasize', 'great', 'editor', 'constructions', 'grasp', 'continuing', 'namespace', 'username', 'metadata', 'tested', 'formula', 'beneficial', 'exclusive', 'concerns', 'dispatched', 'pair', 'ongoing', 'transformations', 'findings', 'overall', 'course', 'instances', 'evolving', 'portability', 'remains', 'mixing', 'standard', 'dedicated', 'length', 'menus', 'mastery', 'pytest', 'sold', 'related', 'details', 'warnings', 'finishing', 'familiarized', 'dataclass', 'verify', 'runtimeerror', 'requests', 'simplehttprequesthandler', 'drop', 'suspended', 'wise', 'anaya', 'cost', 'contracts', 'repeat', 'showed', 'extensions', 'go', 'avoid', 'fit', 'monitoring', 'techniques', 'contrast', 'access', 'imports', 'enter', 'detect', 'reliability', 'compiler', 'brand', 'functionality', 'glass', 'unmodified', 'setup', 'activity', 'repository', 'lost', 'closing', 'requires', 'class', 'limit', 'compacting', 'memorized', 'assertion', 'systems', 'between', 'blends', 'quotations', 'unfortunate', 'serverless', 'cannot', 'mutation', 'challenge', 'required', 'limited', 'credentials', 'weak', 'usage', 'article', 'perceptions', 'unit', 'provided', 'typically', 'facade', 'direction', 'subtleties', 'accessing', 'retrieved', 'dates', 'luck', 'unpacked', 'dataset', 'guess', 'hint', 'generates', 'idiom', 'starts', 'apply', 'expects', 'put', 'during', 'update', 'worse', 'hold', 'using', 'documenting', 'injected', 'idiomatically', 'lots', 'contributes', 'base', 'import', 'almost', 'involves', 'accomplishing', 'excellent', 'anyone', 'draw', 'decorating', 'django', 'similarly', 'phrase', 'analyze', 'fake', 'dry', 'thorough', 'restart', 'edit', 'attrs', 'traces', 'technique', 'coin', 'indicators', 'tracking', 'demonstrating', 'overcome', 'touch', 'distinctive', 'transactional', 'afterwards', 'accommodate', 'block', 'along', 'defined', 'discounts', 'boundaries', 'something', 'continue', 'defect', 'creep', 'rearranged', 'seems', 'last', 'delhi', 'wanted', 'encapsulated', 'grow', 'exist', 'pennington', 'clarifications', 'legitimately', 'annotations', 'successfully', 'misconception', 'fixing', 'proceeding', 'necessarily', 'keep', 'repeats', 'body', 'acquainted', 'reduced', 'iterating', 'issued', 'evolves', 'cpu', 'enumerate', 'distribute', 'nameko', 'twitter', 'rearranging', 'lightweight', 'handbook', 'standardized', 'monitor', 'analyzing', 'uniform', 'went', 'speaking', 'tickets', 'prefix', 'sorted', 'represents', 'ocp', 'supposed', 'invariants', 'overridden', 'undesired', 'topic', 'source', 'questions', 'denim', 'capturing', 'declaration', 'countries', 'simulating', 'chance', 'final', 'sideeffects', 'developers', 'finish', 'processing', 'local', 'unpacking', 'bit', 'arena', 'mutants', 'written', 'since', 'bertrand', 'chapter', 'efficiently', 'bullets', 'knowledge', 'stream', 'positional', 'subtest', 'illustrates', 'tokenizer', 'utilities', 'indicates', 'gl', 'priyanka', 'ended', 'tackling', 'coroutines', 'interactive', 'warning', 'such', 'next', 'pertaining', 'entry', 'chain', 'discussions', 'place', 'analysis', 'dbstring', 'attributes', 'leverage', 'mbranch', 'deep', 'everything', 'mimic', 'degree', 'exc', 'pure', 'mac', 'ought', 'regarding', 'documentation', 'mostly', 'collection', 'common', 'iterators', 'concept', 'merely', 'false', 'simplicity', 'dosctring', 'takes', 'minimizing']\n"
     ]
    }
   ],
   "source": [
    "print(len(words_from_book))\n",
    "print(len(common_words))\n",
    "uncommon_words=set(words_from_book)-set(common_words)\n",
    "uncommon_words=list(uncommon_words)\n",
    "print(len(uncommon_words))\n",
    "\n",
    "print(uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for n in range(len(uncommon_words)):\n",
    "#    try:\n",
    "        \n",
    "        if words_from_book.count(uncommon_words[n-k])<30:\n",
    "            #print(words_from_book.count(uncommon_words[n]))    \n",
    "            del uncommon_words[n-k]\n",
    "            k=k+1\n",
    "                \n",
    "#except IndexError:\n",
    "#break\n",
    "\n",
    "print(len(uncommon_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "['why', 'take', 'mock', 'mind', 'way', 'very', 'multiple', 'tests', 'try', 'ideas', 'over', 'error', 'makes', 'value', 'objects', 'does', 'even', 'pythonic', 'behavior', 'returns', 'patterns', 'def', 'return', 'request', 'second', 'behind', 'applied', 'make', 'doing', 'arguments', 'changes', 'going', 'actually', 'implements', 'used', 'file', 'call', 'cases', 'main', 'types', 'case', 'default', 'probably', 'them', 'software', 'pattern', 'simple', 'good', 'point', 'descriptor', 'correct', 'writing', 'both', 'logic', 'abstract', 'rather', 'application', 'long', 'state', 'test', 'still', 'imagine', 'methods', 'number', 'dictionary', 'parts', 'because', 'type', 'could', 'into', 'must', 'being', 'attribute', 'iteration', 'parameter', 'descriptors', 'order', 'check', 'things', 'classes', 'dependencies', 'change', 'clear', 'context', 'instance', 'improve', 'inheritance', 'created', 'different', 'expected', 'many', 'solution', 'list', 'possible', 'calling', 'principles', 'work', 'event', 'same', 'traits', 'every', 'easier', 'best', 'result', 'original', 'before', 'might', 'pass', 'except', 'works', 'handling', 'define', 'parameters', 'abstraction', 'decorator', 'generator', 'another', 'lot', 'those', 'once', 'want', 'thing', 'principle', 'reason', 'idea', 'raise', 'tools', 'internal', 'while', 'similar', 'magic', 'shown', 'events', 'exceptions', 'implement', 'two', 'uses', 'write', 'problem', 'inside', 'much', 'architecture', 'know', 'explore', 'set', 'function', 'part', 'generic', 'through', 'values', 'each', 'making', 'name', 'implementation', 'creating', 'design', 'run', 'method', 'version', 'library', 'components', 'trying', 'none', 'structure', 'interface', 'notice', 'code', 'data', 'advantage', 'getting', 'decorators', 'system', 'named', 'clean', 'testing', 'domain', 'under', 'better', 'generators', 'sure', 'start', 'particular', 'achieve', 'instead', 'refactoring', 'external', 'module', 'rest', 'understand', 'means', 'where', 'able', 'create', 'needs', 'exception', 'without', 'previous', 'concepts', 'important', 'really', 'python', 'too', 'object', 'yield', 'process', 'just', 'extra', 'look', 'problems', 'client', 'solid', 'passed', 'example', 'after', 'functions', 'user', 'then', 'later', 'following', 'expect', 'always', 'having', 'provide', 'line', 'think', 'program', 'book', 'say', 'examples', 'already', 'most', 'fact', 'general', 'should', 'own', 'single', 'programming', 'iterable', 'called', 'need', 'standard', 'details', 'avoid', 'functionality', 'class', 'between', 'cannot', 'unit', 'apply', 'using', 'base', 'import', 'defined', 'something', 'keep', 'written', 'since', 'chapter', 'coroutines', 'such', 'next', 'place', 'attributes', 'everything', 'common']\n"
     ]
    }
   ],
   "source": [
    "print(len(uncommon_words))\n",
    "print(uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('code', 653), ('class', 424), ('def', 414), ('chapter', 371), ('object', 332), ('return', 282), ('python', 245), ('function', 238), ('design', 230), ('should', 229), ('good', 223), ('want', 218), ('objects', 211), ('method', 202), ('just', 198), ('using', 191), ('might', 187), ('because', 170), ('make', 169), ('unit', 164), ('create', 158), ('way', 148), ('following', 147), ('need', 147), ('different', 146), ('decorators', 146), ('something', 146), ('software', 142), ('value', 141), ('same', 141), ('idea', 141), ('methods', 139), ('each', 135), ('such', 135), ('decorator', 132), ('them', 128), ('descriptors', 128), ('tests', 127), ('then', 123), ('means', 121), ('patterns', 120), ('into', 120), ('data', 119), ('descriptor', 116), ('testing', 109), ('even', 108), ('common', 108), ('could', 106), ('work', 103), ('previous', 103), ('run', 101), ('call', 99), ('going', 98), ('type', 98), ('part', 98), ('logic', 97), ('test', 94), ('creating', 94), ('another', 93), ('much', 93), ('pattern', 92), ('being', 92), ('many', 92), ('most', 92), ('clean', 91), ('where', 91), ('functions', 89), ('take', 88), ('actually', 88), ('used', 88), ('particular', 88), ('case', 87), ('does', 86), ('two', 86), ('general', 86), ('example', 85), ('generators', 84), ('order', 82), ('values', 82), ('exception', 82), ('interface', 81), ('multiple', 80), ('change', 80), ('tools', 78), ('name', 78), ('without', 78), ('know', 77), ('instance', 76), ('original', 75), ('while', 75), ('application', 74), ('attribute', 73), ('possible', 73), ('result', 73), ('those', 73), ('look', 73), ('called', 68), ('classes', 67), ('implement', 67), ('yield', 67), ('after', 67), ('base', 67), ('main', 65), ('context', 65), ('generator', 65), ('implementation', 64), ('event', 63), ('problem', 63), ('notice', 63), ('better', 63), ('defined', 63), ('types', 62), ('important', 62), ('principles', 61), ('principle', 61), ('over', 60), ('things', 60), ('traits', 59), ('parameters', 58), ('able', 58), ('every', 57), ('since', 57), ('define', 56), ('client', 56), ('very', 55), ('pass', 55), ('instead', 55), ('having', 55), ('set', 54), ('makes', 53), ('raise', 53), ('import', 53), ('keep', 53), ('arguments', 52), ('apply', 52), ('pythonic', 51), ('explore', 51), ('already', 51), ('simple', 50), ('before', 50), ('through', 50), ('version', 49), ('getting', 49), ('place', 49), ('easier', 48), ('once', 48), ('always', 48), ('examples', 48), ('single', 48), ('dictionary', 47), ('refactoring', 47), ('think', 47), ('inside', 46), ('system', 46), ('cannot', 46), ('error', 45), ('parts', 45), ('write', 45), ('line', 45), ('point', 44), ('iteration', 44), ('improve', 44), ('architecture', 44), ('programming', 44), ('details', 44), ('file', 43), ('check', 43), ('magic', 43), ('module', 43), ('book', 43), ('solution', 42), ('why', 41), ('changes', 41), ('must', 41), ('works', 41), ('making', 41), ('next', 41), ('probably', 40), ('rather', 40), ('number', 40), ('parameter', 40), ('created', 40), ('start', 40), ('rest', 40), ('understand', 40), ('really', 40), ('too', 40), ('fact', 40), ('iterable', 40), ('cases', 39), ('abstract', 39), ('still', 39), ('attributes', 39), ('long', 38), ('list', 38), ('none', 38), ('external', 38), ('solid', 38), ('own', 38), ('avoid', 38), ('except', 37), ('thing', 37), ('trying', 37), ('named', 37), ('say', 37), ('coroutines', 37), ('second', 36), ('default', 36), ('correct', 36), ('generic', 36), ('provide', 36), ('ideas', 35), ('state', 35), ('calling', 35), ('events', 35), ('uses', 35), ('between', 35), ('try', 34), ('applied', 34), ('doing', 34), ('both', 34), ('expected', 34), ('handling', 34), ('reason', 34), ('components', 34), ('under', 34), ('extra', 34), ('passed', 34), ('standard', 34), ('behavior', 33), ('best', 33), ('abstraction', 33), ('exceptions', 33), ('structure', 33), ('sure', 33), ('user', 33), ('expect', 33), ('functionality', 33), ('everything', 33), ('request', 32), ('behind', 32), ('implements', 32), ('shown', 32), ('advantage', 32), ('achieve', 32), ('needs', 32), ('concepts', 32), ('problems', 32), ('program', 32), ('written', 32), ('returns', 31), ('writing', 31), ('dependencies', 31), ('inheritance', 31), ('lot', 31), ('similar', 31), ('domain', 31), ('mock', 30), ('mind', 30), ('imagine', 30), ('clear', 30), ('internal', 30), ('library', 30), ('process', 30), ('later', 30)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter, attrgetter\n",
    "uncommon_words_and_nummber=[]\n",
    "for n in range(len(uncommon_words)):\n",
    "     \n",
    "    uncommon_words_and_nummber.append((uncommon_words[n],words_from_book.count(uncommon_words[n])))\n",
    "\n",
    "uncommon_words_and_nummber.sort(reverse=True, key=itemgetter(1))\n",
    "print(uncommon_words_and_nummber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beer', 'Lahai'),\n",
       " ('Lahai', 'Roi'),\n",
       " ('gray', 'hairs'),\n",
       " ('Most', 'High'),\n",
       " ('ewe', 'lambs')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# change this to read in your data\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "   nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3) \n",
    "\n",
    "# return the 5 n-grams with the highest PMI\n",
    "finder.nbest(bigram_measures.pmi, 5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
